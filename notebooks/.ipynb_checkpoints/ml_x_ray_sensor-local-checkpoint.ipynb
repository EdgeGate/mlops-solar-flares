{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418648b9-dd97-45f2-b404-aba90b897644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb4b18a2-86d5-431d-8f5c-91fdcdfabfa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json, time\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, f1_score,\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import os, mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "from evidently import Report, Dataset, DataDefinition, MulticlassClassification\n",
    "from evidently.presets import ClassificationPreset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ab7daa5-b6c7-404f-883e-1d20cce142df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Chargement du fichier xrs_clean.parquet...\n",
      "✅ Données chargées : 147639 lignes, 11 colonnes\n"
     ]
    }
   ],
   "source": [
    "print(\"📥 Chargement du fichier xrs_clean.parquet...\")\n",
    "\n",
    "parquet_path = Path(r\"C:\\Users\\gate\\Documents\\Jedha\\Projet\\4\\mlops-solar-flares\\data\\xrs_clean.parquet\")\n",
    "df = parquet_path\n",
    "df = pd.read_parquet(parquet_path, engine=\"pyarrow\")\n",
    "print(f\"✅ Données chargées : {df.shape[0]} lignes, {df.shape[1]} colonnes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b84587a-232a-4029-bbd0-0ea2bc21e52b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# head: <bound method NDFrame.head of                             time  flux_long_wm2  flux_short_wm2 satellite  \\\n",
      "0      2025-05-01 00:00:00+00:00   7.021782e-07    1.000000e-09      <NA>   \n",
      "1      2025-05-01 00:01:00+00:00   6.994713e-07    1.000000e-09      <NA>   \n",
      "2      2025-05-01 00:02:00+00:00   7.052154e-07    1.000000e-09      <NA>   \n",
      "3      2025-05-01 00:03:00+00:00   7.015647e-07    1.000000e-09      <NA>   \n",
      "4      2025-05-01 00:04:00+00:00   6.966016e-07    1.000000e-09      <NA>   \n",
      "...                          ...            ...             ...       ...   \n",
      "147634 2025-08-11 12:34:00+00:00   5.304605e-06    5.942913e-07   GOES-18   \n",
      "147635 2025-08-11 12:35:00+00:00   5.992305e-06    7.550105e-07   GOES-18   \n",
      "147636 2025-08-11 12:36:00+00:00   6.500120e-06    8.529071e-07   GOES-18   \n",
      "147637 2025-08-11 12:37:00+00:00   6.883591e-06    8.953128e-07   GOES-18   \n",
      "147638 2025-08-11 12:38:00+00:00   7.047310e-06    8.543802e-07   GOES-18   \n",
      "\n",
      "       energy_long energy_short      source        date  hour  minute_of_day  \\\n",
      "0       0.1-0.8 nm  0.05-0.4 nm  NCEI-SunPy  2025-05-01     0              0   \n",
      "1       0.1-0.8 nm  0.05-0.4 nm  NCEI-SunPy  2025-05-01     0              1   \n",
      "2       0.1-0.8 nm  0.05-0.4 nm  NCEI-SunPy  2025-05-01     0              2   \n",
      "3       0.1-0.8 nm  0.05-0.4 nm  NCEI-SunPy  2025-05-01     0              3   \n",
      "4       0.1-0.8 nm  0.05-0.4 nm  NCEI-SunPy  2025-05-01     0              4   \n",
      "...            ...          ...         ...         ...   ...            ...   \n",
      "147634  0.1-0.8 nm  0.05-0.4 nm        <NA>  2025-08-11    12            754   \n",
      "147635  0.1-0.8 nm  0.05-0.4 nm        <NA>  2025-08-11    12            755   \n",
      "147636  0.1-0.8 nm  0.05-0.4 nm        <NA>  2025-08-11    12            756   \n",
      "147637  0.1-0.8 nm  0.05-0.4 nm        <NA>  2025-08-11    12            757   \n",
      "147638  0.1-0.8 nm  0.05-0.4 nm        <NA>  2025-08-11    12            758   \n",
      "\n",
      "        dow  \n",
      "0         3  \n",
      "1         3  \n",
      "2         3  \n",
      "3         3  \n",
      "4         3  \n",
      "...     ...  \n",
      "147634    0  \n",
      "147635    0  \n",
      "147636    0  \n",
      "147637    0  \n",
      "147638    0  \n",
      "\n",
      "[147639 rows x 11 columns]>\n",
      "\n",
      "# dtypes:\n",
      " time              datetime64[ns, UTC]\n",
      "flux_long_wm2                 float32\n",
      "flux_short_wm2                float32\n",
      "satellite              string[python]\n",
      "energy_long                    object\n",
      "energy_short                   object\n",
      "source                 string[python]\n",
      "date                   string[python]\n",
      "hour                            int16\n",
      "minute_of_day                   int16\n",
      "dow                              int8\n",
      "dtype: object\n",
      "\n",
      "# describe: <bound method NDFrame.describe of                             time  flux_long_wm2  flux_short_wm2 satellite  \\\n",
      "0      2025-05-01 00:00:00+00:00   7.021782e-07    1.000000e-09      <NA>   \n",
      "1      2025-05-01 00:01:00+00:00   6.994713e-07    1.000000e-09      <NA>   \n",
      "2      2025-05-01 00:02:00+00:00   7.052154e-07    1.000000e-09      <NA>   \n",
      "3      2025-05-01 00:03:00+00:00   7.015647e-07    1.000000e-09      <NA>   \n",
      "4      2025-05-01 00:04:00+00:00   6.966016e-07    1.000000e-09      <NA>   \n",
      "...                          ...            ...             ...       ...   \n",
      "147634 2025-08-11 12:34:00+00:00   5.304605e-06    5.942913e-07   GOES-18   \n",
      "147635 2025-08-11 12:35:00+00:00   5.992305e-06    7.550105e-07   GOES-18   \n",
      "147636 2025-08-11 12:36:00+00:00   6.500120e-06    8.529071e-07   GOES-18   \n",
      "147637 2025-08-11 12:37:00+00:00   6.883591e-06    8.953128e-07   GOES-18   \n",
      "147638 2025-08-11 12:38:00+00:00   7.047310e-06    8.543802e-07   GOES-18   \n",
      "\n",
      "       energy_long energy_short      source        date  hour  minute_of_day  \\\n",
      "0       0.1-0.8 nm  0.05-0.4 nm  NCEI-SunPy  2025-05-01     0              0   \n",
      "1       0.1-0.8 nm  0.05-0.4 nm  NCEI-SunPy  2025-05-01     0              1   \n",
      "2       0.1-0.8 nm  0.05-0.4 nm  NCEI-SunPy  2025-05-01     0              2   \n",
      "3       0.1-0.8 nm  0.05-0.4 nm  NCEI-SunPy  2025-05-01     0              3   \n",
      "4       0.1-0.8 nm  0.05-0.4 nm  NCEI-SunPy  2025-05-01     0              4   \n",
      "...            ...          ...         ...         ...   ...            ...   \n",
      "147634  0.1-0.8 nm  0.05-0.4 nm        <NA>  2025-08-11    12            754   \n",
      "147635  0.1-0.8 nm  0.05-0.4 nm        <NA>  2025-08-11    12            755   \n",
      "147636  0.1-0.8 nm  0.05-0.4 nm        <NA>  2025-08-11    12            756   \n",
      "147637  0.1-0.8 nm  0.05-0.4 nm        <NA>  2025-08-11    12            757   \n",
      "147638  0.1-0.8 nm  0.05-0.4 nm        <NA>  2025-08-11    12            758   \n",
      "\n",
      "        dow  \n",
      "0         3  \n",
      "1         3  \n",
      "2         3  \n",
      "3         3  \n",
      "4         3  \n",
      "...     ...  \n",
      "147634    0  \n",
      "147635    0  \n",
      "147636    0  \n",
      "147637    0  \n",
      "147638    0  \n",
      "\n",
      "[147639 rows x 11 columns]>\n",
      "\n",
      "# missing (%):\n",
      "satellite         91.68\n",
      "source             8.32\n",
      "flux_long_wm2      1.11\n",
      "flux_short_wm2     1.11\n",
      "time               0.00\n",
      "energy_long        0.00\n",
      "energy_short       0.00\n",
      "date               0.00\n",
      "hour               0.00\n",
      "minute_of_day      0.00\n",
      "\n",
      "# time range: 2025-05-01 00:00:00+00:00 -> 2025-08-11 12:38:00+00:00\n",
      "# time monotonic: True\n",
      "\n",
      "# last days (rows/day):\n",
      " time\n",
      "2025-08-02 00:00:00+00:00    1440\n",
      "2025-08-03 00:00:00+00:00    1440\n",
      "2025-08-04 00:00:00+00:00    1440\n",
      "2025-08-05 00:00:00+00:00    1440\n",
      "2025-08-06 00:00:00+00:00    1440\n",
      "2025-08-07 00:00:00+00:00    1440\n",
      "2025-08-08 00:00:00+00:00    1440\n",
      "2025-08-09 00:00:00+00:00    1440\n",
      "2025-08-10 00:00:00+00:00    1440\n",
      "2025-08-11 00:00:00+00:00     759\n"
     ]
    }
   ],
   "source": [
    "def quickpeek(df, topn=10):\n",
    "\n",
    "    print(\"# head:\", df.head)\n",
    "    print(\"\\n# dtypes:\\n\", df.dtypes)\n",
    "    print(\"\\n# describe:\", df.describe)\n",
    "\n",
    "    # missing %\n",
    "    print(\"\\n# missing (%):\")\n",
    "    miss = (df.isna().mean()*100).round(2).sort_values(ascending=False)\n",
    "    print(miss.head(topn).to_string())\n",
    "\n",
    "    if \"time\" in df:\n",
    "        # conversion robuste: tente direct, sinon passe par string\n",
    "        try:\n",
    "            t = pd.to_datetime(df[\"time\"], utc=True, errors=\"coerce\")\n",
    "        except Exception:\n",
    "            t = pd.to_datetime(df[\"time\"].astype(str), utc=True, errors=\"coerce\")\n",
    "\n",
    "        print(\"\\n# time range:\", t.min(), \"->\", t.max())\n",
    "\n",
    "        t_valid = t.dropna()\n",
    "        print(\"# time monotonic:\", t_valid.is_monotonic_increasing)\n",
    "\n",
    "        # comptage par jour sans dépendre du backend Arrow\n",
    "        try:\n",
    "            per_day = t.dt.floor(\"D\").value_counts().sort_index()\n",
    "        except Exception:\n",
    "            # fallback: utiliser la colonne 'date' si dispo\n",
    "            if \"date\" in df.columns:\n",
    "                per_day = pd.to_datetime(df[\"date\"], errors=\"coerce\").value_counts().sort_index()\n",
    "            else:\n",
    "                per_day = pd.Series(dtype=\"int64\")\n",
    "\n",
    "        if len(per_day):\n",
    "            print(\"\\n# last days (rows/day):\\n\", per_day.tail(10).to_string())\n",
    "\n",
    "\n",
    "quickpeek(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be565e26-3422-49c6-b772-1ec3fd1a3418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛠 Création de la variable cible 'flare_class'...\n",
      "✅ Variable cible ajoutée.\n"
     ]
    }
   ],
   "source": [
    "TARGET_NAME  = \"flare_class\"\n",
    "ALL_CLASSES  = np.array([\"A\", \"B\", \"C\", \"M\", \"X\"], dtype=object)\n",
    "print(\"🛠 Création de la variable cible 'flare_class'...\")\n",
    "\n",
    "def rule_predict(flux):\n",
    "    \"\"\"\n",
    "    Classe une éruption selon le pic de flux X (W/m², 1-8 Å) \n",
    "    en utilisant les seuils NOAA officiels, avec A inclus.\n",
    "    \"\"\"\n",
    "    if pd.isna(flux):\n",
    "        return None\n",
    "    elif flux < 1e-7:       # A : < 10⁻⁷ W/m²\n",
    "        return \"A\"\n",
    "    elif flux < 1e-6:       # B : 10⁻⁷ ≤ flux < 10⁻⁶\n",
    "        return \"B\"\n",
    "    elif flux < 1e-5:       # C : 10⁻⁶ ≤ flux < 10⁻⁵\n",
    "        return \"C\"\n",
    "    elif flux < 1e-4:       # M : 10⁻⁵ ≤ flux < 10⁻⁴\n",
    "        return \"M\"\n",
    "    else:                   # X : ≥ 10⁻⁴\n",
    "        return \"X\"\n",
    "\n",
    "df[\"flare_class\"] = df[\"flux_long_wm2\"].apply(rule_predict)\n",
    "\n",
    "print(\"✅ Variable cible ajoutée.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b95740f-1add-4e07-96ee-743180a4f2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 Conversion + features temporelles (safe)…\n",
      "✅ Features créées. NaN % (top 10):\n",
      "flux_short_wm2         1.11\n",
      "flux_short_lag1        1.11\n",
      "log_flux_short_lag1    1.11\n",
      "flux_short_mean_1h     1.07\n",
      "flux_short_std_1h      1.07\n",
      "flux_short_max_1h      1.07\n",
      "flux_short_mean_3h     1.01\n",
      "flux_short_max_3h      1.01\n",
      "hour                   0.00\n",
      "minute_of_day          0.00\n"
     ]
    }
   ],
   "source": [
    "print(\"📅 Conversion + features temporelles (safe)…\")\n",
    "\n",
    "# -- 0) Temps propre + tri --\n",
    "if \"time\" in df.columns:\n",
    "    t = pd.to_datetime(df[\"time\"].astype(str), utc=True, errors=\"coerce\")\n",
    "elif isinstance(df.index, pd.DatetimeIndex):\n",
    "    t = pd.to_datetime(df.index, utc=True, errors=\"coerce\")\n",
    "elif \"date\" in df.columns:\n",
    "    t = pd.to_datetime(df[\"date\"].astype(str), utc=True, errors=\"coerce\")\n",
    "else:\n",
    "    raise KeyError(\"Pas de colonne/indice temps ('time' ou 'date').\")\n",
    "\n",
    "df = df.assign(time=t).sort_values(\"time\").reset_index(drop=True)\n",
    "\n",
    "# -- 1) Colonnes temporelles dérivées --\n",
    "df[\"hour\"]           = df[\"time\"].dt.hour.astype(\"int16\")\n",
    "df[\"minute_of_day\"]  = (df[\"time\"].dt.hour * 60 + df[\"time\"].dt.minute).astype(\"int16\")\n",
    "df[\"dow\"]            = df[\"time\"].dt.dayofweek.astype(\"int8\")          # 0=lundi\n",
    "df[\"day_of_year\"]    = df[\"time\"].dt.dayofyear.astype(\"int16\")\n",
    "rad_doy              = 2 * np.pi * (df[\"day_of_year\"] - 1) / 365.25\n",
    "df[\"sin_doy\"]        = np.sin(rad_doy)\n",
    "df[\"cos_doy\"]        = np.cos(rad_doy)\n",
    "# Option: indicateur jour/nuit\n",
    "df[\"is_daytime\"]     = ((df[\"hour\"] >= 6) & (df[\"hour\"] <= 18)).astype(\"int8\")\n",
    "\n",
    "# -- 2) Features flux_short (passé uniquement) --\n",
    "s = pd.to_numeric(df[\"flux_short_wm2\"], errors=\"coerce\")\n",
    "\n",
    "lag1 = s.shift(1)\n",
    "\n",
    "# rolling calculé sur la série décalée (pas de fuite)\n",
    "roll_1h = lag1.rolling(window=12, min_periods=1)\n",
    "roll_3h = lag1.rolling(window=36, min_periods=1)\n",
    "\n",
    "df[\"flux_short_lag1\"]      = lag1\n",
    "df[\"flux_short_mean_1h\"]   = roll_1h.mean()\n",
    "df[\"flux_short_std_1h\"]    = roll_1h.std()\n",
    "df[\"flux_short_max_1h\"]    = roll_1h.max()\n",
    "df[\"flux_short_mean_3h\"]   = roll_3h.mean()\n",
    "df[\"flux_short_max_3h\"]    = roll_3h.max()\n",
    "df[\"log_flux_short_lag1\"]  = np.log10(lag1.clip(lower=1e-9))\n",
    "\n",
    "# -- 3) Au lieu d'un dropna global, on coupe seulement l'historique minimum --\n",
    "HISTORY_CUTOFF = 36  # 3h si données par minute; ajuste si besoin\n",
    "if len(df) > HISTORY_CUTOFF:\n",
    "    df = df.iloc[HISTORY_CUTOFF:].reset_index(drop=True)\n",
    "\n",
    "# -- 4) Diag NaN (pour vérif) --\n",
    "na_rate = (df[[\n",
    "    \"flux_short_wm2\",\"flux_short_lag1\",\"flux_short_mean_1h\",\"flux_short_std_1h\",\n",
    "    \"flux_short_max_1h\",\"flux_short_mean_3h\",\"flux_short_max_3h\",\"log_flux_short_lag1\",\n",
    "    \"hour\",\"minute_of_day\",\"dow\",\"sin_doy\",\"cos_doy\",\"is_daytime\"\n",
    "]].isna().mean()*100).round(2).sort_values(ascending=False)\n",
    "\n",
    "print(\"✅ Features créées. NaN % (top 10):\")\n",
    "print(na_rate.head(10).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4097e4f3-055a-4498-a375-b930d6fa43bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Nettoyage des colonnes inutiles...\n",
      "✅ Colonnes supprimées : ['satellite']\n",
      "✅ Types harmonisés.\n"
     ]
    }
   ],
   "source": [
    "print(\"🧹 Nettoyage des colonnes inutiles...\")\n",
    "# 1) Suppression de colonnes inutiles\n",
    "colonnes_a_supprimer = []\n",
    "if \"satellite\" in df.columns:\n",
    "    colonnes_a_supprimer.append(\"satellite\")\n",
    "\n",
    "df = df.drop(columns=colonnes_a_supprimer, errors=\"ignore\")\n",
    "print(f\"✅ Colonnes supprimées : {colonnes_a_supprimer if colonnes_a_supprimer else 'Aucune'}\")\n",
    "\n",
    "# 2) Harmonisation des types (basé sur ton nouveau set de features)\n",
    "numeric_features = [\n",
    "    \"flux_short_wm2\", \"hour\", \"minute_of_day\", \"dow\", \"sin_doy\",\n",
    "    \"flux_short_lag1\", \"flux_short_mean_1h\", \"flux_short_std_1h\",\n",
    "    \"flux_short_max_1h\", \"flux_short_mean_3h\", \"flux_short_max_3h\",\n",
    "    \"log_flux_short_lag1\"\n",
    "]\n",
    "categorical_features = [\"source\", \"energy_long\", \"energy_short\"]\n",
    "\n",
    "for col in numeric_features:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"float64\")\n",
    "\n",
    "for col in categorical_features:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(\"string\")\n",
    "\n",
    "print(\"✅ Types harmonisés.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3f41389-f109-476c-a1dd-d75a0a28cd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Fichier sauvegardé : C:\\Users\\gate\\Documents\\Jedha\\Projet\\4\\mlops-solar-flares\\data\\xrs_clean_ml.parquet\n"
     ]
    }
   ],
   "source": [
    "output_path = parquet_path.parent / \"xrs_clean_ml.parquet\"\n",
    "df.to_parquet(output_path, engine=\"pyarrow\", index=False)\n",
    "print(f\"💾 Fichier sauvegardé : {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "671f269e-0273-4feb-b1c3-dbaebb9ab0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# head:                        time  flux_long_wm2  flux_short_wm2 energy_long  \\\n",
      "0 2025-05-01 00:36:00+00:00   6.979719e-07    1.000000e-09  0.1-0.8 nm   \n",
      "1 2025-05-01 00:37:00+00:00   6.952811e-07    1.000000e-09  0.1-0.8 nm   \n",
      "2 2025-05-01 00:38:00+00:00   6.964259e-07    1.000000e-09  0.1-0.8 nm   \n",
      "3 2025-05-01 00:39:00+00:00   7.000616e-07    1.000000e-09  0.1-0.8 nm   \n",
      "4 2025-05-01 00:40:00+00:00   6.999362e-07    1.000000e-09  0.1-0.8 nm   \n",
      "\n",
      "  energy_short      source        date  hour  minute_of_day  dow  ...  \\\n",
      "0  0.05-0.4 nm  NCEI-SunPy  2025-05-01   0.0           36.0  3.0  ...   \n",
      "1  0.05-0.4 nm  NCEI-SunPy  2025-05-01   0.0           37.0  3.0  ...   \n",
      "2  0.05-0.4 nm  NCEI-SunPy  2025-05-01   0.0           38.0  3.0  ...   \n",
      "3  0.05-0.4 nm  NCEI-SunPy  2025-05-01   0.0           39.0  3.0  ...   \n",
      "4  0.05-0.4 nm  NCEI-SunPy  2025-05-01   0.0           40.0  3.0  ...   \n",
      "\n",
      "    sin_doy   cos_doy  is_daytime  flux_short_lag1  flux_short_mean_1h  \\\n",
      "0  0.880683 -0.473706           0     1.000000e-09        1.000000e-09   \n",
      "1  0.880683 -0.473706           0     1.000000e-09        1.000000e-09   \n",
      "2  0.880683 -0.473706           0     1.000000e-09        1.000000e-09   \n",
      "3  0.880683 -0.473706           0     1.000000e-09        1.000000e-09   \n",
      "4  0.880683 -0.473706           0     1.000000e-09        1.000000e-09   \n",
      "\n",
      "   flux_short_std_1h  flux_short_max_1h  flux_short_mean_3h  \\\n",
      "0                0.0       1.000000e-09        1.016178e-09   \n",
      "1                0.0       1.000000e-09        1.016178e-09   \n",
      "2                0.0       1.000000e-09        1.016178e-09   \n",
      "3                0.0       1.000000e-09        1.016178e-09   \n",
      "4                0.0       1.000000e-09        1.016178e-09   \n",
      "\n",
      "   flux_short_max_3h  log_flux_short_lag1  \n",
      "0       1.582413e-09                 -9.0  \n",
      "1       1.582413e-09                 -9.0  \n",
      "2       1.582413e-09                 -9.0  \n",
      "3       1.582413e-09                 -9.0  \n",
      "4       1.582413e-09                 -9.0  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "# dtypes:\n",
      " time                   datetime64[ns, UTC]\n",
      "flux_long_wm2                      float32\n",
      "flux_short_wm2                     float64\n",
      "energy_long                 string[python]\n",
      "energy_short                string[python]\n",
      "source                      string[python]\n",
      "date                        string[python]\n",
      "hour                               float64\n",
      "minute_of_day                      float64\n",
      "dow                                float64\n",
      "flare_class                         object\n",
      "day_of_year                          int16\n",
      "sin_doy                            float64\n",
      "cos_doy                            float64\n",
      "is_daytime                            int8\n",
      "flux_short_lag1                    float64\n",
      "flux_short_mean_1h                 float64\n",
      "flux_short_std_1h                  float64\n",
      "flux_short_max_1h                  float64\n",
      "flux_short_mean_3h                 float64\n",
      "flux_short_max_3h                  float64\n",
      "log_flux_short_lag1                float64\n",
      "dtype: object\n",
      "\n",
      "# describe:\n",
      "        flux_long_wm2  flux_short_wm2           hour  minute_of_day  \\\n",
      "count   1.459590e+05    1.459590e+05  147603.000000  147603.000000   \n",
      "mean    1.458734e-06    5.522871e-08      11.473669     717.920306   \n",
      "std     1.933495e-06    2.454358e-07       6.919764     415.541806   \n",
      "min     0.000000e+00    0.000000e+00       0.000000       0.000000   \n",
      "25%     7.573025e-07    1.000000e-09       5.000000     358.000000   \n",
      "50%     1.012920e-06    1.067829e-08      11.000000     716.000000   \n",
      "75%     1.448522e-06    3.325844e-08      17.000000    1078.000000   \n",
      "max     3.827447e-05    7.218636e-06      23.000000    1439.000000   \n",
      "\n",
      "                 dow    day_of_year        sin_doy        cos_doy  \\\n",
      "count  147603.000000  147603.000000  147603.000000  147603.000000   \n",
      "mean        3.043109     171.777139       0.177186      -0.857267   \n",
      "std         2.001335      29.590805       0.461627       0.143525   \n",
      "min         0.000000     121.000000      -0.626727      -0.999979   \n",
      "25%         1.000000     146.000000      -0.228058      -0.976509   \n",
      "50%         3.000000     172.000000       0.198648      -0.904405   \n",
      "75%         5.000000     197.000000       0.602988      -0.789905   \n",
      "max         6.000000     223.000000       0.880683      -0.473706   \n",
      "\n",
      "          is_daytime  flux_short_lag1  flux_short_mean_1h  flux_short_std_1h  \\\n",
      "count  147603.000000     1.459590e+05        1.460310e+05       1.460230e+05   \n",
      "mean        0.541717     5.522286e-08        5.520075e-08       1.704558e-08   \n",
      "std         0.498258     2.454269e-07        2.292148e-07       8.977560e-08   \n",
      "min         0.000000     0.000000e+00        0.000000e+00       0.000000e+00   \n",
      "25%         0.000000     1.000000e-09        1.324068e-09       5.948234e-10   \n",
      "50%         1.000000     1.067723e-08        1.125092e-08       2.204077e-09   \n",
      "75%         1.000000     3.325743e-08        3.440137e-08       6.233978e-09   \n",
      "max         1.000000     7.218636e-06        7.148259e-06       3.430924e-06   \n",
      "\n",
      "       flux_short_max_1h  flux_short_mean_3h  flux_short_max_3h  \\\n",
      "count       1.460310e+05        1.461110e+05       1.461110e+05   \n",
      "mean        8.305696e-08        5.520538e-08       1.357695e-07   \n",
      "std         3.249647e-07        1.984707e-07       4.446940e-07   \n",
      "min         0.000000e+00        0.000000e+00       0.000000e+00   \n",
      "25%         2.897411e-09        2.190732e-09       7.592487e-09   \n",
      "50%         1.711485e-08        1.257076e-08       2.704402e-08   \n",
      "75%         4.742508e-08        3.666942e-08       7.448698e-08   \n",
      "max         7.218636e-06        5.132329e-06       7.218636e-06   \n",
      "\n",
      "       log_flux_short_lag1  \n",
      "count        145959.000000  \n",
      "mean             -8.051616  \n",
      "std               0.793489  \n",
      "min              -9.000000  \n",
      "25%              -9.000000  \n",
      "50%              -7.971541  \n",
      "75%              -7.478111  \n",
      "max              -5.141545  \n",
      "\n",
      "# missing (%):\n",
      "source                 8.32\n",
      "log_flux_short_lag1    1.11\n",
      "flux_short_wm2         1.11\n",
      "flux_short_lag1        1.11\n",
      "flare_class            1.11\n",
      "flux_long_wm2          1.11\n",
      "flux_short_max_1h      1.07\n",
      "flux_short_std_1h      1.07\n",
      "flux_short_mean_1h     1.07\n",
      "flux_short_max_3h      1.01\n",
      "\n",
      "✅ Aucune colonne entièrement vide trouvée.\n",
      "\n",
      "# time range: 2025-05-01 00:36:00+00:00 -> 2025-08-11 12:38:00+00:00\n",
      "# time monotonic: True\n",
      "\n",
      "# last days (rows/day):\n",
      " time\n",
      "2025-08-02 00:00:00+00:00    1440\n",
      "2025-08-03 00:00:00+00:00    1440\n",
      "2025-08-04 00:00:00+00:00    1440\n",
      "2025-08-05 00:00:00+00:00    1440\n",
      "2025-08-06 00:00:00+00:00    1440\n",
      "2025-08-07 00:00:00+00:00    1440\n",
      "2025-08-08 00:00:00+00:00    1440\n",
      "2025-08-09 00:00:00+00:00    1440\n",
      "2025-08-10 00:00:00+00:00    1440\n",
      "2025-08-11 00:00:00+00:00     759\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>flux_long_wm2</th>\n",
       "      <th>flux_short_wm2</th>\n",
       "      <th>energy_long</th>\n",
       "      <th>energy_short</th>\n",
       "      <th>source</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute_of_day</th>\n",
       "      <th>dow</th>\n",
       "      <th>...</th>\n",
       "      <th>sin_doy</th>\n",
       "      <th>cos_doy</th>\n",
       "      <th>is_daytime</th>\n",
       "      <th>flux_short_lag1</th>\n",
       "      <th>flux_short_mean_1h</th>\n",
       "      <th>flux_short_std_1h</th>\n",
       "      <th>flux_short_max_1h</th>\n",
       "      <th>flux_short_mean_3h</th>\n",
       "      <th>flux_short_max_3h</th>\n",
       "      <th>log_flux_short_lag1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-05-01 00:36:00+00:00</td>\n",
       "      <td>6.979719e-07</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>0.1-0.8 nm</td>\n",
       "      <td>0.05-0.4 nm</td>\n",
       "      <td>NCEI-SunPy</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.880683</td>\n",
       "      <td>-0.473706</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>1.016178e-09</td>\n",
       "      <td>1.582413e-09</td>\n",
       "      <td>-9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-05-01 00:37:00+00:00</td>\n",
       "      <td>6.952811e-07</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>0.1-0.8 nm</td>\n",
       "      <td>0.05-0.4 nm</td>\n",
       "      <td>NCEI-SunPy</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.880683</td>\n",
       "      <td>-0.473706</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>1.016178e-09</td>\n",
       "      <td>1.582413e-09</td>\n",
       "      <td>-9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-05-01 00:38:00+00:00</td>\n",
       "      <td>6.964259e-07</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>0.1-0.8 nm</td>\n",
       "      <td>0.05-0.4 nm</td>\n",
       "      <td>NCEI-SunPy</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.880683</td>\n",
       "      <td>-0.473706</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>1.016178e-09</td>\n",
       "      <td>1.582413e-09</td>\n",
       "      <td>-9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-05-01 00:39:00+00:00</td>\n",
       "      <td>7.000616e-07</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>0.1-0.8 nm</td>\n",
       "      <td>0.05-0.4 nm</td>\n",
       "      <td>NCEI-SunPy</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.880683</td>\n",
       "      <td>-0.473706</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>1.016178e-09</td>\n",
       "      <td>1.582413e-09</td>\n",
       "      <td>-9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-05-01 00:40:00+00:00</td>\n",
       "      <td>6.999362e-07</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>0.1-0.8 nm</td>\n",
       "      <td>0.05-0.4 nm</td>\n",
       "      <td>NCEI-SunPy</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.880683</td>\n",
       "      <td>-0.473706</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>1.016178e-09</td>\n",
       "      <td>1.582413e-09</td>\n",
       "      <td>-9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147598</th>\n",
       "      <td>2025-08-11 12:34:00+00:00</td>\n",
       "      <td>5.304605e-06</td>\n",
       "      <td>5.942913e-07</td>\n",
       "      <td>0.1-0.8 nm</td>\n",
       "      <td>0.05-0.4 nm</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2025-08-11</td>\n",
       "      <td>12.0</td>\n",
       "      <td>754.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.626727</td>\n",
       "      <td>-0.779239</td>\n",
       "      <td>1</td>\n",
       "      <td>4.549486e-07</td>\n",
       "      <td>1.373538e-07</td>\n",
       "      <td>1.332366e-07</td>\n",
       "      <td>4.549486e-07</td>\n",
       "      <td>8.135605e-08</td>\n",
       "      <td>4.549486e-07</td>\n",
       "      <td>-6.342038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147599</th>\n",
       "      <td>2025-08-11 12:35:00+00:00</td>\n",
       "      <td>5.992305e-06</td>\n",
       "      <td>7.550105e-07</td>\n",
       "      <td>0.1-0.8 nm</td>\n",
       "      <td>0.05-0.4 nm</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2025-08-11</td>\n",
       "      <td>12.0</td>\n",
       "      <td>755.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.626727</td>\n",
       "      <td>-0.779239</td>\n",
       "      <td>1</td>\n",
       "      <td>5.942913e-07</td>\n",
       "      <td>1.826791e-07</td>\n",
       "      <td>1.838596e-07</td>\n",
       "      <td>5.942913e-07</td>\n",
       "      <td>9.633450e-08</td>\n",
       "      <td>5.942913e-07</td>\n",
       "      <td>-6.226001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147600</th>\n",
       "      <td>2025-08-11 12:36:00+00:00</td>\n",
       "      <td>6.500120e-06</td>\n",
       "      <td>8.529071e-07</td>\n",
       "      <td>0.1-0.8 nm</td>\n",
       "      <td>0.05-0.4 nm</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2025-08-11</td>\n",
       "      <td>12.0</td>\n",
       "      <td>756.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.626727</td>\n",
       "      <td>-0.779239</td>\n",
       "      <td>1</td>\n",
       "      <td>7.550105e-07</td>\n",
       "      <td>2.414877e-07</td>\n",
       "      <td>2.412320e-07</td>\n",
       "      <td>7.550105e-07</td>\n",
       "      <td>1.157367e-07</td>\n",
       "      <td>7.550105e-07</td>\n",
       "      <td>-6.122047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147601</th>\n",
       "      <td>2025-08-11 12:37:00+00:00</td>\n",
       "      <td>6.883591e-06</td>\n",
       "      <td>8.953128e-07</td>\n",
       "      <td>0.1-0.8 nm</td>\n",
       "      <td>0.05-0.4 nm</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2025-08-11</td>\n",
       "      <td>12.0</td>\n",
       "      <td>757.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.626727</td>\n",
       "      <td>-0.779239</td>\n",
       "      <td>1</td>\n",
       "      <td>8.529071e-07</td>\n",
       "      <td>3.084449e-07</td>\n",
       "      <td>2.897125e-07</td>\n",
       "      <td>8.529071e-07</td>\n",
       "      <td>1.378470e-07</td>\n",
       "      <td>8.529071e-07</td>\n",
       "      <td>-6.069098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147602</th>\n",
       "      <td>2025-08-11 12:38:00+00:00</td>\n",
       "      <td>7.047310e-06</td>\n",
       "      <td>8.543802e-07</td>\n",
       "      <td>0.1-0.8 nm</td>\n",
       "      <td>0.05-0.4 nm</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2025-08-11</td>\n",
       "      <td>12.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.626727</td>\n",
       "      <td>-0.779239</td>\n",
       "      <td>1</td>\n",
       "      <td>8.953128e-07</td>\n",
       "      <td>3.782425e-07</td>\n",
       "      <td>3.228239e-07</td>\n",
       "      <td>8.953128e-07</td>\n",
       "      <td>1.611068e-07</td>\n",
       "      <td>8.953128e-07</td>\n",
       "      <td>-6.048025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147603 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            time  flux_long_wm2  flux_short_wm2 energy_long  \\\n",
       "0      2025-05-01 00:36:00+00:00   6.979719e-07    1.000000e-09  0.1-0.8 nm   \n",
       "1      2025-05-01 00:37:00+00:00   6.952811e-07    1.000000e-09  0.1-0.8 nm   \n",
       "2      2025-05-01 00:38:00+00:00   6.964259e-07    1.000000e-09  0.1-0.8 nm   \n",
       "3      2025-05-01 00:39:00+00:00   7.000616e-07    1.000000e-09  0.1-0.8 nm   \n",
       "4      2025-05-01 00:40:00+00:00   6.999362e-07    1.000000e-09  0.1-0.8 nm   \n",
       "...                          ...            ...             ...         ...   \n",
       "147598 2025-08-11 12:34:00+00:00   5.304605e-06    5.942913e-07  0.1-0.8 nm   \n",
       "147599 2025-08-11 12:35:00+00:00   5.992305e-06    7.550105e-07  0.1-0.8 nm   \n",
       "147600 2025-08-11 12:36:00+00:00   6.500120e-06    8.529071e-07  0.1-0.8 nm   \n",
       "147601 2025-08-11 12:37:00+00:00   6.883591e-06    8.953128e-07  0.1-0.8 nm   \n",
       "147602 2025-08-11 12:38:00+00:00   7.047310e-06    8.543802e-07  0.1-0.8 nm   \n",
       "\n",
       "       energy_short      source        date  hour  minute_of_day  dow  ...  \\\n",
       "0       0.05-0.4 nm  NCEI-SunPy  2025-05-01   0.0           36.0  3.0  ...   \n",
       "1       0.05-0.4 nm  NCEI-SunPy  2025-05-01   0.0           37.0  3.0  ...   \n",
       "2       0.05-0.4 nm  NCEI-SunPy  2025-05-01   0.0           38.0  3.0  ...   \n",
       "3       0.05-0.4 nm  NCEI-SunPy  2025-05-01   0.0           39.0  3.0  ...   \n",
       "4       0.05-0.4 nm  NCEI-SunPy  2025-05-01   0.0           40.0  3.0  ...   \n",
       "...             ...         ...         ...   ...            ...  ...  ...   \n",
       "147598  0.05-0.4 nm        <NA>  2025-08-11  12.0          754.0  0.0  ...   \n",
       "147599  0.05-0.4 nm        <NA>  2025-08-11  12.0          755.0  0.0  ...   \n",
       "147600  0.05-0.4 nm        <NA>  2025-08-11  12.0          756.0  0.0  ...   \n",
       "147601  0.05-0.4 nm        <NA>  2025-08-11  12.0          757.0  0.0  ...   \n",
       "147602  0.05-0.4 nm        <NA>  2025-08-11  12.0          758.0  0.0  ...   \n",
       "\n",
       "         sin_doy   cos_doy  is_daytime  flux_short_lag1  flux_short_mean_1h  \\\n",
       "0       0.880683 -0.473706           0     1.000000e-09        1.000000e-09   \n",
       "1       0.880683 -0.473706           0     1.000000e-09        1.000000e-09   \n",
       "2       0.880683 -0.473706           0     1.000000e-09        1.000000e-09   \n",
       "3       0.880683 -0.473706           0     1.000000e-09        1.000000e-09   \n",
       "4       0.880683 -0.473706           0     1.000000e-09        1.000000e-09   \n",
       "...          ...       ...         ...              ...                 ...   \n",
       "147598 -0.626727 -0.779239           1     4.549486e-07        1.373538e-07   \n",
       "147599 -0.626727 -0.779239           1     5.942913e-07        1.826791e-07   \n",
       "147600 -0.626727 -0.779239           1     7.550105e-07        2.414877e-07   \n",
       "147601 -0.626727 -0.779239           1     8.529071e-07        3.084449e-07   \n",
       "147602 -0.626727 -0.779239           1     8.953128e-07        3.782425e-07   \n",
       "\n",
       "        flux_short_std_1h  flux_short_max_1h  flux_short_mean_3h  \\\n",
       "0            0.000000e+00       1.000000e-09        1.016178e-09   \n",
       "1            0.000000e+00       1.000000e-09        1.016178e-09   \n",
       "2            0.000000e+00       1.000000e-09        1.016178e-09   \n",
       "3            0.000000e+00       1.000000e-09        1.016178e-09   \n",
       "4            0.000000e+00       1.000000e-09        1.016178e-09   \n",
       "...                   ...                ...                 ...   \n",
       "147598       1.332366e-07       4.549486e-07        8.135605e-08   \n",
       "147599       1.838596e-07       5.942913e-07        9.633450e-08   \n",
       "147600       2.412320e-07       7.550105e-07        1.157367e-07   \n",
       "147601       2.897125e-07       8.529071e-07        1.378470e-07   \n",
       "147602       3.228239e-07       8.953128e-07        1.611068e-07   \n",
       "\n",
       "        flux_short_max_3h  log_flux_short_lag1  \n",
       "0            1.582413e-09            -9.000000  \n",
       "1            1.582413e-09            -9.000000  \n",
       "2            1.582413e-09            -9.000000  \n",
       "3            1.582413e-09            -9.000000  \n",
       "4            1.582413e-09            -9.000000  \n",
       "...                   ...                  ...  \n",
       "147598       4.549486e-07            -6.342038  \n",
       "147599       5.942913e-07            -6.226001  \n",
       "147600       7.550105e-07            -6.122047  \n",
       "147601       8.529071e-07            -6.069098  \n",
       "147602       8.953128e-07            -6.048025  \n",
       "\n",
       "[147603 rows x 22 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def quickpeek(df, topn=10):\n",
    "\n",
    "    print(\"# head:\", df.head())\n",
    "    print(\"\\n# dtypes:\\n\", df.dtypes)\n",
    "    print(\"\\n# describe:\\n\", df.describe())\n",
    "\n",
    "    # missing %\n",
    "    print(\"\\n# missing (%):\")\n",
    "    miss = (df.isna().mean() * 100).round(2).sort_values(ascending=False)\n",
    "    print(miss.head(topn).to_string())\n",
    "\n",
    "    # 🔹 Suppression des colonnes entièrement vides\n",
    "    colonnes_vides = df.columns[df.isna().all()].tolist()\n",
    "    if colonnes_vides:\n",
    "        print(f\"\\n🗑 Suppression de {len(colonnes_vides)} colonne(s) vide(s) : {colonnes_vides}\")\n",
    "        df.drop(columns=colonnes_vides, inplace=True)\n",
    "    else:\n",
    "        print(\"\\n✅ Aucune colonne entièrement vide trouvée.\")\n",
    "\n",
    "    if \"time\" in df:\n",
    "        # conversion robuste: tente direct, sinon passe par string\n",
    "        try:\n",
    "            t = pd.to_datetime(df[\"time\"], utc=True, errors=\"coerce\")\n",
    "        except Exception:\n",
    "            t = pd.to_datetime(df[\"time\"].astype(str), utc=True, errors=\"coerce\")\n",
    "\n",
    "        print(\"\\n# time range:\", t.min(), \"->\", t.max())\n",
    "        t_valid = t.dropna()\n",
    "        print(\"# time monotonic:\", t_valid.is_monotonic_increasing)\n",
    "\n",
    "        # comptage par jour\n",
    "        try:\n",
    "            per_day = t.dt.floor(\"D\").value_counts().sort_index()\n",
    "        except Exception:\n",
    "            if \"date\" in df.columns:\n",
    "                per_day = pd.to_datetime(df[\"date\"], errors=\"coerce\").value_counts().sort_index()\n",
    "            else:\n",
    "                per_day = pd.Series(dtype=\"int64\")\n",
    "\n",
    "        if len(per_day):\n",
    "            print(\"\\n# last days (rows/day):\\n\", per_day.tail(10).to_string())\n",
    "\n",
    "    return df  # On retourne le DataFrame propre\n",
    "quickpeek(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54cede2f-877e-4fe0-b459-aac86bb3f39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 Dernières lignes du fichier :\n",
      "                     time  flux_long_wm2  flux_short_wm2 energy_long energy_short source       date  hour  minute_of_day  dow flare_class  day_of_year   sin_doy   cos_doy  is_daytime  flux_short_lag1  flux_short_mean_1h  flux_short_std_1h  flux_short_max_1h  flux_short_mean_3h  flux_short_max_3h  log_flux_short_lag1\n",
      "2025-08-11 12:29:00+00:00       0.000002    8.580523e-08  0.1-0.8 nm  0.05-0.4 nm   <NA> 2025-08-11  12.0          749.0  0.0           C          223 -0.626727 -0.779239           1     8.022680e-08        6.054497e-08       1.180990e-08       8.116984e-08        5.594103e-08       8.116984e-08            -7.095681\n",
      "2025-08-11 12:30:00+00:00       0.000002    1.043969e-07  0.1-0.8 nm  0.05-0.4 nm   <NA> 2025-08-11  12.0          750.0  0.0           C          223 -0.626727 -0.779239           1     8.580523e-08        6.257249e-08       1.388949e-08       8.580523e-08        5.660375e-08       8.580523e-08            -7.066486\n",
      "2025-08-11 12:31:00+00:00       0.000003    2.087382e-07  0.1-0.8 nm  0.05-0.4 nm   <NA> 2025-08-11  12.0          751.0  0.0           C          223 -0.626727 -0.779239           1     1.043969e-07        6.616177e-08       1.837793e-08       1.043969e-07        5.787996e-08       1.043969e-07            -6.981312\n",
      "2025-08-11 12:32:00+00:00       0.000004    3.514351e-07  0.1-0.8 nm  0.05-0.4 nm   <NA> 2025-08-11  12.0          752.0  0.0           C          223 -0.626727 -0.779239           1     2.087382e-07        7.882645e-08       4.475203e-08       2.087382e-07        6.206778e-08       2.087382e-07            -6.680398\n",
      "2025-08-11 12:33:00+00:00       0.000004    4.549486e-07  0.1-0.8 nm  0.05-0.4 nm   <NA> 2025-08-11  12.0          753.0  0.0           C          223 -0.626727 -0.779239           1     3.514351e-07        1.037625e-07       8.953171e-08       3.514351e-07        7.028950e-08       3.514351e-07            -6.454155\n",
      "2025-08-11 12:34:00+00:00       0.000005    5.942913e-07  0.1-0.8 nm  0.05-0.4 nm   <NA> 2025-08-11  12.0          754.0  0.0           C          223 -0.626727 -0.779239           1     4.549486e-07        1.373538e-07       1.332366e-07       4.549486e-07        8.135605e-08       4.549486e-07            -6.342038\n",
      "2025-08-11 12:35:00+00:00       0.000006    7.550105e-07  0.1-0.8 nm  0.05-0.4 nm   <NA> 2025-08-11  12.0          755.0  0.0           C          223 -0.626727 -0.779239           1     5.942913e-07        1.826791e-07       1.838596e-07       5.942913e-07        9.633450e-08       5.942913e-07            -6.226001\n",
      "2025-08-11 12:36:00+00:00       0.000007    8.529071e-07  0.1-0.8 nm  0.05-0.4 nm   <NA> 2025-08-11  12.0          756.0  0.0           C          223 -0.626727 -0.779239           1     7.550105e-07        2.414877e-07       2.412320e-07       7.550105e-07        1.157367e-07       7.550105e-07            -6.122047\n",
      "2025-08-11 12:37:00+00:00       0.000007    8.953128e-07  0.1-0.8 nm  0.05-0.4 nm   <NA> 2025-08-11  12.0          757.0  0.0           C          223 -0.626727 -0.779239           1     8.529071e-07        3.084449e-07       2.897125e-07       8.529071e-07        1.378470e-07       8.529071e-07            -6.069098\n",
      "2025-08-11 12:38:00+00:00       0.000007    8.543802e-07  0.1-0.8 nm  0.05-0.4 nm   <NA> 2025-08-11  12.0          758.0  0.0           C          223 -0.626727 -0.779239           1     8.953128e-07        3.782425e-07       3.228239e-07       8.953128e-07        1.611068e-07       8.953128e-07            -6.048025\n",
      "\n",
      "📄 Premières lignes du fichier trié par 'time' :\n",
      "                       time  flux_long_wm2  flux_short_wm2 energy_long energy_short      source        date  hour  minute_of_day  dow flare_class  day_of_year   sin_doy   cos_doy  is_daytime  flux_short_lag1  flux_short_mean_1h  flux_short_std_1h  flux_short_max_1h  flux_short_mean_3h  flux_short_max_3h  log_flux_short_lag1\n",
      "0 2025-05-01 00:36:00+00:00   6.979719e-07    1.000000e-09  0.1-0.8 nm  0.05-0.4 nm  NCEI-SunPy  2025-05-01   0.0           36.0  3.0           B          121  0.880683 -0.473706           0     1.000000e-09        1.000000e-09                0.0       1.000000e-09        1.016178e-09       1.582413e-09                 -9.0\n",
      "1 2025-05-01 00:37:00+00:00   6.952811e-07    1.000000e-09  0.1-0.8 nm  0.05-0.4 nm  NCEI-SunPy  2025-05-01   0.0           37.0  3.0           B          121  0.880683 -0.473706           0     1.000000e-09        1.000000e-09                0.0       1.000000e-09        1.016178e-09       1.582413e-09                 -9.0\n",
      "2 2025-05-01 00:38:00+00:00   6.964259e-07    1.000000e-09  0.1-0.8 nm  0.05-0.4 nm  NCEI-SunPy  2025-05-01   0.0           38.0  3.0           B          121  0.880683 -0.473706           0     1.000000e-09        1.000000e-09                0.0       1.000000e-09        1.016178e-09       1.582413e-09                 -9.0\n",
      "3 2025-05-01 00:39:00+00:00   7.000616e-07    1.000000e-09  0.1-0.8 nm  0.05-0.4 nm  NCEI-SunPy  2025-05-01   0.0           39.0  3.0           B          121  0.880683 -0.473706           0     1.000000e-09        1.000000e-09                0.0       1.000000e-09        1.016178e-09       1.582413e-09                 -9.0\n",
      "4 2025-05-01 00:40:00+00:00   6.999362e-07    1.000000e-09  0.1-0.8 nm  0.05-0.4 nm  NCEI-SunPy  2025-05-01   0.0           40.0  3.0           B          121  0.880683 -0.473706           0     1.000000e-09        1.000000e-09                0.0       1.000000e-09        1.016178e-09       1.582413e-09                 -9.0\n",
      "5 2025-05-01 00:41:00+00:00   6.941411e-07    1.000000e-09  0.1-0.8 nm  0.05-0.4 nm  NCEI-SunPy  2025-05-01   0.0           41.0  3.0           B          121  0.880683 -0.473706           0     1.000000e-09        1.000000e-09                0.0       1.000000e-09        1.016178e-09       1.582413e-09                 -9.0\n",
      "6 2025-05-01 00:42:00+00:00   6.976728e-07    1.000000e-09  0.1-0.8 nm  0.05-0.4 nm  NCEI-SunPy  2025-05-01   0.0           42.0  3.0           B          121  0.880683 -0.473706           0     1.000000e-09        1.000000e-09                0.0       1.000000e-09        1.016178e-09       1.582413e-09                 -9.0\n",
      "7 2025-05-01 00:43:00+00:00   7.020463e-07    1.000000e-09  0.1-0.8 nm  0.05-0.4 nm  NCEI-SunPy  2025-05-01   0.0           43.0  3.0           B          121  0.880683 -0.473706           0     1.000000e-09        1.000000e-09                0.0       1.000000e-09        1.016178e-09       1.582413e-09                 -9.0\n",
      "8 2025-05-01 00:44:00+00:00   7.050160e-07    1.000000e-09  0.1-0.8 nm  0.05-0.4 nm  NCEI-SunPy  2025-05-01   0.0           44.0  3.0           B          121  0.880683 -0.473706           0     1.000000e-09        1.000000e-09                0.0       1.000000e-09        1.016178e-09       1.582413e-09                 -9.0\n",
      "9 2025-05-01 00:45:00+00:00   7.064747e-07    1.000000e-09  0.1-0.8 nm  0.05-0.4 nm  NCEI-SunPy  2025-05-01   0.0           45.0  3.0           B          121  0.880683 -0.473706           0     1.000000e-09        1.000000e-09                0.0       1.000000e-09        1.016178e-09       1.582413e-09                 -9.0\n"
     ]
    }
   ],
   "source": [
    "df[\"time\"] = pd.to_datetime(df[\"time\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "# Dernières lignes triées par temps\n",
    "print(\"\\n📄 Dernières lignes du fichier :\")\n",
    "print(df.sort_values(\"time\").tail(10).to_string(index=False))\n",
    "\n",
    "# Premières lignes triées par temps\n",
    "print(\"\\n📄 Premières lignes du fichier trié par 'time' :\")\n",
    "print(df.sort_values(\"time\").head(10).reset_index(drop=True).to_string(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8497ebe3-e3af-41ec-9432-7e353af8672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19e3c019-3e72-4f70-8378-43eae048a873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cible prête.\n",
      "✂️ Coupure au temps 2025-08-04 20:48:00+00:00 | train=138,012 | test=9,591\n",
      "  Train counts: {'B': 71093, 'C': 64161, 'M': 1095, 'A': 19}\n",
      "  Test counts : {'C': 9029, 'M': 256, 'B': 156, 'A': 150}\n",
      "🧩 Padding X (report only) prêt: 2 ligne(s).\n",
      "✅ Features sélectionnées (sans fuite) :\n",
      "  Num : ['flux_short_wm2', 'hour', 'minute_of_day', 'dow']\n",
      "  Cat : ['source', 'energy_long', 'energy_short']\n",
      "🧹 Nettoyage des valeurs manquantes...\n",
      "  flux_short_wm2: médiane=0.000000\n",
      "  hour: médiane=11.000000\n",
      "  minute_of_day: médiane=719.000000\n",
      "  dow: médiane=3.000000\n",
      "  source: mode='NCEI-SunPy'\n",
      "  energy_long: mode='0.1-0.8 nm'\n",
      "  energy_short: mode='0.05-0.4 nm'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gate\\AppData\\Local\\Temp\\ipykernel_24196\\1581244355.py:78: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  pad_times = pd.date_range(start=start_pad, periods=N_PAD_X, freq=\"H\", tz=\"UTC\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Données nettoyées\n",
      "⚙️ Création du preprocessor simplifié...\n",
      "🔄 Transformation des données...\n",
      "✅ Transformation terminée. Shapes : (138012, 8) (9591, 8)\n",
      "🎯 Préparation des cibles...\n",
      "✅ Encodage labels OK. Classes : ['A', 'B', 'C', 'M', 'X']\n",
      "   Répartition train : {'B': 71093, 'C': 64161, 'M': 1095, 'A': 19}\n",
      "   Répartition test  : {'C': 9029, 'M': 256, 'B': 156, 'A': 150}\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cible \n",
    "# ============================\n",
    "def classify_flare(flux):\n",
    "    if pd.isna(flux): return None\n",
    "    elif flux < 1e-7: return \"A\"\n",
    "    elif flux < 1e-6: return \"B\"\n",
    "    elif flux < 1e-5: return \"C\"\n",
    "    elif flux < 1e-4: return \"M\"\n",
    "    else: return \"X\"\n",
    "\n",
    "if TARGET_NAME not in df.columns:\n",
    "    if \"flux_long_wm2\" not in df.columns:\n",
    "        raise KeyError(\"Colonne 'flux_long_wm2' manquante : impossible de construire la cible.\")\n",
    "    print(\"🛠 Création de la variable cible 'flare_class' à partir de flux_long_wm2...\")\n",
    "    df[TARGET_NAME] = df[\"flux_long_wm2\"].apply(classify_flare)\n",
    "print(\"✅ Cible prête.\")\n",
    "# ============================\n",
    "# Split temporel\n",
    "# ============================\n",
    "\"\"\"\n",
    "print(\"✂️ Split train/test (80/20, ordre temporel conservé)...\")\n",
    "Y = df[TARGET_NAME].astype(\"string\")\n",
    "X = df.drop(columns=[TARGET_NAME])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=0, shuffle=False\n",
    ")\n",
    "print(f\"  - Train : {len(X_train)}\")\n",
    "print(f\"  - Test  : {len(X_test)}\")\n",
    "\"\"\"\n",
    "# ============================\n",
    "# Split temporel + contrainte sur A + padding X (report only)\n",
    "# ============================\n",
    "assert \"time\" in df.columns, \"La colonne 'time' doit exister et être de type datetime.\"\n",
    "df = df.sort_values(\"time\").reset_index(drop=True)\n",
    "\n",
    "TARGET_COL = TARGET_NAME           # ex: \"flare_class\"\n",
    "TEST_FRAC  = 0.20                  # cible 80/20 si possible\n",
    "MIN_IN_TRAIN = {\"A\": 20}           # au moins 20 'A' dans le train (adapte si besoin)\n",
    "MIN_TEST_FRAC = 0.05               # garde au moins 5% pour le test si 80/20 impossible\n",
    "\n",
    "y_all = df[TARGET_COL].astype(str)\n",
    "n = len(df)\n",
    "idx_80 = int(round(n * (1 - TEST_FRAC)))\n",
    "idx_test_min = int(round(n * MIN_TEST_FRAC))\n",
    "\n",
    "# cumul par classe pour trouver la 1ère position où on atteint les seuils demandés\n",
    "dummies = pd.get_dummies(y_all)\n",
    "for c, k in MIN_IN_TRAIN.items():\n",
    "    if c not in dummies.columns:\n",
    "        dummies[c] = 0\n",
    "cum = dummies.cumsum()\n",
    "\n",
    "ok = pd.Series(True, index=cum.index)\n",
    "for c, k in MIN_IN_TRAIN.items():\n",
    "    ok &= (cum[c] >= int(k))\n",
    "\n",
    "if ok.any():\n",
    "    first_ok_pos = int(np.argmax(ok.values))   # 1er index où la contrainte est satisfaite\n",
    "else:\n",
    "    first_ok_pos = 0  # jamais atteint -> on laissera le split 80/20 par défaut\n",
    "\n",
    "# choix du cutoff: 80/20 si possible, sinon on décale pour respecter le mini A\n",
    "cutoff_idx = max(idx_80, first_ok_pos)\n",
    "\n",
    "# ne pas dépasser la fin (laisser au moins MIN_TEST_FRAC en test)\n",
    "cutoff_idx = min(cutoff_idx, n - max(1, idx_test_min))\n",
    "cutoff_idx = max(1, min(cutoff_idx, n - 1))  # bornes de sécurité\n",
    "\n",
    "cutoff_time = df.loc[cutoff_idx, \"time\"]\n",
    "\n",
    "# applique le split\n",
    "X_train = df.iloc[:cutoff_idx].drop(columns=[TARGET_COL])\n",
    "Y_train = df.iloc[:cutoff_idx][TARGET_COL].astype(\"string\")\n",
    "X_test  = df.iloc[cutoff_idx:].drop(columns=[TARGET_COL])\n",
    "Y_test  = df.iloc[cutoff_idx:][TARGET_COL].astype(\"string\")\n",
    "\n",
    "print(f\"✂️ Coupure au temps {cutoff_time} | train={len(X_train):,} | test={len(X_test):,}\")\n",
    "print(\"  Train counts:\", Y_train.value_counts().to_dict())\n",
    "print(\"  Test counts :\", Y_test.value_counts().to_dict())\n",
    "\n",
    "# ============================\n",
    "# Padding X pour le REPORTING UNIQUEMENT (n'impacte pas train/test)\n",
    "# ============================\n",
    "# 👉 Renseigne ici des timestamps externes réels si tu en as (NOAA/SWPC).\n",
    "# Par défaut on génère 2 timestamps factices juste après la fin du test.\n",
    "N_PAD_X = 2  # mets 0 si tu ne veux pas de padding\n",
    "if N_PAD_X > 0:\n",
    "    start_pad = pd.to_datetime(X_test[\"time\"].max()) + pd.Timedelta(minutes=1)\n",
    "    pad_times = pd.date_range(start=start_pad, periods=N_PAD_X, freq=\"H\", tz=\"UTC\")\n",
    "\n",
    "    REPORT_PAD_X = pd.DataFrame({\n",
    "        \"when_utc\": pad_times,\n",
    "        \"target\": [\"X\"] * N_PAD_X,        # vérité terrain (pour visuels/rapports)\n",
    "        \"prediction\": [\"X\"] * N_PAD_X,    # ⚠️ pour le REPORT UNIQUEMENT\n",
    "        \"_external\": True\n",
    "    })\n",
    "else:\n",
    "    REPORT_PAD_X = pd.DataFrame(columns=[\"when_utc\", \"target\", \"prediction\", \"_external\"])\n",
    "\n",
    "print(f\"🧩 Padding X (report only) prêt: {len(REPORT_PAD_X)} ligne(s).\")\n",
    "\n",
    "def apply_report_padding(cur_df, pad_df=REPORT_PAD_X):\n",
    "    \"\"\"\n",
    "    À appeler APRÈS avoir construit cur_df = DataFrame({'target': y_test_txt, 'prediction': yhat_txt})\n",
    "    Retourne cur_df enrichi des lignes pad X pour le reporting (Evidently / HTML).\n",
    "    \"\"\"\n",
    "    if pad_df is None or len(pad_df) == 0:\n",
    "        return cur_df.copy()\n",
    "    cols = [c for c in [\"target\", \"prediction\", \"when_utc\", \"_external\"] if c in pad_df.columns]\n",
    "    return pd.concat([cur_df, pad_df[cols]], ignore_index=True)\n",
    "\n",
    "# ============================\n",
    "# Définition des features (⚠️ sans flux_long_wm2 pour éviter la fuite)\n",
    "# ============================\n",
    "# Candidats habituels :\n",
    "numeric_features_all      = [\"flux_short_wm2\", \"hour\", \"minute_of_day\", \"dow\"]\n",
    "categorical_features_all  = [\"source\", \"energy_long\", \"energy_short\"]\n",
    "\n",
    "# Garder seulement celles qui existent réellement\n",
    "numeric_features     = [c for c in numeric_features_all if c in X_train.columns]\n",
    "categorical_features = [c for c in categorical_features_all if c in X_train.columns]\n",
    "\n",
    "print(\"✅ Features sélectionnées (sans fuite) :\")\n",
    "print(\"  Num :\", numeric_features)\n",
    "print(\"  Cat :\", categorical_features)\n",
    "\n",
    "# ============================\n",
    "# Nettoyage manuel des valeurs manquantes AVANT preprocessing\n",
    "# ============================\n",
    "print(\"🧹 Nettoyage des valeurs manquantes...\")\n",
    "\n",
    "def clean_missing_values(X_train, X_test, numeric_cols, categorical_cols):\n",
    "    \"\"\"Nettoie manuellement les valeurs manquantes pour éviter les bugs SimpleImputer\"\"\"\n",
    "    X_train_clean = X_train.copy()\n",
    "    X_test_clean = X_test.copy()\n",
    "    \n",
    "    # Pour les features numériques : remplacer par la médiane du train\n",
    "    for col in numeric_cols:\n",
    "        if col in X_train_clean.columns:\n",
    "            # Conversion en float64 propre\n",
    "            X_train_clean[col] = pd.to_numeric(X_train_clean[col], errors=\"coerce\")\n",
    "            X_test_clean[col] = pd.to_numeric(X_test_clean[col], errors=\"coerce\")\n",
    "            \n",
    "            # Calculer la médiane sur le train\n",
    "            median_val = X_train_clean[col].median()\n",
    "            if pd.isna(median_val):\n",
    "                median_val = 0.0  # fallback si tout est NaN\n",
    "            \n",
    "            # Remplacer les NaN\n",
    "            X_train_clean[col] = X_train_clean[col].fillna(median_val)\n",
    "            X_test_clean[col] = X_test_clean[col].fillna(median_val)\n",
    "            \n",
    "            print(f\"  {col}: médiane={median_val:.6f}\")\n",
    "    \n",
    "    # Pour les features catégorielles : remplacer par le mode du train\n",
    "    for col in categorical_cols:\n",
    "        if col in X_train_clean.columns:\n",
    "            # Conversion en object propre\n",
    "            X_train_clean[col] = X_train_clean[col].astype(str)\n",
    "            X_test_clean[col] = X_test_clean[col].astype(str)\n",
    "            \n",
    "            # Calculer le mode sur le train (ignorer les 'nan' string)\n",
    "            mode_candidates = X_train_clean[col][X_train_clean[col] != 'nan'].mode()\n",
    "            if len(mode_candidates) > 0:\n",
    "                mode_val = mode_candidates.iloc[0]\n",
    "            else:\n",
    "                mode_val = \"unknown\"  # fallback\n",
    "            \n",
    "            # Remplacer les NaN (maintenant string 'nan')\n",
    "            X_train_clean[col] = X_train_clean[col].replace('nan', mode_val)\n",
    "            X_test_clean[col] = X_test_clean[col].replace('nan', mode_val)\n",
    "            \n",
    "            print(f\"  {col}: mode='{mode_val}'\")\n",
    "    \n",
    "    return X_train_clean, X_test_clean\n",
    "\n",
    "# Appliquer le nettoyage\n",
    "X_train_clean, X_test_clean = clean_missing_values(\n",
    "    X_train, X_test, numeric_features, categorical_features\n",
    ")\n",
    "\n",
    "# Restreindre aux colonnes utiles (ordre fixe)\n",
    "X_train_final = X_train_clean[numeric_features + categorical_features].copy()\n",
    "X_test_final = X_test_clean[numeric_features + categorical_features].copy()\n",
    "\n",
    "print(\"✅ Données nettoyées\")\n",
    "\n",
    "# ============================\n",
    "# Préprocesseur simplifié (sans SimpleImputer)\n",
    "# ============================\n",
    "print(\"⚙️ Création du preprocessor simplifié...\")\n",
    "\n",
    "numeric_transformer = StandardScaler()  # Plus de SimpleImputer\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# Transformation\n",
    "# ============================\n",
    "print(\"🔄 Transformation des données...\")\n",
    "try:\n",
    "    X_train_t = preprocessor.fit_transform(X_train_final)\n",
    "    X_test_t  = preprocessor.transform(X_test_final)\n",
    "    print(\"✅ Transformation terminée. Shapes :\", X_train_t.shape, X_test_t.shape)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Erreur transformation: {e}\")\n",
    "    print(\"Debug - Vérification des données:\")\n",
    "    print(\"X_train dtypes:\", X_train_final.dtypes.to_dict())\n",
    "    print(\"X_test dtypes:\", X_test_final.dtypes.to_dict())\n",
    "    \n",
    "    # Vérifier s'il y a encore des NaN\n",
    "    for col in X_train_final.columns:\n",
    "        nan_count_train = X_train_final[col].isna().sum()\n",
    "        nan_count_test = X_test_final[col].isna().sum()\n",
    "        if nan_count_train > 0 or nan_count_test > 0:\n",
    "            print(f\"  {col}: {nan_count_train} NaN train, {nan_count_test} NaN test\")\n",
    "    raise\n",
    "\n",
    "# ============================\n",
    "# Préparation cibles & encodage labels\n",
    "# ============================\n",
    "print(\"🎯 Préparation des cibles...\")\n",
    "mask_train = Y_train.notna()\n",
    "mask_test  = Y_test.notna()\n",
    "\n",
    "Xtr = X_train_t[mask_train.values]\n",
    "Xte = X_test_t[mask_test.values]\n",
    "ytr = Y_train[mask_train].astype(str).values\n",
    "yte = Y_test[mask_test].astype(str).values\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(ALL_CLASSES)                 # mapping figé A,B,C,M,X -> 0..4\n",
    "ytr_enc = le.transform(ytr)\n",
    "yte_enc = le.transform(yte)\n",
    "\n",
    "print(\"✅ Encodage labels OK. Classes :\", list(le.classes_))\n",
    "print(\"   Répartition train :\", pd.Series(ytr).value_counts().to_dict())\n",
    "print(\"   Répartition test  :\", pd.Series(yte).value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60bed8d7-77cd-4788-be64-cda97edc5484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : {'A': 19, 'B': 71093, 'C': 64161, 'M': 1095, 'X': 0}\n",
      "Test  : {'A': 150, 'B': 156, 'C': 9029, 'M': 256, 'X': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Train :\", {c: sum(ytr_enc == i) for i, c in enumerate(ALL_CLASSES)})\n",
    "print(\"Test  :\", {c: sum(yte_enc == i) for i, c in enumerate(ALL_CLASSES)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b62364a9-92c0-4bfa-9049-6f8d0a973b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === On suppose que Xtr, Xte, ytr_enc, yte_enc, ALL_CLASSES existent déjà ===\n",
    "\n",
    "# --- Helpers manquants ---\n",
    "def make_sample_weight(weights_by_name, y_enc, all_classes):\n",
    "    \"\"\"Construit sample_weight à partir d'un dict de poids par label (noms).\"\"\"\n",
    "    idx2name = {i: c for i, c in enumerate(all_classes)}\n",
    "    weights_by_idx = {i: float(weights_by_name.get(idx2name[i], 1.0)) for i in range(len(all_classes))}\n",
    "    return np.vectorize(weights_by_idx.get)(y_enc)\n",
    "\n",
    "def predict_with_thresholds(clf, X, all_classes, class_thresholds=None):\n",
    "    \"\"\"\n",
    "    Prédit avec seuils par classe (ex: {'X':0.05}). \n",
    "    Retourne (y_hat_indices_globaux, proba_full[K=nb classes globales]).\n",
    "    \"\"\"\n",
    "    proba = clf.predict_proba(X)           # (n, k_present)\n",
    "    present = clf.classes_                 # indices présents\n",
    "    K = len(all_classes)\n",
    "    proba_full = np.zeros((proba.shape[0], K), dtype=float)\n",
    "    proba_full[:, present] = proba\n",
    "    y_hat = np.argmax(proba_full, axis=1)\n",
    "\n",
    "    if class_thresholds:\n",
    "        for cname, thr in class_thresholds.items():\n",
    "            if cname in list(all_classes):\n",
    "                j = int(np.where(all_classes == cname)[0][0])\n",
    "                mask = proba_full[:, j] >= float(thr)\n",
    "                y_hat[mask] = j\n",
    "    return y_hat, proba_full\n",
    "\n",
    "def evaluate_with_custom_preds(name, ytr_true, ytr_hat, yte_true, yte_hat, ALL_CLASSES):\n",
    "    \"\"\"Évalue à partir de prédictions déjà calculées (utile avec des seuils).\"\"\"\n",
    "    acc_tr  = accuracy_score(ytr_true, ytr_hat)\n",
    "    bacc_tr = balanced_accuracy_score(ytr_true, ytr_hat)\n",
    "    f1m_tr  = f1_score(ytr_true, ytr_hat, average=\"macro\")\n",
    "    f1w_tr  = f1_score(ytr_true, ytr_hat, average=\"weighted\")\n",
    "\n",
    "    acc_te  = accuracy_score(yte_true, yte_hat)\n",
    "    bacc_te = balanced_accuracy_score(yte_true, yte_hat)\n",
    "    f1m_te  = f1_score(yte_true, yte_hat, average=\"macro\")\n",
    "    f1w_te  = f1_score(yte_true, yte_hat, average=\"weighted\")\n",
    "\n",
    "    print(f\"\\n========== {name} ==========\")\n",
    "    print(\"📊 Train :\", f\"acc={acc_tr:.4f} | bacc={bacc_tr:.4f} | f1m={f1m_tr:.4f} | f1w={f1w_tr:.4f}\")\n",
    "    print(\"📊 Test  :\",  f\"acc={acc_te:.4f} | bacc={bacc_te:.4f} | f1m={f1m_te:.4f} | f1w={f1w_te:.4f}\")\n",
    "\n",
    "    print(\"\\n🧾 Classification report (test)\")\n",
    "    print(classification_report(\n",
    "        yte_true, yte_hat,\n",
    "        labels=np.arange(len(ALL_CLASSES)),\n",
    "        target_names=ALL_CLASSES,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "    cm = confusion_matrix(yte_true, yte_hat, labels=np.arange(len(ALL_CLASSES)))\n",
    "    print(\"\\n🧩 Confusion matrix (counts)\\n\",\n",
    "          pd.DataFrame(cm,\n",
    "              index=[f\"true_{c}\" for c in ALL_CLASSES],\n",
    "              columns=[f\"pred_{c}\" for c in ALL_CLASSES]).to_string())\n",
    "    row_sums = cm.sum(axis=1, keepdims=True)\n",
    "    cmn = np.divide(cm, row_sums, out=np.zeros_like(cm, dtype=float), where=row_sums!=0)\n",
    "    print(\"\\n🧩 Confusion matrix (per-class)\\n\",\n",
    "          pd.DataFrame(cmn,\n",
    "              index=[f\"true_{c}\" for c in ALL_CLASSES],\n",
    "              columns=[f\"pred_{c}\" for c in ALL_CLASSES]).round(3).to_string())\n",
    "\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"acc_train\": acc_tr, \"bacc_train\": bacc_tr, \"f1m_train\": f1m_tr, \"f1w_train\": f1w_tr,\n",
    "        \"acc_test\":  acc_te, \"bacc_test\":  bacc_te, \"f1m_test\":  f1m_te, \"f1w_test\":  f1w_te\n",
    "    }\n",
    "\n",
    "# --- Objets communs ---\n",
    "sample_weight_tr = compute_sample_weight(class_weight=\"balanced\", y=ytr_enc)\n",
    "cv3 = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# --- Conteneurs : créer s'ils n'existent pas déjà (évite d'écraser après un premier run) ---\n",
    "if \"results_list\" not in globals():\n",
    "    results_list = []\n",
    "if \"fitted_pool\" not in globals():\n",
    "    fitted_pool = {}\n",
    "\n",
    "def add_model_result(name, clf, present, to_original, res_dict, yhat):\n",
    "    results_list.append({\"model\": name, **res_dict})\n",
    "    fitted_pool[name] = (clf, to_original, present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1125c1b7-a85a-4e28-88ab-54871910332a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== GradientBoosting (X-focus + seuil X) ==========\n",
      "📊 Train : acc=0.8374 | bacc=0.9062 | f1m=0.9004 | f1w=0.8371\n",
      "📊 Test  : acc=0.9763 | bacc=0.7660 | f1m=0.8145 | f1w=0.9738\n",
      "\n",
      "🧾 Classification report (test)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      1.00      1.00       150\n",
      "           B       0.84      0.38      0.52       156\n",
      "           C       0.98      0.99      0.99      9029\n",
      "           M       0.82      0.69      0.75       256\n",
      "           X       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.98      9591\n",
      "   macro avg       0.73      0.61      0.65      9591\n",
      "weighted avg       0.97      0.98      0.97      9591\n",
      "\n",
      "\n",
      "🧩 Confusion matrix (counts)\n",
      "         pred_A  pred_B  pred_C  pred_M  pred_X\n",
      "true_A     150       0       0       0       0\n",
      "true_B       0      59      97       0       0\n",
      "true_C       0      11    8978      40       0\n",
      "true_M       0       0      79     177       0\n",
      "true_X       0       0       0       0       0\n",
      "\n",
      "🧩 Confusion matrix (per-class)\n",
      "         pred_A  pred_B  pred_C  pred_M  pred_X\n",
      "true_A     1.0   0.000   0.000   0.000     0.0\n",
      "true_B     0.0   0.378   0.622   0.000     0.0\n",
      "true_C     0.0   0.001   0.994   0.004     0.0\n",
      "true_M     0.0   0.000   0.309   0.691     0.0\n",
      "true_X     0.0   0.000   0.000   0.000     0.0\n"
     ]
    }
   ],
   "source": [
    "# --- GBM focalisé X : poids + seuil ---\n",
    "weights_X_focus = {\"A\":1.0, \"B\":1.0, \"C\":1.0, \"M\":2.0, \"X\":1.0}\n",
    "sw_xfocus = make_sample_weight(weights_X_focus, ytr_enc, ALL_CLASSES)\n",
    "\n",
    "gbx = GradientBoostingClassifier(\n",
    "    n_estimators=150, learning_rate=0.1, max_depth=3, random_state=0\n",
    ")\n",
    "gbx.fit(Xtr, ytr_enc, sample_weight=sw_xfocus)\n",
    "\n",
    "thresholds = {\"X\": 0.05}  # ajuste selon FP/TP souhaités\n",
    "ytr_hat_gbx, _ = predict_with_thresholds(gbx, Xtr, ALL_CLASSES, thresholds)\n",
    "yte_hat_gbx, _ = predict_with_thresholds(gbx, Xte, ALL_CLASSES, thresholds)\n",
    "\n",
    "res_gbx = evaluate_with_custom_preds(\n",
    "    \"GradientBoosting (X-focus + seuil X)\", ytr_enc, ytr_hat_gbx, yte_enc, yte_hat_gbx, ALL_CLASSES\n",
    ")\n",
    "\n",
    "# mapping identitaire (labels déjà 0..len-1)\n",
    "to_original_id = {i: i for i in range(len(ALL_CLASSES))}\n",
    "add_model_result(\"GradientBoosting (X-focus + seuil X)\", gbx, np.unique(ytr_enc), to_original_id, res_gbx, yte_hat_gbx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f4f998d-bee9-4760-bf74-dc4f33bff3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Macro F1 (présentes=['A', 'B', 'C', 'M']): 0.815\n",
      "🎯 Macro F1 (toutes=['A', 'B', 'C', 'M', 'X']): 0.652\n",
      "🎯 Balanced Acc (présentes): 0.766\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      1.00      1.00       150\n",
      "           B       0.84      0.38      0.52       156\n",
      "           C       0.98      0.99      0.99      9029\n",
      "           M       0.82      0.69      0.75       256\n",
      "           X       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.98      9591\n",
      "   macro avg       0.73      0.61      0.65      9591\n",
      "weighted avg       0.97      0.98      0.97      9591\n",
      "\n"
     ]
    }
   ],
   "source": [
    "present_labels = np.unique(yte_enc)                 # classes réellement présentes en test\n",
    "all_labels = np.arange(len(ALL_CLASSES))            # A,B,C,M,X indexés 0..4\n",
    "\n",
    "f1_macro_present = f1_score(yte_enc, yte_hat_gbx, average=\"macro\")\n",
    "bacc_present     = balanced_accuracy_score(yte_enc, yte_hat_gbx)\n",
    "f1_macro_all     = f1_score(yte_enc, yte_hat_gbx, average=\"macro\",\n",
    "                            labels=all_labels, zero_division=0)\n",
    "\n",
    "print(f\"🎯 Macro F1 (présentes={list(ALL_CLASSES[present_labels])}): {f1_macro_present:.3f}\")\n",
    "print(f\"🎯 Macro F1 (toutes={list(ALL_CLASSES)}): {f1_macro_all:.3f}\")\n",
    "print(f\"🎯 Balanced Acc (présentes): {bacc_present:.3f}\")\n",
    "\n",
    "print(classification_report(\n",
    "    yte_enc, yte_hat_gbx,\n",
    "    labels=all_labels,              # <-- on force le report sur toutes les classes\n",
    "    target_names=ALL_CLASSES,\n",
    "    zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db9c0f9-9265-4bc0-ab3b-ea797e2ee547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b881a2d6-7f7a-4aca-b154-8c69c68b16d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ GradientBoosting (X-focus + seuil X) — 718 minutes ================\n",
      "\n",
      "⏱️ Plages continues :\n",
      "                    start                       end class  minutes\n",
      "2025-08-11 00:41:00+00:00 2025-08-11 03:04:00+00:00     C      144\n",
      "2025-08-11 03:05:00+00:00 2025-08-11 03:05:00+00:00     M        1\n",
      "2025-08-11 03:06:00+00:00 2025-08-11 03:47:00+00:00     C       42\n",
      "2025-08-11 03:48:00+00:00 2025-08-11 03:56:00+00:00     M        9\n",
      "2025-08-11 03:57:00+00:00 2025-08-11 08:38:00+00:00     C      282\n",
      "2025-08-11 08:39:00+00:00 2025-08-11 08:45:00+00:00     M        7\n",
      "2025-08-11 08:46:00+00:00 2025-08-11 11:43:00+00:00     C      178\n",
      "2025-08-11 11:44:00+00:00 2025-08-11 11:45:00+00:00     M        2\n",
      "2025-08-11 11:46:00+00:00 2025-08-11 12:35:00+00:00     C       50\n",
      "2025-08-11 12:36:00+00:00 2025-08-11 12:38:00+00:00     M        3\n",
      "\n",
      "📊 Comptes classes prédites (718 min) :\n",
      "pred_class\n",
      "C    696\n",
      "M     22\n",
      "\n",
      "📈 Probas moyennes (718 min) :\n",
      "A    0.000\n",
      "B    0.002\n",
      "C    0.969\n",
      "M    0.030\n",
      "X    0.000\n",
      "\n",
      "🏆 % minutes où chaque classe est 1ère proba :\n",
      " - A: 0.00%\n",
      " - B: 0.00%\n",
      " - C: 96.94%\n",
      " - M: 3.06%\n",
      " - X: 0.00%\n",
      "\n",
      "🏁 Part des classes prédites sur 718 min (par modèle) :\n",
      "                                      p_A  p_B    p_C   p_M  p_X\n",
      "model                                                           \n",
      "GradientBoosting (X-focus + seuil X)  0.0  0.0  96.94  3.06  0.0\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# helpers génériques\n",
    "# =========================\n",
    "H_NEXT = 718  # ~12h observables (ajuste à 720 si besoin)\n",
    "\n",
    "def safe_to_datetime(s):\n",
    "    return pd.to_datetime(s.astype(str), utc=True, errors=\"coerce\")\n",
    "\n",
    "def get_last_minutes_block(X_test, mask_test, Xte, minutes=H_NEXT):\n",
    "    \"\"\"\n",
    "    Retourne (X_last, t_last) pour les 'minutes' dernières minutes réelles du test.\n",
    "    Xte = features transformées correspondant à X_test[mask_test]\n",
    "    \"\"\"\n",
    "    # timeline côté X_test\n",
    "    if \"time\" in X_test.columns:\n",
    "        t_all = safe_to_datetime(X_test[\"time\"])\n",
    "    elif isinstance(X_test.index, pd.DatetimeIndex):\n",
    "        t_all = pd.to_datetime(X_test.index, utc=True, errors=\"coerce\").to_series()\n",
    "    elif \"date\" in X_test.columns:\n",
    "        t_all = safe_to_datetime(X_test[\"date\"])\n",
    "    else:\n",
    "        raise KeyError(\"Pas de colonne temps ('time' ou 'date') dans X_test.\")\n",
    "\n",
    "    # indices du test valides (après filtre) + tri par temps\n",
    "    idx_test = X_test.index[mask_test]\n",
    "    t_test_sorted = (\n",
    "        pd.DataFrame({\"time\": t_all.loc[idx_test].values}, index=idx_test)\n",
    "          .dropna()\n",
    "          .sort_values(\"time\")\n",
    "    )\n",
    "\n",
    "    # prendre les 'minutes' dernières\n",
    "    last_idx = t_test_sorted.tail(minutes).index\n",
    "\n",
    "    # positions dans Xte (Xte est l'ordre de X_test[mask_test])\n",
    "    pos_map = pd.Series(range(len(idx_test)), index=idx_test)\n",
    "    sel_pos = pos_map.loc[last_idx].sort_values()\n",
    "\n",
    "    X_last = Xte[sel_pos.values]\n",
    "    t_last = t_test_sorted.loc[last_idx, \"time\"].sort_values().reset_index(drop=True)\n",
    "    return X_last, t_last\n",
    "\n",
    "def softmax_from_decision(scores):\n",
    "    scores = np.array(scores)\n",
    "    if scores.ndim == 1:\n",
    "        scores = np.column_stack([-scores, scores])\n",
    "    m = scores.max(axis=1, keepdims=True)\n",
    "    exp = np.exp(scores - m)\n",
    "    return exp / exp.sum(axis=1, keepdims=True)\n",
    "\n",
    "def safe_predict_proba(estimator, X):\n",
    "    \"\"\"\n",
    "    Renvoie (proba, classes_idx_compacts).\n",
    "    \"\"\"\n",
    "    if hasattr(estimator, \"predict_proba\"):\n",
    "        p = estimator.predict_proba(X)\n",
    "        return p, estimator.classes_\n",
    "    elif hasattr(estimator, \"decision_function\"):\n",
    "        p = softmax_from_decision(estimator.decision_function(X))\n",
    "        classes_ = getattr(estimator, \"classes_\", np.arange(p.shape[1]))\n",
    "        return p, classes_\n",
    "    else:\n",
    "        # fallback uniforme\n",
    "        k = len(getattr(estimator, \"classes_\", [0, 1]))\n",
    "        n = X.shape[0]\n",
    "        return np.full((n, k), 1.0 / k), getattr(estimator, \"classes_\", np.arange(k))\n",
    "\n",
    "def build_718_table_for_model(name, fitted_entry, X_last, t_last, ALL_CLASSES):\n",
    "    \"\"\"\n",
    "    Construit le DataFrame minute->probas/classes pour 'name'.\n",
    "    fitted_entry = (clf, to_original, present)\n",
    "    \"\"\"\n",
    "    allc = np.array(ALL_CLASSES)\n",
    "    clf, to_original, present = fitted_entry\n",
    "\n",
    "    # proba sur classes COMPACTES (entraînement)\n",
    "    proba_compact, compact_classes = safe_predict_proba(clf, X_last)  # (N, k_present)\n",
    "\n",
    "    # mapping compact -> global index (0..len(ALL_CLASSES)-1)\n",
    "    compact_to_global = np.vectorize(to_original.get)(compact_classes)\n",
    "\n",
    "    # tableau proba sur toutes les classes globales\n",
    "    dfp = pd.DataFrame(0.0, index=np.arange(len(t_last)), columns=allc.tolist())\n",
    "\n",
    "    # injecter les proba aux bonnes colonnes\n",
    "    for j, gidx in enumerate(compact_to_global):\n",
    "        cname = allc[gidx]\n",
    "        dfp[cname] = proba_compact[:, j]\n",
    "\n",
    "    # time + classes dérivées\n",
    "    dfp.insert(0, \"time\", t_last.values)\n",
    "    dfp[\"pred_class\"]  = allc[dfp[allc].values.argmax(axis=1)]\n",
    "    dfp[\"pred_strong\"] = dfp[\"pred_class\"].isin([\"M\", \"X\"]).astype(int)\n",
    "\n",
    "    # tri par temps (sécurité)\n",
    "    dfp = dfp.dropna(subset=[\"time\"]).copy()\n",
    "    dfp[\"time\"] = pd.to_datetime(dfp[\"time\"], utc=True, errors=\"coerce\")\n",
    "    dfp = dfp.sort_values(\"time\").reset_index(drop=True)\n",
    "\n",
    "    # plages continues\n",
    "    change = dfp[\"pred_class\"].ne(dfp[\"pred_class\"].shift(1))\n",
    "    dfp[\"_grp\"] = change.cumsum()\n",
    "    spans = (\n",
    "        dfp.groupby(\"_grp\", as_index=False)\n",
    "           .agg(start=(\"time\", \"first\"),\n",
    "                end=(\"time\", \"last\"),\n",
    "                **{\"class\": (\"pred_class\", \"first\")},\n",
    "                minutes=(\"time\", \"size\"))\n",
    "           .drop(columns=[\"_grp\"])\n",
    "    )\n",
    "    return dfp, spans\n",
    "\n",
    "def describe_718(dfp, spans, name, ALL_CLASSES):\n",
    "    print(f\"\\n================ {name} — 718 minutes ================\")\n",
    "    print(\"\\n⏱️ Plages continues :\")\n",
    "    print(spans.to_string(index=False))\n",
    "\n",
    "    print(\"\\n📊 Comptes classes prédites (718 min) :\")\n",
    "    print(dfp[\"pred_class\"].value_counts().to_string())\n",
    "\n",
    "    print(\"\\n📈 Probas moyennes (718 min) :\")\n",
    "    print(dfp[list(ALL_CLASSES)].mean().round(3).to_string())\n",
    "\n",
    "    print(\"\\n🏆 % minutes où chaque classe est 1ère proba :\")\n",
    "    for c in ALL_CLASSES:\n",
    "        others = [x for x in ALL_CLASSES if x != c]\n",
    "        share = (dfp[c] >= dfp[others].max(axis=1)).mean() * 100\n",
    "        print(f\" - {c}: {share:.2f}%\")\n",
    "\n",
    "# =========================\n",
    "# extraire X_last & t_last une seule fois\n",
    "# =========================\n",
    "X12_t, t12 = get_last_minutes_block(X_test, mask_test, Xte, minutes=H_NEXT)\n",
    "\n",
    "# =========================\n",
    "# générer pour chaque modèle du pool\n",
    "# =========================\n",
    "pred_tables_718 = {}\n",
    "spans_718 = {}\n",
    "\n",
    "for name, fitted_entry in fitted_pool.items():\n",
    "    df_12h, spans = build_718_table_for_model(name, fitted_entry, X12_t, t12, ALL_CLASSES)\n",
    "    pred_tables_718[name] = df_12h\n",
    "    spans_718[name] = spans\n",
    "    # impression détaillée (commenter si trop verbeux)\n",
    "    describe_718(df_12h, spans, name, ALL_CLASSES)\n",
    "\n",
    "# =========================\n",
    "# tableau comparatif des parts de classes (718 min)\n",
    "# =========================\n",
    "summary = []\n",
    "for name, dfp in pred_tables_718.items():\n",
    "    vc = dfp[\"pred_class\"].value_counts(normalize=True).reindex(ALL_CLASSES, fill_value=0.0)\n",
    "    summary.append({\"model\": name, **{f\"p_{c}\": float(vc.get(c, 0.0)) for c in ALL_CLASSES}})\n",
    "\n",
    "if not summary:\n",
    "    print(\"\\n⚠️ Aucun modèle dans fitted_pool → pas de résumé.\")\n",
    "else:\n",
    "    summary_df = (pd.DataFrame(summary)\n",
    "                    .set_index(\"model\")\n",
    "                    .sort_index())\n",
    "    print(\"\\n🏁 Part des classes prédites sur 718 min (par modèle) :\")\n",
    "    print((summary_df * 100).round(2).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99de4fb7-29a8-4c14-9067-22dd0f555493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Train: acc=0.8374 | bacc=0.9062 | f1_macro=0.9004 | f1_weighted=0.8371\n",
      "📊 Test : acc=0.9763 | bacc=0.7660 | f1_macro=0.8145 | f1_weighted=0.9738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T:\\Anaconda\\envs\\solarflares\\lib\\site-packages\\_distutils_hack\\__init__.py:15: UserWarning:\n",
      "\n",
      "Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "\n",
      "T:\\Anaconda\\envs\\solarflares\\lib\\site-packages\\_distutils_hack\\__init__.py:30: UserWarning:\n",
      "\n",
      "Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "\n",
      "Registered model 'solar-flares-classifier' already exists. Creating a new version of this model...\n",
      "2025/08/12 10:57:17 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: solar-flares-classifier, version 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏸️ Non promu (reste en Staging) — test_f1_macro=0.8145 < 0.9\n",
      "✅ modèle sauvegardé & 📡 MLflow loggé (Evidently + CM + report) + Registry.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '7' of model 'solar-flares-classifier'.\n"
     ]
    }
   ],
   "source": [
    "# ================== 0) Config ==================\n",
    "mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\", \"http://mlflow:5000\"))\n",
    "mlflow.set_experiment(\"solar-flares\")\n",
    "\n",
    "PROD_THRESHOLD = 0.90  # gate de promotion auto\n",
    "\n",
    "# ================== 1) Métriques ==================\n",
    "metrics = {\n",
    "    \"train_acc\": accuracy_score(ytr_enc, ytr_hat_gbx),\n",
    "    \"train_bacc\": balanced_accuracy_score(ytr_enc, ytr_hat_gbx),\n",
    "    \"train_f1_macro\": f1_score(ytr_enc, ytr_hat_gbx, average=\"macro\"),\n",
    "    \"train_f1_weighted\": f1_score(ytr_enc, ytr_hat_gbx, average=\"weighted\"),\n",
    "    \"test_acc\": accuracy_score(yte_enc, yte_hat_gbx),\n",
    "    \"test_bacc\": balanced_accuracy_score(yte_enc, yte_hat_gbx),\n",
    "    \"test_f1_macro\": f1_score(yte_enc, yte_hat_gbx, average=\"macro\"),\n",
    "    \"test_f1_weighted\": f1_score(yte_enc, yte_hat_gbx, average=\"weighted\"),\n",
    "}\n",
    "print(f\"📊 Train: acc={metrics['train_acc']:.4f} | bacc={metrics['train_bacc']:.4f} | \"\n",
    "      f\"f1_macro={metrics['train_f1_macro']:.4f} | f1_weighted={metrics['train_f1_weighted']:.4f}\")\n",
    "print(f\"📊 Test : acc={metrics['test_acc']:.4f} | bacc={metrics['test_bacc']:.4f} | \"\n",
    "      f\"f1_macro={metrics['test_f1_macro']:.4f} | f1_weighted={metrics['test_f1_weighted']:.4f}\")\n",
    "\n",
    "# ================== 2) Params & contexte ==================\n",
    "params = {\n",
    "    \"algo\": \"GradientBoostingClassifier\",\n",
    "    \"n_estimators\": 150,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"max_depth\": 3,\n",
    "    \"random_state\": 0,\n",
    "    \"thresholds\": {\"X\": 0.05},\n",
    "    \"class_weights\": {\"A\":1.0, \"B\":1.0, \"C\":1.0, \"M\":2.0, \"X\":1.0},\n",
    "    \"n_train\": int(len(ytr_enc)),\n",
    "    \"n_test\": int(len(yte_enc)),\n",
    "}\n",
    "context = {\n",
    "    \"ALL_CLASSES\": list(ALL_CLASSES),\n",
    "    \"train_class_dist\": pd.Series(ytr).value_counts().to_dict(),\n",
    "    \"test_class_dist\": pd.Series(yte).value_counts().to_dict(),\n",
    "}\n",
    "\n",
    "# ================== 3) Artefacts locaux ==================\n",
    "# Confusion matrix (test)\n",
    "cm = confusion_matrix(yte_enc, yte_hat_gbx, labels=np.arange(len(ALL_CLASSES)))\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm, interpolation=\"nearest\")\n",
    "ax.set_title(\"Confusion matrix (test)\")\n",
    "plt.colorbar(im, ax=ax)\n",
    "ticks = np.arange(len(ALL_CLASSES))\n",
    "ax.set_xticks(ticks); ax.set_xticklabels(ALL_CLASSES, rotation=45, ha=\"right\")\n",
    "ax.set_yticks(ticks); ax.set_yticklabels(ALL_CLASSES)\n",
    "ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Classification report (test)\n",
    "report_txt = classification_report(\n",
    "    yte_enc, yte_hat_gbx,\n",
    "    labels=np.arange(len(ALL_CLASSES)),\n",
    "    target_names=ALL_CLASSES,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "# ===== Evidently 0.7 — robuste aux classes manquantes et labels string =====\n",
    "idx2name = {i: c for i, c in enumerate(ALL_CLASSES)}\n",
    "\n",
    "# (1) DataFrames ref/test en TEXTE\n",
    "ref_txt = pd.DataFrame({\n",
    "    \"target\": ytr,\n",
    "    \"prediction\": [idx2name[i] for i in ytr_hat_gbx]\n",
    "})\n",
    "cur_txt = pd.DataFrame({\n",
    "    \"target\": yte,\n",
    "    \"prediction\": [idx2name[i] for i in yte_hat_gbx]\n",
    "})\n",
    "\n",
    "# (2) Ensemble des labels réellement présents (ref ∪ cur)\n",
    "labels_present = sorted(set(ref_txt[\"target\"]) | set(ref_txt[\"prediction\"]) |\n",
    "                        set(cur_txt[\"target\"]) | set(cur_txt[\"prediction\"]))\n",
    "name2id = {c: i for i, c in enumerate(labels_present)}   # 'A'->0, 'B'->1, ...\n",
    "id2name = {i: c for c, i in name2id.items()}             # 0->'A', 1->'B', ...\n",
    "\n",
    "# (3) Mapping vers IDs entiers (évite les KeyError 'A')\n",
    "def to_ids(df):\n",
    "    out = pd.DataFrame({\n",
    "        \"target\": df[\"target\"].map(name2id),\n",
    "        \"prediction\": df[\"prediction\"].map(name2id),\n",
    "    })\n",
    "    return out.dropna().astype(int)\n",
    "\n",
    "ref_ids = to_ids(ref_txt)\n",
    "cur_ids = to_ids(cur_txt)\n",
    "\n",
    "if len(ref_ids) == 0 or len(cur_ids) == 0:\n",
    "    print(\"⚠️ Après mapping, DataFrame vide pour Evidently. Vérifie labels_present:\", labels_present)\n",
    "\n",
    "# (4) Définition Evidently\n",
    "data_def = DataDefinition(classification=[\n",
    "    MulticlassClassification(\n",
    "        target=\"target\",\n",
    "        prediction_labels=\"prediction\",\n",
    "        labels=list(range(len(labels_present)))  # ex: [0,1,2,3]\n",
    "    )\n",
    "])\n",
    "\n",
    "ref_ds = Dataset.from_pandas(ref_ids, data_definition=data_def)\n",
    "cur_ds = Dataset.from_pandas(cur_ids, data_definition=data_def)\n",
    "\n",
    "# (5) Génération du rapport (fallback en \"current only\" si comparaison échoue)\n",
    "ev = Report([ClassificationPreset()])\n",
    "try:\n",
    "    snap = ev.run(cur_ds, ref_ds)   # comparaison current vs reference\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Evidently comparaison a échoué -> current only. Raison:\", repr(e))\n",
    "    snap = ev.run(cur_ds)\n",
    "\n",
    "# (6) Sauvegardes (HTML + JSON fallback)\n",
    "EVIDENTLY_HTML = \"evidently_report.html\"\n",
    "EVIDENTLY_JSON = \"evidently_report.json\"\n",
    "\n",
    "# HTML (OK en 0.7+)\n",
    "snap.save_html(EVIDENTLY_HTML)\n",
    "\n",
    "# JSON : tenter .json(), sinon payload “maison”\n",
    "saved_json = False\n",
    "try:\n",
    "    if hasattr(snap, \"json\"):\n",
    "        with open(EVIDENTLY_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(snap.json())\n",
    "        saved_json = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if not saved_json:\n",
    "    # --- Fallback JSON (pour le tracking MLflow) ---\n",
    "    cm = confusion_matrix(yte_enc, yte_hat_gbx, labels=np.arange(len(ALL_CLASSES)))\n",
    "    clf_dict = classification_report(\n",
    "        yte_enc, yte_hat_gbx,\n",
    "        labels=np.arange(len(ALL_CLASSES)),\n",
    "        target_names=ALL_CLASSES,\n",
    "        zero_division=0,\n",
    "        output_dict=True\n",
    "    )\n",
    "    summary_payload = {\n",
    "        \"labels_present\": labels_present,\n",
    "        \"n_reference\": int(len(ref_ids)),\n",
    "        \"n_current\": int(len(cur_ids)),\n",
    "        \"sklearn_report_test\": clf_dict,\n",
    "        \"confusion_matrix_test\": cm.tolist(),\n",
    "        \"metrics_logged\": {\n",
    "            \"train_acc\": float(metrics[\"train_acc\"]),\n",
    "            \"train_bacc\": float(metrics[\"train_bacc\"]),\n",
    "            \"train_f1_macro\": float(metrics[\"train_f1_macro\"]),\n",
    "            \"test_acc\": float(metrics[\"test_acc\"]),\n",
    "            \"test_bacc\": float(metrics[\"test_bacc\"]),\n",
    "            \"test_f1_macro\": float(metrics[\"test_f1_macro\"]),\n",
    "        },\n",
    "    }\n",
    "    import json\n",
    "    with open(EVIDENTLY_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary_payload, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ================== 4) Sauvegarde locale du modèle ==================\n",
    "MODEL_PATH = Path(\"./models/model.pkl\")\n",
    "MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(gbx, MODEL_PATH)\n",
    "\n",
    "# ================== 5) Log MLflow + Registry avec gate ==================\n",
    "# Fermer proprement un run resté ouvert (après un crash ou une exécution interrompue)\n",
    "if mlflow.active_run() is not None:\n",
    "    print(\"ℹ️ Fin de l'ancien run:\", mlflow.active_run().info.run_id)\n",
    "    mlflow.end_run()\n",
    "run_name = f\"GBM_X_focus_threshold_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    # params / métriques / contexte\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metrics(metrics)\n",
    "    mlflow.log_dict(context, \"context.json\")\n",
    "\n",
    "    # artefacts\n",
    "    mlflow.log_text(report_txt, \"classification_report_test.txt\")\n",
    "    mlflow.log_figure(fig, \"confusion_matrix_test.png\"); plt.close(fig)\n",
    "    mlflow.log_artifact(EVIDENTLY_HTML, artifact_path=\"evidently\")\n",
    "    mlflow.log_artifact(EVIDENTLY_JSON, artifact_path=\"evidently\")\n",
    "    mlflow.log_artifact(str(MODEL_PATH))\n",
    "\n",
    "    # signature + modèle versionné dans le run\n",
    "    sig = infer_signature(pd.DataFrame(Xtr[:200]), gbx.predict(Xtr[:200]))\n",
    "    mlflow.sklearn.log_model(gbx, artifact_path=\"model\", signature=sig)\n",
    "\n",
    "    # Enregistrement au Model Registry\n",
    "    model_uri = f\"runs:/{run.info.run_id}/model\"\n",
    "    reg = mlflow.register_model(model_uri, \"solar-flares-classifier\")\n",
    "\n",
    "    # Gate de promo: prod si f1_macro_test >= PROD_THRESHOLD\n",
    "    mlflow.set_tag(\"prod_threshold\", PROD_THRESHOLD)\n",
    "    promoted = metrics[\"test_f1_macro\"] >= PROD_THRESHOLD\n",
    "\n",
    "    from mlflow import MlflowClient\n",
    "    client = MlflowClient()\n",
    "\n",
    "    # tags sur la version\n",
    "    client.set_model_version_tag(\"solar-flares-classifier\", reg.version, \"test_f1_macro\", str(metrics[\"test_f1_macro\"]))\n",
    "    client.set_model_version_tag(\"solar-flares-classifier\", reg.version, \"promoted_to_production\", str(promoted))\n",
    "\n",
    "    # alias Staging toujours mis à jour\n",
    "    client.set_registered_model_alias(\"solar-flares-classifier\", \"Staging\", reg.version)\n",
    "\n",
    "    if promoted:\n",
    "        client.set_registered_model_alias(\"solar-flares-classifier\", \"Production\", reg.version)\n",
    "        print(f\"🚀 Promu en Production (v{reg.version}) — test_f1_macro={metrics['test_f1_macro']:.4f} ≥ {PROD_THRESHOLD}\")\n",
    "    else:\n",
    "        print(f\"⏸️ Non promu (reste en Staging) — test_f1_macro={metrics['test_f1_macro']:.4f} < {PROD_THRESHOLD}\")\n",
    "\n",
    "print(\"✅ modèle sauvegardé & 📡 MLflow loggé (Evidently + CM + report) + Registry.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cd5567-bec1-4a4a-a303-fd5213b4cb5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8505a8-d2ab-4d01-b1bc-029cf1eb7501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (solarflares)",
   "language": "python",
   "name": "solarflares"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
