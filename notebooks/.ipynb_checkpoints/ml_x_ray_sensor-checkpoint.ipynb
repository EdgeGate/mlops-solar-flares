{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb4b18a2-86d5-431d-8f5c-91fdcdfabfa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json, time\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, f1_score,\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import os, mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models.signature import infer_signature\n",
    "from evidently import Report, Dataset, DataDefinition, MulticlassClassification\n",
    "from evidently.presets import ClassificationPreset\n",
    "import time\n",
    "import tempfile, json, os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9d14b18-8d91-4888-b40e-3de307fbb2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Chargement du fichier xrs_clean.parquet...\n",
      "📥 Reading: \\opt\\project\\data\\xrs_clean.parquet\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '\\\\opt\\\\project\\\\data\\\\xrs_clean.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m parquet_path \u001b[38;5;241m=\u001b[39m ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxrs_clean.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📥 Reading:\u001b[39m\u001b[38;5;124m\"\u001b[39m, parquet_path)\n\u001b[1;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparquet_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Données chargées : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m lignes, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m colonnes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mT:\\Anaconda\\envs\\solarflares\\lib\\site-packages\\pandas\\io\\parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    668\u001b[0m     path,\n\u001b[0;32m    669\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    670\u001b[0m     filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[0;32m    671\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    672\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[0;32m    673\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    674\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    676\u001b[0m )\n",
      "File \u001b[1;32mT:\\Anaconda\\envs\\solarflares\\lib\\site-packages\\pandas\\io\\parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[0;32m    275\u001b[0m         path_or_handle,\n\u001b[0;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    280\u001b[0m     )\n",
      "File \u001b[1;32mT:\\Anaconda\\envs\\solarflares\\lib\\site-packages\\pandas\\io\\parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mT:\\Anaconda\\envs\\solarflares\\lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '\\\\opt\\\\project\\\\data\\\\xrs_clean.parquet'"
     ]
    }
   ],
   "source": [
    "print(\"📥 Chargement du fichier xrs_clean.parquet...\")\n",
    "ROOT = Path(os.environ.get(\"PROJECT_ROOT\", \"/opt/project\"))\n",
    "\n",
    "# --- Lecture\n",
    "parquet_path = ROOT / \"data\" / \"xrs_clean.parquet\"\n",
    "print(\"📥 Reading:\", parquet_path)\n",
    "df = pd.read_parquet(parquet_path, engine=\"pyarrow\")\n",
    "print(f\"✅ Données chargées : {df.shape[0]} lignes, {df.shape[1]} colonnes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b84587a-232a-4029-bbd0-0ea2bc21e52b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quickpeek(df, topn=10):\n",
    "\n",
    "    print(\"# head:\", df.head)\n",
    "    print(\"\\n# dtypes:\\n\", df.dtypes)\n",
    "    print(\"\\n# describe:\", df.describe)\n",
    "\n",
    "    # missing %\n",
    "    print(\"\\n# missing (%):\")\n",
    "    miss = (df.isna().mean()*100).round(2).sort_values(ascending=False)\n",
    "    print(miss.head(topn).to_string())\n",
    "\n",
    "    if \"time\" in df:\n",
    "        # conversion robuste: tente direct, sinon passe par string\n",
    "        try:\n",
    "            t = pd.to_datetime(df[\"time\"], utc=True, errors=\"coerce\")\n",
    "        except Exception:\n",
    "            t = pd.to_datetime(df[\"time\"].astype(str), utc=True, errors=\"coerce\")\n",
    "\n",
    "        print(\"\\n# time range:\", t.min(), \"->\", t.max())\n",
    "\n",
    "        t_valid = t.dropna()\n",
    "        print(\"# time monotonic:\", t_valid.is_monotonic_increasing)\n",
    "\n",
    "        # comptage par jour sans dépendre du backend Arrow\n",
    "        try:\n",
    "            per_day = t.dt.floor(\"D\").value_counts().sort_index()\n",
    "        except Exception:\n",
    "            # fallback: utiliser la colonne 'date' si dispo\n",
    "            if \"date\" in df.columns:\n",
    "                per_day = pd.to_datetime(df[\"date\"], errors=\"coerce\").value_counts().sort_index()\n",
    "            else:\n",
    "                per_day = pd.Series(dtype=\"int64\")\n",
    "\n",
    "        if len(per_day):\n",
    "            print(\"\\n# last days (rows/day):\\n\", per_day.tail(10).to_string())\n",
    "\n",
    "\n",
    "quickpeek(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be565e26-3422-49c6-b772-1ec3fd1a3418",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_NAME  = \"flare_class\"\n",
    "ALL_CLASSES  = np.array([\"A\", \"B\", \"C\", \"M\", \"X\"], dtype=object)\n",
    "print(\"🛠 Création de la variable cible 'flare_class'...\")\n",
    "\n",
    "def rule_predict(flux):\n",
    "    \"\"\"\n",
    "    Classe une éruption selon le pic de flux X (W/m², 1-8 Å) \n",
    "    en utilisant les seuils NOAA officiels, avec A inclus.\n",
    "    \"\"\"\n",
    "    if pd.isna(flux):\n",
    "        return None\n",
    "    elif flux < 1e-7:       # A : < 10⁻⁷ W/m²\n",
    "        return \"A\"\n",
    "    elif flux < 1e-6:       # B : 10⁻⁷ ≤ flux < 10⁻⁶\n",
    "        return \"B\"\n",
    "    elif flux < 1e-5:       # C : 10⁻⁶ ≤ flux < 10⁻⁵\n",
    "        return \"C\"\n",
    "    elif flux < 1e-4:       # M : 10⁻⁵ ≤ flux < 10⁻⁴\n",
    "        return \"M\"\n",
    "    else:                   # X : ≥ 10⁻⁴\n",
    "        return \"X\"\n",
    "\n",
    "df[\"flare_class\"] = df[\"flux_long_wm2\"].apply(rule_predict)\n",
    "\n",
    "print(\"✅ Variable cible ajoutée.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b95740f-1add-4e07-96ee-743180a4f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📅 Conversion + features temporelles (safe)…\")\n",
    "\n",
    "# -- 0) Temps propre + tri --\n",
    "if \"time\" in df.columns:\n",
    "    t = pd.to_datetime(df[\"time\"].astype(str), utc=True, errors=\"coerce\")\n",
    "elif isinstance(df.index, pd.DatetimeIndex):\n",
    "    t = pd.to_datetime(df.index, utc=True, errors=\"coerce\")\n",
    "elif \"date\" in df.columns:\n",
    "    t = pd.to_datetime(df[\"date\"].astype(str), utc=True, errors=\"coerce\")\n",
    "else:\n",
    "    raise KeyError(\"Pas de colonne/indice temps ('time' ou 'date').\")\n",
    "\n",
    "df = df.assign(time=t).sort_values(\"time\").reset_index(drop=True)\n",
    "\n",
    "# -- 1) Colonnes temporelles dérivées --\n",
    "df[\"hour\"]           = df[\"time\"].dt.hour.astype(\"int16\")\n",
    "df[\"minute_of_day\"]  = (df[\"time\"].dt.hour * 60 + df[\"time\"].dt.minute).astype(\"int16\")\n",
    "df[\"dow\"]            = df[\"time\"].dt.dayofweek.astype(\"int8\")          # 0=lundi\n",
    "df[\"day_of_year\"]    = df[\"time\"].dt.dayofyear.astype(\"int16\")\n",
    "rad_doy              = 2 * np.pi * (df[\"day_of_year\"] - 1) / 365.25\n",
    "df[\"sin_doy\"]        = np.sin(rad_doy)\n",
    "df[\"cos_doy\"]        = np.cos(rad_doy)\n",
    "# Option: indicateur jour/nuit\n",
    "df[\"is_daytime\"]     = ((df[\"hour\"] >= 6) & (df[\"hour\"] <= 18)).astype(\"int8\")\n",
    "\n",
    "# -- 2) Features flux_short (passé uniquement) --\n",
    "s = pd.to_numeric(df[\"flux_short_wm2\"], errors=\"coerce\")\n",
    "\n",
    "lag1 = s.shift(1)\n",
    "\n",
    "# rolling calculé sur la série décalée (pas de fuite)\n",
    "roll_1h = lag1.rolling(window=12, min_periods=1)\n",
    "roll_3h = lag1.rolling(window=36, min_periods=1)\n",
    "\n",
    "df[\"flux_short_lag1\"]      = lag1\n",
    "df[\"flux_short_mean_1h\"]   = roll_1h.mean()\n",
    "df[\"flux_short_std_1h\"]    = roll_1h.std()\n",
    "df[\"flux_short_max_1h\"]    = roll_1h.max()\n",
    "df[\"flux_short_mean_3h\"]   = roll_3h.mean()\n",
    "df[\"flux_short_max_3h\"]    = roll_3h.max()\n",
    "df[\"log_flux_short_lag1\"]  = np.log10(lag1.clip(lower=1e-9))\n",
    "\n",
    "# -- 3) Au lieu d'un dropna global, on coupe seulement l'historique minimum --\n",
    "HISTORY_CUTOFF = 36  # 3h si données par minute; ajuste si besoin\n",
    "if len(df) > HISTORY_CUTOFF:\n",
    "    df = df.iloc[HISTORY_CUTOFF:].reset_index(drop=True)\n",
    "\n",
    "# -- 4) Diag NaN (pour vérif) --\n",
    "na_rate = (df[[\n",
    "    \"flux_short_wm2\",\"flux_short_lag1\",\"flux_short_mean_1h\",\"flux_short_std_1h\",\n",
    "    \"flux_short_max_1h\",\"flux_short_mean_3h\",\"flux_short_max_3h\",\"log_flux_short_lag1\",\n",
    "    \"hour\",\"minute_of_day\",\"dow\",\"sin_doy\",\"cos_doy\",\"is_daytime\"\n",
    "]].isna().mean()*100).round(2).sort_values(ascending=False)\n",
    "\n",
    "print(\"✅ Features créées. NaN % (top 10):\")\n",
    "print(na_rate.head(10).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4097e4f3-055a-4498-a375-b930d6fa43bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🧹 Nettoyage des colonnes inutiles...\")\n",
    "# 1) Suppression de colonnes inutiles\n",
    "colonnes_a_supprimer = []\n",
    "if \"satellite\" in df.columns:\n",
    "    colonnes_a_supprimer.append(\"satellite\")\n",
    "\n",
    "df = df.drop(columns=colonnes_a_supprimer, errors=\"ignore\")\n",
    "print(f\"✅ Colonnes supprimées : {colonnes_a_supprimer if colonnes_a_supprimer else 'Aucune'}\")\n",
    "\n",
    "# 2) Harmonisation des types (basé sur ton nouveau set de features)\n",
    "numeric_features = [\n",
    "    \"flux_short_wm2\", \"hour\", \"minute_of_day\", \"dow\", \"sin_doy\",\n",
    "    \"flux_short_lag1\", \"flux_short_mean_1h\", \"flux_short_std_1h\",\n",
    "    \"flux_short_max_1h\", \"flux_short_mean_3h\", \"flux_short_max_3h\",\n",
    "    \"log_flux_short_lag1\"\n",
    "]\n",
    "categorical_features = [\"source\", \"energy_long\", \"energy_short\"]\n",
    "\n",
    "for col in numeric_features:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"float64\")\n",
    "\n",
    "for col in categorical_features:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(\"string\")\n",
    "\n",
    "print(\"✅ Types harmonisés.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59be6aed-050e-4388-802b-d6778f37123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = ROOT / \"data\" / \"xrs_clean_ml.parquet\"\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)  # au cas où\n",
    "df.to_parquet(output_path, engine=\"pyarrow\", index=False)\n",
    "print(f\"💾 Fichier sauvegardé : {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671f269e-0273-4feb-b1c3-dbaebb9ab0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quickpeek(df, topn=10):\n",
    "\n",
    "    print(\"# head:\", df.head())\n",
    "    print(\"\\n# dtypes:\\n\", df.dtypes)\n",
    "    print(\"\\n# describe:\\n\", df.describe())\n",
    "\n",
    "    # missing %\n",
    "    print(\"\\n# missing (%):\")\n",
    "    miss = (df.isna().mean() * 100).round(2).sort_values(ascending=False)\n",
    "    print(miss.head(topn).to_string())\n",
    "\n",
    "    # 🔹 Suppression des colonnes entièrement vides\n",
    "    colonnes_vides = df.columns[df.isna().all()].tolist()\n",
    "    if colonnes_vides:\n",
    "        print(f\"\\n🗑 Suppression de {len(colonnes_vides)} colonne(s) vide(s) : {colonnes_vides}\")\n",
    "        df.drop(columns=colonnes_vides, inplace=True)\n",
    "    else:\n",
    "        print(\"\\n✅ Aucune colonne entièrement vide trouvée.\")\n",
    "\n",
    "    if \"time\" in df:\n",
    "        # conversion robuste: tente direct, sinon passe par string\n",
    "        try:\n",
    "            t = pd.to_datetime(df[\"time\"], utc=True, errors=\"coerce\")\n",
    "        except Exception:\n",
    "            t = pd.to_datetime(df[\"time\"].astype(str), utc=True, errors=\"coerce\")\n",
    "\n",
    "        print(\"\\n# time range:\", t.min(), \"->\", t.max())\n",
    "        t_valid = t.dropna()\n",
    "        print(\"# time monotonic:\", t_valid.is_monotonic_increasing)\n",
    "\n",
    "        # comptage par jour\n",
    "        try:\n",
    "            per_day = t.dt.floor(\"D\").value_counts().sort_index()\n",
    "        except Exception:\n",
    "            if \"date\" in df.columns:\n",
    "                per_day = pd.to_datetime(df[\"date\"], errors=\"coerce\").value_counts().sort_index()\n",
    "            else:\n",
    "                per_day = pd.Series(dtype=\"int64\")\n",
    "\n",
    "        if len(per_day):\n",
    "            print(\"\\n# last days (rows/day):\\n\", per_day.tail(10).to_string())\n",
    "\n",
    "    return df  # On retourne le DataFrame propre\n",
    "quickpeek(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cede2f-877e-4fe0-b459-aac86bb3f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"time\"] = pd.to_datetime(df[\"time\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "# Dernières lignes triées par temps\n",
    "print(\"\\n📄 Dernières lignes du fichier :\")\n",
    "print(df.sort_values(\"time\").tail(10).to_string(index=False))\n",
    "\n",
    "# Premières lignes triées par temps\n",
    "print(\"\\n📄 Premières lignes du fichier trié par 'time' :\")\n",
    "print(df.sort_values(\"time\").head(10).reset_index(drop=True).to_string(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8497ebe3-e3af-41ec-9432-7e353af8672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e3c019-3e72-4f70-8378-43eae048a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cible \n",
    "# ============================\n",
    "def classify_flare(flux):\n",
    "    if pd.isna(flux): return None\n",
    "    elif flux < 1e-7: return \"A\"\n",
    "    elif flux < 1e-6: return \"B\"\n",
    "    elif flux < 1e-5: return \"C\"\n",
    "    elif flux < 1e-4: return \"M\"\n",
    "    else: return \"X\"\n",
    "\n",
    "if TARGET_NAME not in df.columns:\n",
    "    if \"flux_long_wm2\" not in df.columns:\n",
    "        raise KeyError(\"Colonne 'flux_long_wm2' manquante : impossible de construire la cible.\")\n",
    "    print(\"🛠 Création de la variable cible 'flare_class' à partir de flux_long_wm2...\")\n",
    "    df[TARGET_NAME] = df[\"flux_long_wm2\"].apply(classify_flare)\n",
    "print(\"✅ Cible prête.\")\n",
    "# ============================\n",
    "# Split temporel\n",
    "# ============================\n",
    "\"\"\"\n",
    "print(\"✂️ Split train/test (80/20, ordre temporel conservé)...\")\n",
    "Y = df[TARGET_NAME].astype(\"string\")\n",
    "X = df.drop(columns=[TARGET_NAME])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=0, shuffle=False\n",
    ")\n",
    "print(f\"  - Train : {len(X_train)}\")\n",
    "print(f\"  - Test  : {len(X_test)}\")\n",
    "\"\"\"\n",
    "# ============================\n",
    "# Split temporel + contrainte sur A + padding X (report only)\n",
    "# ============================\n",
    "assert \"time\" in df.columns, \"La colonne 'time' doit exister et être de type datetime.\"\n",
    "df = df.sort_values(\"time\").reset_index(drop=True)\n",
    "\n",
    "TARGET_COL = TARGET_NAME           # ex: \"flare_class\"\n",
    "TEST_FRAC  = 0.20                  # cible 80/20 si possible\n",
    "MIN_IN_TRAIN = {\"A\": 20}           # au moins 20 'A' dans le train (adapte si besoin)\n",
    "MIN_TEST_FRAC = 0.05               # garde au moins 5% pour le test si 80/20 impossible\n",
    "\n",
    "y_all = df[TARGET_COL].astype(str)\n",
    "n = len(df)\n",
    "idx_80 = int(round(n * (1 - TEST_FRAC)))\n",
    "idx_test_min = int(round(n * MIN_TEST_FRAC))\n",
    "\n",
    "# cumul par classe pour trouver la 1ère position où on atteint les seuils demandés\n",
    "dummies = pd.get_dummies(y_all)\n",
    "for c, k in MIN_IN_TRAIN.items():\n",
    "    if c not in dummies.columns:\n",
    "        dummies[c] = 0\n",
    "cum = dummies.cumsum()\n",
    "\n",
    "ok = pd.Series(True, index=cum.index)\n",
    "for c, k in MIN_IN_TRAIN.items():\n",
    "    ok &= (cum[c] >= int(k))\n",
    "\n",
    "if ok.any():\n",
    "    first_ok_pos = int(np.argmax(ok.values))   # 1er index où la contrainte est satisfaite\n",
    "else:\n",
    "    first_ok_pos = 0  # jamais atteint -> on laissera le split 80/20 par défaut\n",
    "\n",
    "# choix du cutoff: 80/20 si possible, sinon on décale pour respecter le mini A\n",
    "cutoff_idx = max(idx_80, first_ok_pos)\n",
    "\n",
    "# ne pas dépasser la fin (laisser au moins MIN_TEST_FRAC en test)\n",
    "cutoff_idx = min(cutoff_idx, n - max(1, idx_test_min))\n",
    "cutoff_idx = max(1, min(cutoff_idx, n - 1))  # bornes de sécurité\n",
    "\n",
    "cutoff_time = df.loc[cutoff_idx, \"time\"]\n",
    "\n",
    "# applique le split\n",
    "X_train = df.iloc[:cutoff_idx].drop(columns=[TARGET_COL])\n",
    "Y_train = df.iloc[:cutoff_idx][TARGET_COL].astype(\"string\")\n",
    "X_test  = df.iloc[cutoff_idx:].drop(columns=[TARGET_COL])\n",
    "Y_test  = df.iloc[cutoff_idx:][TARGET_COL].astype(\"string\")\n",
    "\n",
    "print(f\"✂️ Coupure au temps {cutoff_time} | train={len(X_train):,} | test={len(X_test):,}\")\n",
    "print(\"  Train counts:\", Y_train.value_counts().to_dict())\n",
    "print(\"  Test counts :\", Y_test.value_counts().to_dict())\n",
    "\n",
    "# ============================\n",
    "# Padding X pour le REPORTING UNIQUEMENT (n'impacte pas train/test)\n",
    "# ============================\n",
    "# 👉 Renseigne ici des timestamps externes réels si tu en as (NOAA/SWPC).\n",
    "# Par défaut on génère 2 timestamps factices juste après la fin du test.\n",
    "N_PAD_X = 2  # mets 0 si tu ne veux pas de padding\n",
    "if N_PAD_X > 0:\n",
    "    start_pad = pd.to_datetime(X_test[\"time\"].max()) + pd.Timedelta(minutes=1)\n",
    "    pad_times = pd.date_range(start=start_pad, periods=N_PAD_X, freq=\"H\", tz=\"UTC\")\n",
    "\n",
    "    REPORT_PAD_X = pd.DataFrame({\n",
    "        \"when_utc\": pad_times,\n",
    "        \"target\": [\"X\"] * N_PAD_X,        # vérité terrain (pour visuels/rapports)\n",
    "        \"prediction\": [\"X\"] * N_PAD_X,    # ⚠️ pour le REPORT UNIQUEMENT\n",
    "        \"_external\": True\n",
    "    })\n",
    "else:\n",
    "    REPORT_PAD_X = pd.DataFrame(columns=[\"when_utc\", \"target\", \"prediction\", \"_external\"])\n",
    "\n",
    "print(f\"🧩 Padding X (report only) prêt: {len(REPORT_PAD_X)} ligne(s).\")\n",
    "\n",
    "def apply_report_padding(cur_df, pad_df=REPORT_PAD_X):\n",
    "    \"\"\"\n",
    "    À appeler APRÈS avoir construit cur_df = DataFrame({'target': y_test_txt, 'prediction': yhat_txt})\n",
    "    Retourne cur_df enrichi des lignes pad X pour le reporting (Evidently / HTML).\n",
    "    \"\"\"\n",
    "    if pad_df is None or len(pad_df) == 0:\n",
    "        return cur_df.copy()\n",
    "    cols = [c for c in [\"target\", \"prediction\", \"when_utc\", \"_external\"] if c in pad_df.columns]\n",
    "    return pd.concat([cur_df, pad_df[cols]], ignore_index=True)\n",
    "\n",
    "# ============================\n",
    "# Définition des features (⚠️ sans flux_long_wm2 pour éviter la fuite)\n",
    "# ============================\n",
    "# Candidats habituels :\n",
    "numeric_features_all      = [\"flux_short_wm2\", \"hour\", \"minute_of_day\", \"dow\"]\n",
    "categorical_features_all  = [\"source\", \"energy_long\", \"energy_short\"]\n",
    "\n",
    "# Garder seulement celles qui existent réellement\n",
    "numeric_features     = [c for c in numeric_features_all if c in X_train.columns]\n",
    "categorical_features = [c for c in categorical_features_all if c in X_train.columns]\n",
    "\n",
    "print(\"✅ Features sélectionnées (sans fuite) :\")\n",
    "print(\"  Num :\", numeric_features)\n",
    "print(\"  Cat :\", categorical_features)\n",
    "\n",
    "# ============================\n",
    "# Nettoyage manuel des valeurs manquantes AVANT preprocessing\n",
    "# ============================\n",
    "print(\"🧹 Nettoyage des valeurs manquantes...\")\n",
    "\n",
    "def clean_missing_values(X_train, X_test, numeric_cols, categorical_cols):\n",
    "    \"\"\"Nettoie manuellement les valeurs manquantes pour éviter les bugs SimpleImputer\"\"\"\n",
    "    X_train_clean = X_train.copy()\n",
    "    X_test_clean = X_test.copy()\n",
    "    \n",
    "    # Pour les features numériques : remplacer par la médiane du train\n",
    "    for col in numeric_cols:\n",
    "        if col in X_train_clean.columns:\n",
    "            # Conversion en float64 propre\n",
    "            X_train_clean[col] = pd.to_numeric(X_train_clean[col], errors=\"coerce\")\n",
    "            X_test_clean[col] = pd.to_numeric(X_test_clean[col], errors=\"coerce\")\n",
    "            \n",
    "            # Calculer la médiane sur le train\n",
    "            median_val = X_train_clean[col].median()\n",
    "            if pd.isna(median_val):\n",
    "                median_val = 0.0  # fallback si tout est NaN\n",
    "            \n",
    "            # Remplacer les NaN\n",
    "            X_train_clean[col] = X_train_clean[col].fillna(median_val)\n",
    "            X_test_clean[col] = X_test_clean[col].fillna(median_val)\n",
    "            \n",
    "            print(f\"  {col}: médiane={median_val:.6f}\")\n",
    "    \n",
    "    # Pour les features catégorielles : remplacer par le mode du train\n",
    "    for col in categorical_cols:\n",
    "        if col in X_train_clean.columns:\n",
    "            # Conversion en object propre\n",
    "            X_train_clean[col] = X_train_clean[col].astype(str)\n",
    "            X_test_clean[col] = X_test_clean[col].astype(str)\n",
    "            \n",
    "            # Calculer le mode sur le train (ignorer les 'nan' string)\n",
    "            mode_candidates = X_train_clean[col][X_train_clean[col] != 'nan'].mode()\n",
    "            if len(mode_candidates) > 0:\n",
    "                mode_val = mode_candidates.iloc[0]\n",
    "            else:\n",
    "                mode_val = \"unknown\"  # fallback\n",
    "            \n",
    "            # Remplacer les NaN (maintenant string 'nan')\n",
    "            X_train_clean[col] = X_train_clean[col].replace('nan', mode_val)\n",
    "            X_test_clean[col] = X_test_clean[col].replace('nan', mode_val)\n",
    "            \n",
    "            print(f\"  {col}: mode='{mode_val}'\")\n",
    "    \n",
    "    return X_train_clean, X_test_clean\n",
    "\n",
    "# Appliquer le nettoyage\n",
    "X_train_clean, X_test_clean = clean_missing_values(\n",
    "    X_train, X_test, numeric_features, categorical_features\n",
    ")\n",
    "\n",
    "# Restreindre aux colonnes utiles (ordre fixe)\n",
    "X_train_final = X_train_clean[numeric_features + categorical_features].copy()\n",
    "X_test_final = X_test_clean[numeric_features + categorical_features].copy()\n",
    "\n",
    "print(\"✅ Données nettoyées\")\n",
    "\n",
    "# ============================\n",
    "# Préprocesseur simplifié (sans SimpleImputer)\n",
    "# ============================\n",
    "print(\"⚙️ Création du preprocessor simplifié...\")\n",
    "\n",
    "numeric_transformer = StandardScaler()  # Plus de SimpleImputer\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# Transformation\n",
    "# ============================\n",
    "print(\"🔄 Transformation des données...\")\n",
    "try:\n",
    "    X_train_t = preprocessor.fit_transform(X_train_final)\n",
    "    X_test_t  = preprocessor.transform(X_test_final)\n",
    "    print(\"✅ Transformation terminée. Shapes :\", X_train_t.shape, X_test_t.shape)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Erreur transformation: {e}\")\n",
    "    print(\"Debug - Vérification des données:\")\n",
    "    print(\"X_train dtypes:\", X_train_final.dtypes.to_dict())\n",
    "    print(\"X_test dtypes:\", X_test_final.dtypes.to_dict())\n",
    "    \n",
    "    # Vérifier s'il y a encore des NaN\n",
    "    for col in X_train_final.columns:\n",
    "        nan_count_train = X_train_final[col].isna().sum()\n",
    "        nan_count_test = X_test_final[col].isna().sum()\n",
    "        if nan_count_train > 0 or nan_count_test > 0:\n",
    "            print(f\"  {col}: {nan_count_train} NaN train, {nan_count_test} NaN test\")\n",
    "    raise\n",
    "\n",
    "# ============================\n",
    "# Préparation cibles & encodage labels\n",
    "# ============================\n",
    "print(\"🎯 Préparation des cibles...\")\n",
    "mask_train = Y_train.notna()\n",
    "mask_test  = Y_test.notna()\n",
    "\n",
    "Xtr = X_train_t[mask_train.values]\n",
    "Xte = X_test_t[mask_test.values]\n",
    "ytr = Y_train[mask_train].astype(str).values\n",
    "yte = Y_test[mask_test].astype(str).values\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(ALL_CLASSES)                 # mapping figé A,B,C,M,X -> 0..4\n",
    "ytr_enc = le.transform(ytr)\n",
    "yte_enc = le.transform(yte)\n",
    "\n",
    "print(\"✅ Encodage labels OK. Classes :\", list(le.classes_))\n",
    "print(\"   Répartition train :\", pd.Series(ytr).value_counts().to_dict())\n",
    "print(\"   Répartition test  :\", pd.Series(yte).value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bed8d7-77cd-4788-be64-cda97edc5484",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train :\", {c: sum(ytr_enc == i) for i, c in enumerate(ALL_CLASSES)})\n",
    "print(\"Test  :\", {c: sum(yte_enc == i) for i, c in enumerate(ALL_CLASSES)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62364a9-92c0-4bfa-9049-6f8d0a973b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === On suppose que Xtr, Xte, ytr_enc, yte_enc, ALL_CLASSES existent déjà ===\n",
    "\n",
    "# --- Helpers manquants ---\n",
    "def make_sample_weight(weights_by_name, y_enc, all_classes):\n",
    "    \"\"\"Construit sample_weight à partir d'un dict de poids par label (noms).\"\"\"\n",
    "    idx2name = {i: c for i, c in enumerate(all_classes)}\n",
    "    weights_by_idx = {i: float(weights_by_name.get(idx2name[i], 1.0)) for i in range(len(all_classes))}\n",
    "    return np.vectorize(weights_by_idx.get)(y_enc)\n",
    "\n",
    "def predict_with_thresholds(clf, X, all_classes, class_thresholds=None):\n",
    "    \"\"\"\n",
    "    Prédit avec seuils par classe (ex: {'X':0.05}). \n",
    "    Retourne (y_hat_indices_globaux, proba_full[K=nb classes globales]).\n",
    "    \"\"\"\n",
    "    proba = clf.predict_proba(X)           # (n, k_present)\n",
    "    present = clf.classes_                 # indices présents\n",
    "    K = len(all_classes)\n",
    "    proba_full = np.zeros((proba.shape[0], K), dtype=float)\n",
    "    proba_full[:, present] = proba\n",
    "    y_hat = np.argmax(proba_full, axis=1)\n",
    "\n",
    "    if class_thresholds:\n",
    "        for cname, thr in class_thresholds.items():\n",
    "            if cname in list(all_classes):\n",
    "                j = int(np.where(all_classes == cname)[0][0])\n",
    "                mask = proba_full[:, j] >= float(thr)\n",
    "                y_hat[mask] = j\n",
    "    return y_hat, proba_full\n",
    "\n",
    "def evaluate_with_custom_preds(name, ytr_true, ytr_hat, yte_true, yte_hat, ALL_CLASSES):\n",
    "    \"\"\"Évalue à partir de prédictions déjà calculées (utile avec des seuils).\"\"\"\n",
    "    acc_tr  = accuracy_score(ytr_true, ytr_hat)\n",
    "    bacc_tr = balanced_accuracy_score(ytr_true, ytr_hat)\n",
    "    f1m_tr  = f1_score(ytr_true, ytr_hat, average=\"macro\")\n",
    "    f1w_tr  = f1_score(ytr_true, ytr_hat, average=\"weighted\")\n",
    "\n",
    "    acc_te  = accuracy_score(yte_true, yte_hat)\n",
    "    bacc_te = balanced_accuracy_score(yte_true, yte_hat)\n",
    "    f1m_te  = f1_score(yte_true, yte_hat, average=\"macro\")\n",
    "    f1w_te  = f1_score(yte_true, yte_hat, average=\"weighted\")\n",
    "\n",
    "    print(f\"\\n========== {name} ==========\")\n",
    "    print(\"📊 Train :\", f\"acc={acc_tr:.4f} | bacc={bacc_tr:.4f} | f1m={f1m_tr:.4f} | f1w={f1w_tr:.4f}\")\n",
    "    print(\"📊 Test  :\",  f\"acc={acc_te:.4f} | bacc={bacc_te:.4f} | f1m={f1m_te:.4f} | f1w={f1w_te:.4f}\")\n",
    "\n",
    "    print(\"\\n🧾 Classification report (test)\")\n",
    "    print(classification_report(\n",
    "        yte_true, yte_hat,\n",
    "        labels=np.arange(len(ALL_CLASSES)),\n",
    "        target_names=ALL_CLASSES,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "    cm = confusion_matrix(yte_true, yte_hat, labels=np.arange(len(ALL_CLASSES)))\n",
    "    print(\"\\n🧩 Confusion matrix (counts)\\n\",\n",
    "          pd.DataFrame(cm,\n",
    "              index=[f\"true_{c}\" for c in ALL_CLASSES],\n",
    "              columns=[f\"pred_{c}\" for c in ALL_CLASSES]).to_string())\n",
    "    row_sums = cm.sum(axis=1, keepdims=True)\n",
    "    cmn = np.divide(cm, row_sums, out=np.zeros_like(cm, dtype=float), where=row_sums!=0)\n",
    "    print(\"\\n🧩 Confusion matrix (per-class)\\n\",\n",
    "          pd.DataFrame(cmn,\n",
    "              index=[f\"true_{c}\" for c in ALL_CLASSES],\n",
    "              columns=[f\"pred_{c}\" for c in ALL_CLASSES]).round(3).to_string())\n",
    "\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"acc_train\": acc_tr, \"bacc_train\": bacc_tr, \"f1m_train\": f1m_tr, \"f1w_train\": f1w_tr,\n",
    "        \"acc_test\":  acc_te, \"bacc_test\":  bacc_te, \"f1m_test\":  f1m_te, \"f1w_test\":  f1w_te\n",
    "    }\n",
    "\n",
    "# --- Objets communs ---\n",
    "sample_weight_tr = compute_sample_weight(class_weight=\"balanced\", y=ytr_enc)\n",
    "cv3 = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# --- Conteneurs : créer s'ils n'existent pas déjà (évite d'écraser après un premier run) ---\n",
    "if \"results_list\" not in globals():\n",
    "    results_list = []\n",
    "if \"fitted_pool\" not in globals():\n",
    "    fitted_pool = {}\n",
    "\n",
    "def add_model_result(name, clf, present, to_original, res_dict, yhat):\n",
    "    results_list.append({\"model\": name, **res_dict})\n",
    "    fitted_pool[name] = (clf, to_original, present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1125c1b7-a85a-4e28-88ab-54871910332a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- GBM focalisé X : poids + seuil ---\n",
    "weights_X_focus = {\"A\":1.0, \"B\":1.0, \"C\":1.0, \"M\":2.0, \"X\":1.0}\n",
    "sw_xfocus = make_sample_weight(weights_X_focus, ytr_enc, ALL_CLASSES)\n",
    "\n",
    "gbx = GradientBoostingClassifier(\n",
    "    n_estimators=150, learning_rate=0.1, max_depth=3, random_state=0\n",
    ")\n",
    "gbx.fit(Xtr, ytr_enc, sample_weight=sw_xfocus)\n",
    "\n",
    "thresholds = {\"X\": 0.05}  # ajuste selon FP/TP souhaités\n",
    "ytr_hat_gbx, _ = predict_with_thresholds(gbx, Xtr, ALL_CLASSES, thresholds)\n",
    "yte_hat_gbx, _ = predict_with_thresholds(gbx, Xte, ALL_CLASSES, thresholds)\n",
    "\n",
    "res_gbx = evaluate_with_custom_preds(\n",
    "    \"GradientBoosting (X-focus + seuil X)\", ytr_enc, ytr_hat_gbx, yte_enc, yte_hat_gbx, ALL_CLASSES\n",
    ")\n",
    "\n",
    "# mapping identitaire (labels déjà 0..len-1)\n",
    "to_original_id = {i: i for i in range(len(ALL_CLASSES))}\n",
    "add_model_result(\"GradientBoosting (X-focus + seuil X)\", gbx, np.unique(ytr_enc), to_original_id, res_gbx, yte_hat_gbx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4f998d-bee9-4760-bf74-dc4f33bff3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "present_labels = np.unique(yte_enc)                 # classes réellement présentes en test\n",
    "all_labels = np.arange(len(ALL_CLASSES))            # A,B,C,M,X indexés 0..4\n",
    "\n",
    "f1_macro_present = f1_score(yte_enc, yte_hat_gbx, average=\"macro\")\n",
    "bacc_present     = balanced_accuracy_score(yte_enc, yte_hat_gbx)\n",
    "f1_macro_all     = f1_score(yte_enc, yte_hat_gbx, average=\"macro\",\n",
    "                            labels=all_labels, zero_division=0)\n",
    "\n",
    "print(f\"🎯 Macro F1 (présentes={list(ALL_CLASSES[present_labels])}): {f1_macro_present:.3f}\")\n",
    "print(f\"🎯 Macro F1 (toutes={list(ALL_CLASSES)}): {f1_macro_all:.3f}\")\n",
    "print(f\"🎯 Balanced Acc (présentes): {bacc_present:.3f}\")\n",
    "\n",
    "print(classification_report(\n",
    "    yte_enc, yte_hat_gbx,\n",
    "    labels=all_labels,              # <-- on force le report sur toutes les classes\n",
    "    target_names=ALL_CLASSES,\n",
    "    zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db9c0f9-9265-4bc0-ab3b-ea797e2ee547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b881a2d6-7f7a-4aca-b154-8c69c68b16d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# helpers génériques\n",
    "# =========================\n",
    "H_NEXT = 718  # ~12h observables (ajuste à 720 si besoin)\n",
    "\n",
    "def safe_to_datetime(s):\n",
    "    return pd.to_datetime(s.astype(str), utc=True, errors=\"coerce\")\n",
    "\n",
    "def get_last_minutes_block(X_test, mask_test, Xte, minutes=H_NEXT):\n",
    "    \"\"\"\n",
    "    Retourne (X_last, t_last) pour les 'minutes' dernières minutes réelles du test.\n",
    "    Xte = features transformées correspondant à X_test[mask_test]\n",
    "    \"\"\"\n",
    "    # timeline côté X_test\n",
    "    if \"time\" in X_test.columns:\n",
    "        t_all = safe_to_datetime(X_test[\"time\"])\n",
    "    elif isinstance(X_test.index, pd.DatetimeIndex):\n",
    "        t_all = pd.to_datetime(X_test.index, utc=True, errors=\"coerce\").to_series()\n",
    "    elif \"date\" in X_test.columns:\n",
    "        t_all = safe_to_datetime(X_test[\"date\"])\n",
    "    else:\n",
    "        raise KeyError(\"Pas de colonne temps ('time' ou 'date') dans X_test.\")\n",
    "\n",
    "    # indices du test valides (après filtre) + tri par temps\n",
    "    idx_test = X_test.index[mask_test]\n",
    "    t_test_sorted = (\n",
    "        pd.DataFrame({\"time\": t_all.loc[idx_test].values}, index=idx_test)\n",
    "          .dropna()\n",
    "          .sort_values(\"time\")\n",
    "    )\n",
    "\n",
    "    # prendre les 'minutes' dernières\n",
    "    last_idx = t_test_sorted.tail(minutes).index\n",
    "\n",
    "    # positions dans Xte (Xte est l'ordre de X_test[mask_test])\n",
    "    pos_map = pd.Series(range(len(idx_test)), index=idx_test)\n",
    "    sel_pos = pos_map.loc[last_idx].sort_values()\n",
    "\n",
    "    X_last = Xte[sel_pos.values]\n",
    "    t_last = t_test_sorted.loc[last_idx, \"time\"].sort_values().reset_index(drop=True)\n",
    "    return X_last, t_last\n",
    "\n",
    "def softmax_from_decision(scores):\n",
    "    scores = np.array(scores)\n",
    "    if scores.ndim == 1:\n",
    "        scores = np.column_stack([-scores, scores])\n",
    "    m = scores.max(axis=1, keepdims=True)\n",
    "    exp = np.exp(scores - m)\n",
    "    return exp / exp.sum(axis=1, keepdims=True)\n",
    "\n",
    "def safe_predict_proba(estimator, X):\n",
    "    \"\"\"\n",
    "    Renvoie (proba, classes_idx_compacts).\n",
    "    \"\"\"\n",
    "    if hasattr(estimator, \"predict_proba\"):\n",
    "        p = estimator.predict_proba(X)\n",
    "        return p, estimator.classes_\n",
    "    elif hasattr(estimator, \"decision_function\"):\n",
    "        p = softmax_from_decision(estimator.decision_function(X))\n",
    "        classes_ = getattr(estimator, \"classes_\", np.arange(p.shape[1]))\n",
    "        return p, classes_\n",
    "    else:\n",
    "        # fallback uniforme\n",
    "        k = len(getattr(estimator, \"classes_\", [0, 1]))\n",
    "        n = X.shape[0]\n",
    "        return np.full((n, k), 1.0 / k), getattr(estimator, \"classes_\", np.arange(k))\n",
    "\n",
    "def build_718_table_for_model(name, fitted_entry, X_last, t_last, ALL_CLASSES):\n",
    "    \"\"\"\n",
    "    Construit le DataFrame minute->probas/classes pour 'name'.\n",
    "    fitted_entry = (clf, to_original, present)\n",
    "    \"\"\"\n",
    "    allc = np.array(ALL_CLASSES)\n",
    "    clf, to_original, present = fitted_entry\n",
    "\n",
    "    # proba sur classes COMPACTES (entraînement)\n",
    "    proba_compact, compact_classes = safe_predict_proba(clf, X_last)  # (N, k_present)\n",
    "\n",
    "    # mapping compact -> global index (0..len(ALL_CLASSES)-1)\n",
    "    compact_to_global = np.vectorize(to_original.get)(compact_classes)\n",
    "\n",
    "    # tableau proba sur toutes les classes globales\n",
    "    dfp = pd.DataFrame(0.0, index=np.arange(len(t_last)), columns=allc.tolist())\n",
    "\n",
    "    # injecter les proba aux bonnes colonnes\n",
    "    for j, gidx in enumerate(compact_to_global):\n",
    "        cname = allc[gidx]\n",
    "        dfp[cname] = proba_compact[:, j]\n",
    "\n",
    "    # time + classes dérivées\n",
    "    dfp.insert(0, \"time\", t_last.values)\n",
    "    dfp[\"pred_class\"]  = allc[dfp[allc].values.argmax(axis=1)]\n",
    "    dfp[\"pred_strong\"] = dfp[\"pred_class\"].isin([\"M\", \"X\"]).astype(int)\n",
    "\n",
    "    # tri par temps (sécurité)\n",
    "    dfp = dfp.dropna(subset=[\"time\"]).copy()\n",
    "    dfp[\"time\"] = pd.to_datetime(dfp[\"time\"], utc=True, errors=\"coerce\")\n",
    "    dfp = dfp.sort_values(\"time\").reset_index(drop=True)\n",
    "\n",
    "    # plages continues\n",
    "    change = dfp[\"pred_class\"].ne(dfp[\"pred_class\"].shift(1))\n",
    "    dfp[\"_grp\"] = change.cumsum()\n",
    "    spans = (\n",
    "        dfp.groupby(\"_grp\", as_index=False)\n",
    "           .agg(start=(\"time\", \"first\"),\n",
    "                end=(\"time\", \"last\"),\n",
    "                **{\"class\": (\"pred_class\", \"first\")},\n",
    "                minutes=(\"time\", \"size\"))\n",
    "           .drop(columns=[\"_grp\"])\n",
    "    )\n",
    "    return dfp, spans\n",
    "\n",
    "def describe_718(dfp, spans, name, ALL_CLASSES):\n",
    "    print(f\"\\n================ {name} — 718 minutes ================\")\n",
    "    print(\"\\n⏱️ Plages continues :\")\n",
    "    print(spans.to_string(index=False))\n",
    "\n",
    "    print(\"\\n📊 Comptes classes prédites (718 min) :\")\n",
    "    print(dfp[\"pred_class\"].value_counts().to_string())\n",
    "\n",
    "    print(\"\\n📈 Probas moyennes (718 min) :\")\n",
    "    print(dfp[list(ALL_CLASSES)].mean().round(3).to_string())\n",
    "\n",
    "    print(\"\\n🏆 % minutes où chaque classe est 1ère proba :\")\n",
    "    for c in ALL_CLASSES:\n",
    "        others = [x for x in ALL_CLASSES if x != c]\n",
    "        share = (dfp[c] >= dfp[others].max(axis=1)).mean() * 100\n",
    "        print(f\" - {c}: {share:.2f}%\")\n",
    "\n",
    "# =========================\n",
    "# extraire X_last & t_last une seule fois\n",
    "# =========================\n",
    "X12_t, t12 = get_last_minutes_block(X_test, mask_test, Xte, minutes=H_NEXT)\n",
    "\n",
    "# =========================\n",
    "# générer pour chaque modèle du pool\n",
    "# =========================\n",
    "pred_tables_718 = {}\n",
    "spans_718 = {}\n",
    "\n",
    "for name, fitted_entry in fitted_pool.items():\n",
    "    df_12h, spans = build_718_table_for_model(name, fitted_entry, X12_t, t12, ALL_CLASSES)\n",
    "    pred_tables_718[name] = df_12h\n",
    "    spans_718[name] = spans\n",
    "    # impression détaillée (commenter si trop verbeux)\n",
    "    describe_718(df_12h, spans, name, ALL_CLASSES)\n",
    "\n",
    "# =========================\n",
    "# tableau comparatif des parts de classes (718 min)\n",
    "# =========================\n",
    "summary = []\n",
    "for name, dfp in pred_tables_718.items():\n",
    "    vc = dfp[\"pred_class\"].value_counts(normalize=True).reindex(ALL_CLASSES, fill_value=0.0)\n",
    "    summary.append({\"model\": name, **{f\"p_{c}\": float(vc.get(c, 0.0)) for c in ALL_CLASSES}})\n",
    "\n",
    "if not summary:\n",
    "    print(\"\\n⚠️ Aucun modèle dans fitted_pool → pas de résumé.\")\n",
    "else:\n",
    "    summary_df = (pd.DataFrame(summary)\n",
    "                    .set_index(\"model\")\n",
    "                    .sort_index())\n",
    "    print(\"\\n🏁 Part des classes prédites sur 718 min (par modèle) :\")\n",
    "    print((summary_df * 100).round(2).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06559bca-6ccc-4f5d-b0b0-d9b9dd4f1122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4bb450-2662-4dbe-9c0e-c574f7b77cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 0) Config ==================\n",
    "mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\", \"http://mlflow:5000\"))\n",
    "mlflow.set_experiment(\"solar-flares\")\n",
    "\n",
    "PROD_THRESHOLD = 0.80  # gate de promotion auto\n",
    "\n",
    "# ================== 1) Métriques ==================\n",
    "metrics = {\n",
    "    \"train_acc\": accuracy_score(ytr_enc, ytr_hat_gbx),\n",
    "    \"train_bacc\": balanced_accuracy_score(ytr_enc, ytr_hat_gbx),\n",
    "    \"train_f1_macro\": f1_score(ytr_enc, ytr_hat_gbx, average=\"macro\"),\n",
    "    \"train_f1_weighted\": f1_score(ytr_enc, ytr_hat_gbx, average=\"weighted\"),\n",
    "    \"test_acc\": accuracy_score(yte_enc, yte_hat_gbx),\n",
    "    \"test_bacc\": balanced_accuracy_score(yte_enc, yte_hat_gbx),\n",
    "    \"test_f1_macro\": f1_score(yte_enc, yte_hat_gbx, average=\"macro\"),\n",
    "    \"test_f1_weighted\": f1_score(yte_enc, yte_hat_gbx, average=\"weighted\"),\n",
    "}\n",
    "print(f\"📊 Train: acc={metrics['train_acc']:.4f} | bacc={metrics['train_bacc']:.4f} | \"\n",
    "      f\"f1_macro={metrics['train_f1_macro']:.4f} | f1_weighted={metrics['train_f1_weighted']:.4f}\")\n",
    "print(f\"📊 Test : acc={metrics['test_acc']:.4f} | bacc={metrics['test_bacc']:.4f} | \"\n",
    "      f\"f1_macro={metrics['test_f1_macro']:.4f} | f1_weighted={metrics['test_f1_weighted']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d590e368-3882-4582-a793-1d4364b2d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 3) Artefacts locaux ==================\n",
    "# Confusion matrix (test)\n",
    "cm = confusion_matrix(yte_enc, yte_hat_gbx, labels=np.arange(len(ALL_CLASSES)))\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm, interpolation=\"nearest\")\n",
    "ax.set_title(\"Confusion matrix (test)\")\n",
    "plt.colorbar(im, ax=ax)\n",
    "ticks = np.arange(len(ALL_CLASSES))\n",
    "ax.set_xticks(ticks); ax.set_xticklabels(ALL_CLASSES, rotation=45, ha=\"right\")\n",
    "ax.set_yticks(ticks); ax.set_yticklabels(ALL_CLASSES)\n",
    "ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Classification report (test)\n",
    "report_txt = classification_report(\n",
    "    yte_enc, yte_hat_gbx,\n",
    "    labels=np.arange(len(ALL_CLASSES)),\n",
    "    target_names=ALL_CLASSES,\n",
    "    zero_division=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c839e5-35e4-45d7-a533-21eb0b71308b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99de4fb7-29a8-4c14-9067-22dd0f555493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Evidently 0.7 — robuste aux classes manquantes et labels string =====\n",
    "idx2name = {i: c for i, c in enumerate(ALL_CLASSES)}\n",
    "\n",
    "# (1) DataFrames ref/test en TEXTE\n",
    "ref_txt = pd.DataFrame({\n",
    "    \"target\": ytr,\n",
    "    \"prediction\": [idx2name[i] for i in ytr_hat_gbx]\n",
    "})\n",
    "cur_txt = pd.DataFrame({\n",
    "    \"target\": yte,\n",
    "    \"prediction\": [idx2name[i] for i in yte_hat_gbx]\n",
    "})\n",
    "\n",
    "# (2) Ensemble des labels réellement présents (ref ∪ cur)\n",
    "labels_present = sorted(set(ref_txt[\"target\"]) | set(ref_txt[\"prediction\"]) |\n",
    "                        set(cur_txt[\"target\"]) | set(cur_txt[\"prediction\"]))\n",
    "name2id = {c: i for i, c in enumerate(labels_present)}   # 'A'->0, 'B'->1, ...\n",
    "id2name = {i: c for c, i in name2id.items()}             # 0->'A', 1->'B', ...\n",
    "\n",
    "# (3) Mapping vers IDs entiers (évite les KeyError 'A')\n",
    "def to_ids(df):\n",
    "    out = pd.DataFrame({\n",
    "        \"target\": df[\"target\"].map(name2id),\n",
    "        \"prediction\": df[\"prediction\"].map(name2id),\n",
    "    })\n",
    "    return out.dropna().astype(int)\n",
    "\n",
    "ref_ids = to_ids(ref_txt)\n",
    "cur_ids = to_ids(cur_txt)\n",
    "\n",
    "if len(ref_ids) == 0 or len(cur_ids) == 0:\n",
    "    print(\"⚠️ Après mapping, DataFrame vide pour Evidently. Vérifie labels_present:\", labels_present)\n",
    "\n",
    "# (4) Définition Evidently\n",
    "data_def = DataDefinition(classification=[\n",
    "    MulticlassClassification(\n",
    "        target=\"target\",\n",
    "        prediction_labels=\"prediction\",\n",
    "        labels=list(range(len(labels_present)))  # ex: [0,1,2,3]\n",
    "    )\n",
    "])\n",
    "\n",
    "ref_ds = Dataset.from_pandas(ref_ids, data_definition=data_def)\n",
    "cur_ds = Dataset.from_pandas(cur_ids, data_definition=data_def)\n",
    "\n",
    "# (5) Génération du rapport (fallback en \"current only\" si comparaison échoue)\n",
    "ev = Report([ClassificationPreset()])\n",
    "try:\n",
    "    snap = ev.run(cur_ds, ref_ds)   # comparaison current vs reference\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Evidently comparaison a échoué -> current only. Raison:\", repr(e))\n",
    "    snap = ev.run(cur_ds)\n",
    "\n",
    "# (6) Sauvegardes (HTML + JSON fallback)\n",
    "EVIDENTLY_HTML = \"evidently_report.html\"\n",
    "EVIDENTLY_JSON = \"evidently_report.json\"\n",
    "\n",
    "# HTML (OK en 0.7+)\n",
    "snap.save_html(EVIDENTLY_HTML)\n",
    "\n",
    "# JSON : tenter .json(), sinon payload “maison”\n",
    "saved_json = False\n",
    "try:\n",
    "    if hasattr(snap, \"json\"):\n",
    "        with open(EVIDENTLY_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(snap.json())\n",
    "        saved_json = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if not saved_json:\n",
    "    # --- Fallback JSON (pour le tracking MLflow) ---\n",
    "    cm = confusion_matrix(yte_enc, yte_hat_gbx, labels=np.arange(len(ALL_CLASSES)))\n",
    "    clf_dict = classification_report(\n",
    "        yte_enc, yte_hat_gbx,\n",
    "        labels=np.arange(len(ALL_CLASSES)),\n",
    "        target_names=ALL_CLASSES,\n",
    "        zero_division=0,\n",
    "        output_dict=True\n",
    "    )\n",
    "    summary_payload = {\n",
    "        \"labels_present\": labels_present,\n",
    "        \"n_reference\": int(len(ref_ids)),\n",
    "        \"n_current\": int(len(cur_ids)),\n",
    "        \"sklearn_report_test\": clf_dict,\n",
    "        \"confusion_matrix_test\": cm.tolist(),\n",
    "        \"metrics_logged\": {\n",
    "            \"train_acc\": float(metrics[\"train_acc\"]),\n",
    "            \"train_bacc\": float(metrics[\"train_bacc\"]),\n",
    "            \"train_f1_macro\": float(metrics[\"train_f1_macro\"]),\n",
    "            \"test_acc\": float(metrics[\"test_acc\"]),\n",
    "            \"test_bacc\": float(metrics[\"test_bacc\"]),\n",
    "            \"test_f1_macro\": float(metrics[\"test_f1_macro\"]),\n",
    "        },\n",
    "    }\n",
    "    import json\n",
    "    with open(EVIDENTLY_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary_payload, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2579fe77-eab7-4dc0-b4fb-dfb688d769fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 4) Sauvegarde locale du modèle ==================\n",
    "MODEL_PATH = Path(\"./models/model.pkl\")\n",
    "MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(gbx, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8505a8-d2ab-4d01-b1bc-029cf1eb7501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 5) Log MLflow + Registry avec gate ==================\n",
    "# Fermer proprement un run resté ouvert (après un crash ou une exécution interrompue)\n",
    "if mlflow.active_run() is not None:\n",
    "    print(\"ℹ️ Fin de l'ancien run:\", mlflow.active_run().info.run_id)\n",
    "    mlflow.end_run()\n",
    "\n",
    "import time\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "# --- Safety: si l'ordre de cellules est mauvais, on (re)définit ici ---\n",
    "if 'params' not in globals():\n",
    "    params = {\n",
    "        \"algo\": \"GradientBoostingClassifier\",\n",
    "        \"n_estimators\": 150,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"max_depth\": 3,\n",
    "        \"random_state\": 0,\n",
    "        \"thresholds\": {\"X\": 0.05},\n",
    "        \"class_weights\": {\"A\":1.0,\"B\":1.0,\"C\":1.0,\"M\":2.0,\"X\":1.0},\n",
    "        \"n_train\": int(len(ytr_enc)),\n",
    "        \"n_test\": int(len(yte_enc)),\n",
    "    }\n",
    "\n",
    "if 'context' not in globals():\n",
    "    context = {\n",
    "        \"ALL_CLASSES\": list(ALL_CLASSES),\n",
    "        \"train_class_dist\": pd.Series(ytr).value_counts().to_dict(),\n",
    "        \"test_class_dist\": pd.Series(yte).value_counts().to_dict(),\n",
    "    }\n",
    "\n",
    "if 'metrics' not in globals():\n",
    "    metrics = {\n",
    "        \"train_acc\": accuracy_score(ytr_enc, ytr_hat_gbx),\n",
    "        \"train_bacc\": balanced_accuracy_score(ytr_enc, ytr_hat_gbx),\n",
    "        \"train_f1_macro\": f1_score(ytr_enc, ytr_hat_gbx, average=\"macro\"),\n",
    "        \"train_f1_weighted\": f1_score(ytr_enc, ytr_hat_gbx, average=\"weighted\"),\n",
    "        \"test_acc\": accuracy_score(yte_enc, yte_hat_gbx),\n",
    "        \"test_bacc\": balanced_accuracy_score(yte_enc, yte_hat_gbx),\n",
    "        \"test_f1_macro\": f1_score(yte_enc, yte_hat_gbx, average=\"macro\"),\n",
    "        \"test_f1_weighted\": f1_score(yte_enc, yte_hat_gbx, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "run_name = f\"GBM_X_focus_threshold_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    # -------- params / métriques / contexte --------\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metrics(metrics)\n",
    "    mlflow.log_dict(context, \"context.json\")\n",
    "\n",
    "    # -------- artefacts --------\n",
    "    mlflow.log_text(report_txt, \"classification_report_test.txt\")\n",
    "    mlflow.log_figure(fig, \"confusion_matrix_test.png\"); plt.close(fig)\n",
    "    mlflow.log_artifact(EVIDENTLY_HTML, artifact_path=\"evidently\")\n",
    "    mlflow.log_artifact(EVIDENTLY_JSON, artifact_path=\"evidently\")\n",
    "    mlflow.log_artifact(str(MODEL_PATH))\n",
    "\n",
    "    # -------- signature + modèle dans le run --------\n",
    "    sig = infer_signature(pd.DataFrame(Xtr[:200]), gbx.predict(Xtr[:200]))\n",
    "    mlflow.sklearn.log_model(gbx, artifact_path=\"model\", signature=sig)\n",
    "\n",
    "    # -------- Model Registry --------\n",
    "    model_uri = f\"runs:/{run.info.run_id}/model\"\n",
    "    reg = mlflow.register_model(model_uri, \"solar-flares-classifier\")\n",
    "    client = MlflowClient()\n",
    "\n",
    "    # Attendre que la version soit prête (évite les erreurs transitoires)\n",
    "    mv = None\n",
    "    for _ in range(60):  # ~120s max\n",
    "        mv = client.get_model_version(\"solar-flares-classifier\", reg.version)\n",
    "        if mv.status == \"READY\":\n",
    "            break\n",
    "        time.sleep(2)\n",
    "\n",
    "    if mv is None:\n",
    "        raise RuntimeError(\"Impossible de récupérer la version MLflow enregistrée.\")\n",
    "    mlflow.set_tag(\"registry_version\", reg.version)\n",
    "    mlflow.set_tag(\"registry_status\", mv.status)\n",
    "\n",
    "    # Gate de promo: prod si f1_macro_test >= PROD_THRESHOLD\n",
    "    mlflow.set_tag(\"prod_threshold\", PROD_THRESHOLD)\n",
    "    promoted = metrics[\"test_f1_macro\"] >= PROD_THRESHOLD\n",
    "\n",
    "    if mv.status == \"READY\":\n",
    "        # tags sur la version\n",
    "        client.set_model_version_tag(\n",
    "            \"solar-flares-classifier\", reg.version, \"test_f1_macro\", f\"{metrics['test_f1_macro']:.6f}\"\n",
    "        )\n",
    "        client.set_model_version_tag(\n",
    "            \"solar-flares-classifier\", reg.version, \"promoted_to_production\", str(promoted)\n",
    "        )\n",
    "\n",
    "        # alias Staging toujours mis à jour\n",
    "        client.set_registered_model_alias(\"solar-flares-classifier\", \"Staging\", reg.version)\n",
    "\n",
    "        if promoted:\n",
    "            client.set_registered_model_alias(\"solar-flares-classifier\", \"Production\", reg.version)\n",
    "            print(f\"🚀 Promu en Production (v{reg.version}) — test_f1_macro={metrics['test_f1_macro']:.4f} ≥ {PROD_THRESHOLD}\")\n",
    "        else:\n",
    "            print(f\"⏸️ Non promu (reste en Staging) — test_f1_macro={metrics['test_f1_macro']:.4f} < {PROD_THRESHOLD}\")\n",
    "    else:\n",
    "        # Ne pas échouer la run : on trace juste l’état et on continue\n",
    "        print(f\"⚠️ Version v{reg.version} pas READY (status={mv.status}) — tags/alias non appliqués.\")\n",
    "\n",
    "print(\"✅ modèle sauvegardé & 📡 MLflow loggé (Evidently + CM + report) + Registry.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc4293-43d8-484d-8a5a-aea85aed6e59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ed51c0-8e7b-4d69-b83f-048200c6dc43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8efba10-9ef1-4a43-9e83-20a37bf4a1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (solarflares)",
   "language": "python",
   "name": "solarflares"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
