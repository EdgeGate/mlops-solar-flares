{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb4b18a2-86d5-431d-8f5c-91fdcdfabfa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json, time\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, f1_score,\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import os, mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models.signature import infer_signature\n",
    "from evidently import Report, Dataset, DataDefinition, MulticlassClassification\n",
    "from evidently.presets import ClassificationPreset\n",
    "import time\n",
    "import tempfile, json, os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9d14b18-8d91-4888-b40e-3de307fbb2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Chargement du fichier xrs_clean.parquet...\n",
      "üì• Reading: \\opt\\project\\data\\xrs_clean.parquet\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '\\\\opt\\\\project\\\\data\\\\xrs_clean.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m parquet_path \u001b[38;5;241m=\u001b[39m ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxrs_clean.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müì• Reading:\u001b[39m\u001b[38;5;124m\"\u001b[39m, parquet_path)\n\u001b[1;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparquet_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Donn√©es charg√©es : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m lignes, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m colonnes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mT:\\Anaconda\\envs\\solarflares\\lib\\site-packages\\pandas\\io\\parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    668\u001b[0m     path,\n\u001b[0;32m    669\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    670\u001b[0m     filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[0;32m    671\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    672\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[0;32m    673\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    674\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    676\u001b[0m )\n",
      "File \u001b[1;32mT:\\Anaconda\\envs\\solarflares\\lib\\site-packages\\pandas\\io\\parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[0;32m    275\u001b[0m         path_or_handle,\n\u001b[0;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    280\u001b[0m     )\n",
      "File \u001b[1;32mT:\\Anaconda\\envs\\solarflares\\lib\\site-packages\\pandas\\io\\parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mT:\\Anaconda\\envs\\solarflares\\lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '\\\\opt\\\\project\\\\data\\\\xrs_clean.parquet'"
     ]
    }
   ],
   "source": [
    "print(\"üì• Chargement du fichier xrs_clean.parquet...\")\n",
    "ROOT = Path(os.environ.get(\"PROJECT_ROOT\", \"/opt/project\"))\n",
    "\n",
    "# --- Lecture\n",
    "parquet_path = ROOT / \"data\" / \"xrs_clean.parquet\"\n",
    "print(\"üì• Reading:\", parquet_path)\n",
    "df = pd.read_parquet(parquet_path, engine=\"pyarrow\")\n",
    "print(f\"‚úÖ Donn√©es charg√©es : {df.shape[0]} lignes, {df.shape[1]} colonnes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b84587a-232a-4029-bbd0-0ea2bc21e52b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quickpeek(df, topn=10):\n",
    "\n",
    "    print(\"# head:\", df.head)\n",
    "    print(\"\\n# dtypes:\\n\", df.dtypes)\n",
    "    print(\"\\n# describe:\", df.describe)\n",
    "\n",
    "    # missing %\n",
    "    print(\"\\n# missing (%):\")\n",
    "    miss = (df.isna().mean()*100).round(2).sort_values(ascending=False)\n",
    "    print(miss.head(topn).to_string())\n",
    "\n",
    "    if \"time\" in df:\n",
    "        # conversion robuste: tente direct, sinon passe par string\n",
    "        try:\n",
    "            t = pd.to_datetime(df[\"time\"], utc=True, errors=\"coerce\")\n",
    "        except Exception:\n",
    "            t = pd.to_datetime(df[\"time\"].astype(str), utc=True, errors=\"coerce\")\n",
    "\n",
    "        print(\"\\n# time range:\", t.min(), \"->\", t.max())\n",
    "\n",
    "        t_valid = t.dropna()\n",
    "        print(\"# time monotonic:\", t_valid.is_monotonic_increasing)\n",
    "\n",
    "        # comptage par jour sans d√©pendre du backend Arrow\n",
    "        try:\n",
    "            per_day = t.dt.floor(\"D\").value_counts().sort_index()\n",
    "        except Exception:\n",
    "            # fallback: utiliser la colonne 'date' si dispo\n",
    "            if \"date\" in df.columns:\n",
    "                per_day = pd.to_datetime(df[\"date\"], errors=\"coerce\").value_counts().sort_index()\n",
    "            else:\n",
    "                per_day = pd.Series(dtype=\"int64\")\n",
    "\n",
    "        if len(per_day):\n",
    "            print(\"\\n# last days (rows/day):\\n\", per_day.tail(10).to_string())\n",
    "\n",
    "\n",
    "quickpeek(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be565e26-3422-49c6-b772-1ec3fd1a3418",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_NAME  = \"flare_class\"\n",
    "ALL_CLASSES  = np.array([\"A\", \"B\", \"C\", \"M\", \"X\"], dtype=object)\n",
    "print(\"üõ† Cr√©ation de la variable cible 'flare_class'...\")\n",
    "\n",
    "def rule_predict(flux):\n",
    "    \"\"\"\n",
    "    Classe une √©ruption selon le pic de flux X (W/m¬≤, 1-8 √Ö) \n",
    "    en utilisant les seuils NOAA officiels, avec A inclus.\n",
    "    \"\"\"\n",
    "    if pd.isna(flux):\n",
    "        return None\n",
    "    elif flux < 1e-7:       # A : < 10‚Åª‚Å∑ W/m¬≤\n",
    "        return \"A\"\n",
    "    elif flux < 1e-6:       # B : 10‚Åª‚Å∑ ‚â§ flux < 10‚Åª‚Å∂\n",
    "        return \"B\"\n",
    "    elif flux < 1e-5:       # C : 10‚Åª‚Å∂ ‚â§ flux < 10‚Åª‚Åµ\n",
    "        return \"C\"\n",
    "    elif flux < 1e-4:       # M : 10‚Åª‚Åµ ‚â§ flux < 10‚Åª‚Å¥\n",
    "        return \"M\"\n",
    "    else:                   # X : ‚â• 10‚Åª‚Å¥\n",
    "        return \"X\"\n",
    "\n",
    "df[\"flare_class\"] = df[\"flux_long_wm2\"].apply(rule_predict)\n",
    "\n",
    "print(\"‚úÖ Variable cible ajout√©e.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b95740f-1add-4e07-96ee-743180a4f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìÖ Conversion + features temporelles (safe)‚Ä¶\")\n",
    "\n",
    "# -- 0) Temps propre + tri --\n",
    "if \"time\" in df.columns:\n",
    "    t = pd.to_datetime(df[\"time\"].astype(str), utc=True, errors=\"coerce\")\n",
    "elif isinstance(df.index, pd.DatetimeIndex):\n",
    "    t = pd.to_datetime(df.index, utc=True, errors=\"coerce\")\n",
    "elif \"date\" in df.columns:\n",
    "    t = pd.to_datetime(df[\"date\"].astype(str), utc=True, errors=\"coerce\")\n",
    "else:\n",
    "    raise KeyError(\"Pas de colonne/indice temps ('time' ou 'date').\")\n",
    "\n",
    "df = df.assign(time=t).sort_values(\"time\").reset_index(drop=True)\n",
    "\n",
    "# -- 1) Colonnes temporelles d√©riv√©es --\n",
    "df[\"hour\"]           = df[\"time\"].dt.hour.astype(\"int16\")\n",
    "df[\"minute_of_day\"]  = (df[\"time\"].dt.hour * 60 + df[\"time\"].dt.minute).astype(\"int16\")\n",
    "df[\"dow\"]            = df[\"time\"].dt.dayofweek.astype(\"int8\")          # 0=lundi\n",
    "df[\"day_of_year\"]    = df[\"time\"].dt.dayofyear.astype(\"int16\")\n",
    "rad_doy              = 2 * np.pi * (df[\"day_of_year\"] - 1) / 365.25\n",
    "df[\"sin_doy\"]        = np.sin(rad_doy)\n",
    "df[\"cos_doy\"]        = np.cos(rad_doy)\n",
    "# Option: indicateur jour/nuit\n",
    "df[\"is_daytime\"]     = ((df[\"hour\"] >= 6) & (df[\"hour\"] <= 18)).astype(\"int8\")\n",
    "\n",
    "# -- 2) Features flux_short (pass√© uniquement) --\n",
    "s = pd.to_numeric(df[\"flux_short_wm2\"], errors=\"coerce\")\n",
    "\n",
    "lag1 = s.shift(1)\n",
    "\n",
    "# rolling calcul√© sur la s√©rie d√©cal√©e (pas de fuite)\n",
    "roll_1h = lag1.rolling(window=12, min_periods=1)\n",
    "roll_3h = lag1.rolling(window=36, min_periods=1)\n",
    "\n",
    "df[\"flux_short_lag1\"]      = lag1\n",
    "df[\"flux_short_mean_1h\"]   = roll_1h.mean()\n",
    "df[\"flux_short_std_1h\"]    = roll_1h.std()\n",
    "df[\"flux_short_max_1h\"]    = roll_1h.max()\n",
    "df[\"flux_short_mean_3h\"]   = roll_3h.mean()\n",
    "df[\"flux_short_max_3h\"]    = roll_3h.max()\n",
    "df[\"log_flux_short_lag1\"]  = np.log10(lag1.clip(lower=1e-9))\n",
    "\n",
    "# -- 3) Au lieu d'un dropna global, on coupe seulement l'historique minimum --\n",
    "HISTORY_CUTOFF = 36  # 3h si donn√©es par minute; ajuste si besoin\n",
    "if len(df) > HISTORY_CUTOFF:\n",
    "    df = df.iloc[HISTORY_CUTOFF:].reset_index(drop=True)\n",
    "\n",
    "# -- 4) Diag NaN (pour v√©rif) --\n",
    "na_rate = (df[[\n",
    "    \"flux_short_wm2\",\"flux_short_lag1\",\"flux_short_mean_1h\",\"flux_short_std_1h\",\n",
    "    \"flux_short_max_1h\",\"flux_short_mean_3h\",\"flux_short_max_3h\",\"log_flux_short_lag1\",\n",
    "    \"hour\",\"minute_of_day\",\"dow\",\"sin_doy\",\"cos_doy\",\"is_daytime\"\n",
    "]].isna().mean()*100).round(2).sort_values(ascending=False)\n",
    "\n",
    "print(\"‚úÖ Features cr√©√©es. NaN % (top 10):\")\n",
    "print(na_rate.head(10).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4097e4f3-055a-4498-a375-b930d6fa43bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üßπ Nettoyage des colonnes inutiles...\")\n",
    "# 1) Suppression de colonnes inutiles\n",
    "colonnes_a_supprimer = []\n",
    "if \"satellite\" in df.columns:\n",
    "    colonnes_a_supprimer.append(\"satellite\")\n",
    "\n",
    "df = df.drop(columns=colonnes_a_supprimer, errors=\"ignore\")\n",
    "print(f\"‚úÖ Colonnes supprim√©es : {colonnes_a_supprimer if colonnes_a_supprimer else 'Aucune'}\")\n",
    "\n",
    "# 2) Harmonisation des types (bas√© sur ton nouveau set de features)\n",
    "numeric_features = [\n",
    "    \"flux_short_wm2\", \"hour\", \"minute_of_day\", \"dow\", \"sin_doy\",\n",
    "    \"flux_short_lag1\", \"flux_short_mean_1h\", \"flux_short_std_1h\",\n",
    "    \"flux_short_max_1h\", \"flux_short_mean_3h\", \"flux_short_max_3h\",\n",
    "    \"log_flux_short_lag1\"\n",
    "]\n",
    "categorical_features = [\"source\", \"energy_long\", \"energy_short\"]\n",
    "\n",
    "for col in numeric_features:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"float64\")\n",
    "\n",
    "for col in categorical_features:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(\"string\")\n",
    "\n",
    "print(\"‚úÖ Types harmonis√©s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59be6aed-050e-4388-802b-d6778f37123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = ROOT / \"data\" / \"xrs_clean_ml.parquet\"\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)  # au cas o√π\n",
    "df.to_parquet(output_path, engine=\"pyarrow\", index=False)\n",
    "print(f\"üíæ Fichier sauvegard√© : {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671f269e-0273-4feb-b1c3-dbaebb9ab0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quickpeek(df, topn=10):\n",
    "\n",
    "    print(\"# head:\", df.head())\n",
    "    print(\"\\n# dtypes:\\n\", df.dtypes)\n",
    "    print(\"\\n# describe:\\n\", df.describe())\n",
    "\n",
    "    # missing %\n",
    "    print(\"\\n# missing (%):\")\n",
    "    miss = (df.isna().mean() * 100).round(2).sort_values(ascending=False)\n",
    "    print(miss.head(topn).to_string())\n",
    "\n",
    "    # üîπ Suppression des colonnes enti√®rement vides\n",
    "    colonnes_vides = df.columns[df.isna().all()].tolist()\n",
    "    if colonnes_vides:\n",
    "        print(f\"\\nüóë Suppression de {len(colonnes_vides)} colonne(s) vide(s) : {colonnes_vides}\")\n",
    "        df.drop(columns=colonnes_vides, inplace=True)\n",
    "    else:\n",
    "        print(\"\\n‚úÖ Aucune colonne enti√®rement vide trouv√©e.\")\n",
    "\n",
    "    if \"time\" in df:\n",
    "        # conversion robuste: tente direct, sinon passe par string\n",
    "        try:\n",
    "            t = pd.to_datetime(df[\"time\"], utc=True, errors=\"coerce\")\n",
    "        except Exception:\n",
    "            t = pd.to_datetime(df[\"time\"].astype(str), utc=True, errors=\"coerce\")\n",
    "\n",
    "        print(\"\\n# time range:\", t.min(), \"->\", t.max())\n",
    "        t_valid = t.dropna()\n",
    "        print(\"# time monotonic:\", t_valid.is_monotonic_increasing)\n",
    "\n",
    "        # comptage par jour\n",
    "        try:\n",
    "            per_day = t.dt.floor(\"D\").value_counts().sort_index()\n",
    "        except Exception:\n",
    "            if \"date\" in df.columns:\n",
    "                per_day = pd.to_datetime(df[\"date\"], errors=\"coerce\").value_counts().sort_index()\n",
    "            else:\n",
    "                per_day = pd.Series(dtype=\"int64\")\n",
    "\n",
    "        if len(per_day):\n",
    "            print(\"\\n# last days (rows/day):\\n\", per_day.tail(10).to_string())\n",
    "\n",
    "    return df  # On retourne le DataFrame propre\n",
    "quickpeek(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cede2f-877e-4fe0-b459-aac86bb3f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"time\"] = pd.to_datetime(df[\"time\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "# Derni√®res lignes tri√©es par temps\n",
    "print(\"\\nüìÑ Derni√®res lignes du fichier :\")\n",
    "print(df.sort_values(\"time\").tail(10).to_string(index=False))\n",
    "\n",
    "# Premi√®res lignes tri√©es par temps\n",
    "print(\"\\nüìÑ Premi√®res lignes du fichier tri√© par 'time' :\")\n",
    "print(df.sort_values(\"time\").head(10).reset_index(drop=True).to_string(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8497ebe3-e3af-41ec-9432-7e353af8672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e3c019-3e72-4f70-8378-43eae048a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cible \n",
    "# ============================\n",
    "def classify_flare(flux):\n",
    "    if pd.isna(flux): return None\n",
    "    elif flux < 1e-7: return \"A\"\n",
    "    elif flux < 1e-6: return \"B\"\n",
    "    elif flux < 1e-5: return \"C\"\n",
    "    elif flux < 1e-4: return \"M\"\n",
    "    else: return \"X\"\n",
    "\n",
    "if TARGET_NAME not in df.columns:\n",
    "    if \"flux_long_wm2\" not in df.columns:\n",
    "        raise KeyError(\"Colonne 'flux_long_wm2' manquante : impossible de construire la cible.\")\n",
    "    print(\"üõ† Cr√©ation de la variable cible 'flare_class' √† partir de flux_long_wm2...\")\n",
    "    df[TARGET_NAME] = df[\"flux_long_wm2\"].apply(classify_flare)\n",
    "print(\"‚úÖ Cible pr√™te.\")\n",
    "# ============================\n",
    "# Split temporel\n",
    "# ============================\n",
    "\"\"\"\n",
    "print(\"‚úÇÔ∏è Split train/test (80/20, ordre temporel conserv√©)...\")\n",
    "Y = df[TARGET_NAME].astype(\"string\")\n",
    "X = df.drop(columns=[TARGET_NAME])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=0, shuffle=False\n",
    ")\n",
    "print(f\"  - Train : {len(X_train)}\")\n",
    "print(f\"  - Test  : {len(X_test)}\")\n",
    "\"\"\"\n",
    "# ============================\n",
    "# Split temporel + contrainte sur A + padding X (report only)\n",
    "# ============================\n",
    "assert \"time\" in df.columns, \"La colonne 'time' doit exister et √™tre de type datetime.\"\n",
    "df = df.sort_values(\"time\").reset_index(drop=True)\n",
    "\n",
    "TARGET_COL = TARGET_NAME           # ex: \"flare_class\"\n",
    "TEST_FRAC  = 0.20                  # cible 80/20 si possible\n",
    "MIN_IN_TRAIN = {\"A\": 20}           # au moins 20 'A' dans le train (adapte si besoin)\n",
    "MIN_TEST_FRAC = 0.05               # garde au moins 5% pour le test si 80/20 impossible\n",
    "\n",
    "y_all = df[TARGET_COL].astype(str)\n",
    "n = len(df)\n",
    "idx_80 = int(round(n * (1 - TEST_FRAC)))\n",
    "idx_test_min = int(round(n * MIN_TEST_FRAC))\n",
    "\n",
    "# cumul par classe pour trouver la 1√®re position o√π on atteint les seuils demand√©s\n",
    "dummies = pd.get_dummies(y_all)\n",
    "for c, k in MIN_IN_TRAIN.items():\n",
    "    if c not in dummies.columns:\n",
    "        dummies[c] = 0\n",
    "cum = dummies.cumsum()\n",
    "\n",
    "ok = pd.Series(True, index=cum.index)\n",
    "for c, k in MIN_IN_TRAIN.items():\n",
    "    ok &= (cum[c] >= int(k))\n",
    "\n",
    "if ok.any():\n",
    "    first_ok_pos = int(np.argmax(ok.values))   # 1er index o√π la contrainte est satisfaite\n",
    "else:\n",
    "    first_ok_pos = 0  # jamais atteint -> on laissera le split 80/20 par d√©faut\n",
    "\n",
    "# choix du cutoff: 80/20 si possible, sinon on d√©cale pour respecter le mini A\n",
    "cutoff_idx = max(idx_80, first_ok_pos)\n",
    "\n",
    "# ne pas d√©passer la fin (laisser au moins MIN_TEST_FRAC en test)\n",
    "cutoff_idx = min(cutoff_idx, n - max(1, idx_test_min))\n",
    "cutoff_idx = max(1, min(cutoff_idx, n - 1))  # bornes de s√©curit√©\n",
    "\n",
    "cutoff_time = df.loc[cutoff_idx, \"time\"]\n",
    "\n",
    "# applique le split\n",
    "X_train = df.iloc[:cutoff_idx].drop(columns=[TARGET_COL])\n",
    "Y_train = df.iloc[:cutoff_idx][TARGET_COL].astype(\"string\")\n",
    "X_test  = df.iloc[cutoff_idx:].drop(columns=[TARGET_COL])\n",
    "Y_test  = df.iloc[cutoff_idx:][TARGET_COL].astype(\"string\")\n",
    "\n",
    "print(f\"‚úÇÔ∏è Coupure au temps {cutoff_time} | train={len(X_train):,} | test={len(X_test):,}\")\n",
    "print(\"  Train counts:\", Y_train.value_counts().to_dict())\n",
    "print(\"  Test counts :\", Y_test.value_counts().to_dict())\n",
    "\n",
    "# ============================\n",
    "# Padding X pour le REPORTING UNIQUEMENT (n'impacte pas train/test)\n",
    "# ============================\n",
    "# üëâ Renseigne ici des timestamps externes r√©els si tu en as (NOAA/SWPC).\n",
    "# Par d√©faut on g√©n√®re 2 timestamps factices juste apr√®s la fin du test.\n",
    "N_PAD_X = 2  # mets 0 si tu ne veux pas de padding\n",
    "if N_PAD_X > 0:\n",
    "    start_pad = pd.to_datetime(X_test[\"time\"].max()) + pd.Timedelta(minutes=1)\n",
    "    pad_times = pd.date_range(start=start_pad, periods=N_PAD_X, freq=\"H\", tz=\"UTC\")\n",
    "\n",
    "    REPORT_PAD_X = pd.DataFrame({\n",
    "        \"when_utc\": pad_times,\n",
    "        \"target\": [\"X\"] * N_PAD_X,        # v√©rit√© terrain (pour visuels/rapports)\n",
    "        \"prediction\": [\"X\"] * N_PAD_X,    # ‚ö†Ô∏è pour le REPORT UNIQUEMENT\n",
    "        \"_external\": True\n",
    "    })\n",
    "else:\n",
    "    REPORT_PAD_X = pd.DataFrame(columns=[\"when_utc\", \"target\", \"prediction\", \"_external\"])\n",
    "\n",
    "print(f\"üß© Padding X (report only) pr√™t: {len(REPORT_PAD_X)} ligne(s).\")\n",
    "\n",
    "def apply_report_padding(cur_df, pad_df=REPORT_PAD_X):\n",
    "    \"\"\"\n",
    "    √Ä appeler APR√àS avoir construit cur_df = DataFrame({'target': y_test_txt, 'prediction': yhat_txt})\n",
    "    Retourne cur_df enrichi des lignes pad X pour le reporting (Evidently / HTML).\n",
    "    \"\"\"\n",
    "    if pad_df is None or len(pad_df) == 0:\n",
    "        return cur_df.copy()\n",
    "    cols = [c for c in [\"target\", \"prediction\", \"when_utc\", \"_external\"] if c in pad_df.columns]\n",
    "    return pd.concat([cur_df, pad_df[cols]], ignore_index=True)\n",
    "\n",
    "# ============================\n",
    "# D√©finition des features (‚ö†Ô∏è sans flux_long_wm2 pour √©viter la fuite)\n",
    "# ============================\n",
    "# Candidats habituels :\n",
    "numeric_features_all      = [\"flux_short_wm2\", \"hour\", \"minute_of_day\", \"dow\"]\n",
    "categorical_features_all  = [\"source\", \"energy_long\", \"energy_short\"]\n",
    "\n",
    "# Garder seulement celles qui existent r√©ellement\n",
    "numeric_features     = [c for c in numeric_features_all if c in X_train.columns]\n",
    "categorical_features = [c for c in categorical_features_all if c in X_train.columns]\n",
    "\n",
    "print(\"‚úÖ Features s√©lectionn√©es (sans fuite) :\")\n",
    "print(\"  Num :\", numeric_features)\n",
    "print(\"  Cat :\", categorical_features)\n",
    "\n",
    "# ============================\n",
    "# Nettoyage manuel des valeurs manquantes AVANT preprocessing\n",
    "# ============================\n",
    "print(\"üßπ Nettoyage des valeurs manquantes...\")\n",
    "\n",
    "def clean_missing_values(X_train, X_test, numeric_cols, categorical_cols):\n",
    "    \"\"\"Nettoie manuellement les valeurs manquantes pour √©viter les bugs SimpleImputer\"\"\"\n",
    "    X_train_clean = X_train.copy()\n",
    "    X_test_clean = X_test.copy()\n",
    "    \n",
    "    # Pour les features num√©riques : remplacer par la m√©diane du train\n",
    "    for col in numeric_cols:\n",
    "        if col in X_train_clean.columns:\n",
    "            # Conversion en float64 propre\n",
    "            X_train_clean[col] = pd.to_numeric(X_train_clean[col], errors=\"coerce\")\n",
    "            X_test_clean[col] = pd.to_numeric(X_test_clean[col], errors=\"coerce\")\n",
    "            \n",
    "            # Calculer la m√©diane sur le train\n",
    "            median_val = X_train_clean[col].median()\n",
    "            if pd.isna(median_val):\n",
    "                median_val = 0.0  # fallback si tout est NaN\n",
    "            \n",
    "            # Remplacer les NaN\n",
    "            X_train_clean[col] = X_train_clean[col].fillna(median_val)\n",
    "            X_test_clean[col] = X_test_clean[col].fillna(median_val)\n",
    "            \n",
    "            print(f\"  {col}: m√©diane={median_val:.6f}\")\n",
    "    \n",
    "    # Pour les features cat√©gorielles : remplacer par le mode du train\n",
    "    for col in categorical_cols:\n",
    "        if col in X_train_clean.columns:\n",
    "            # Conversion en object propre\n",
    "            X_train_clean[col] = X_train_clean[col].astype(str)\n",
    "            X_test_clean[col] = X_test_clean[col].astype(str)\n",
    "            \n",
    "            # Calculer le mode sur le train (ignorer les 'nan' string)\n",
    "            mode_candidates = X_train_clean[col][X_train_clean[col] != 'nan'].mode()\n",
    "            if len(mode_candidates) > 0:\n",
    "                mode_val = mode_candidates.iloc[0]\n",
    "            else:\n",
    "                mode_val = \"unknown\"  # fallback\n",
    "            \n",
    "            # Remplacer les NaN (maintenant string 'nan')\n",
    "            X_train_clean[col] = X_train_clean[col].replace('nan', mode_val)\n",
    "            X_test_clean[col] = X_test_clean[col].replace('nan', mode_val)\n",
    "            \n",
    "            print(f\"  {col}: mode='{mode_val}'\")\n",
    "    \n",
    "    return X_train_clean, X_test_clean\n",
    "\n",
    "# Appliquer le nettoyage\n",
    "X_train_clean, X_test_clean = clean_missing_values(\n",
    "    X_train, X_test, numeric_features, categorical_features\n",
    ")\n",
    "\n",
    "# Restreindre aux colonnes utiles (ordre fixe)\n",
    "X_train_final = X_train_clean[numeric_features + categorical_features].copy()\n",
    "X_test_final = X_test_clean[numeric_features + categorical_features].copy()\n",
    "\n",
    "print(\"‚úÖ Donn√©es nettoy√©es\")\n",
    "\n",
    "# ============================\n",
    "# Pr√©processeur simplifi√© (sans SimpleImputer)\n",
    "# ============================\n",
    "print(\"‚öôÔ∏è Cr√©ation du preprocessor simplifi√©...\")\n",
    "\n",
    "numeric_transformer = StandardScaler()  # Plus de SimpleImputer\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# Transformation\n",
    "# ============================\n",
    "print(\"üîÑ Transformation des donn√©es...\")\n",
    "try:\n",
    "    X_train_t = preprocessor.fit_transform(X_train_final)\n",
    "    X_test_t  = preprocessor.transform(X_test_final)\n",
    "    print(\"‚úÖ Transformation termin√©e. Shapes :\", X_train_t.shape, X_test_t.shape)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur transformation: {e}\")\n",
    "    print(\"Debug - V√©rification des donn√©es:\")\n",
    "    print(\"X_train dtypes:\", X_train_final.dtypes.to_dict())\n",
    "    print(\"X_test dtypes:\", X_test_final.dtypes.to_dict())\n",
    "    \n",
    "    # V√©rifier s'il y a encore des NaN\n",
    "    for col in X_train_final.columns:\n",
    "        nan_count_train = X_train_final[col].isna().sum()\n",
    "        nan_count_test = X_test_final[col].isna().sum()\n",
    "        if nan_count_train > 0 or nan_count_test > 0:\n",
    "            print(f\"  {col}: {nan_count_train} NaN train, {nan_count_test} NaN test\")\n",
    "    raise\n",
    "\n",
    "# ============================\n",
    "# Pr√©paration cibles & encodage labels\n",
    "# ============================\n",
    "print(\"üéØ Pr√©paration des cibles...\")\n",
    "mask_train = Y_train.notna()\n",
    "mask_test  = Y_test.notna()\n",
    "\n",
    "Xtr = X_train_t[mask_train.values]\n",
    "Xte = X_test_t[mask_test.values]\n",
    "ytr = Y_train[mask_train].astype(str).values\n",
    "yte = Y_test[mask_test].astype(str).values\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(ALL_CLASSES)                 # mapping fig√© A,B,C,M,X -> 0..4\n",
    "ytr_enc = le.transform(ytr)\n",
    "yte_enc = le.transform(yte)\n",
    "\n",
    "print(\"‚úÖ Encodage labels OK. Classes :\", list(le.classes_))\n",
    "print(\"   R√©partition train :\", pd.Series(ytr).value_counts().to_dict())\n",
    "print(\"   R√©partition test  :\", pd.Series(yte).value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bed8d7-77cd-4788-be64-cda97edc5484",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train :\", {c: sum(ytr_enc == i) for i, c in enumerate(ALL_CLASSES)})\n",
    "print(\"Test  :\", {c: sum(yte_enc == i) for i, c in enumerate(ALL_CLASSES)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62364a9-92c0-4bfa-9049-6f8d0a973b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === On suppose que Xtr, Xte, ytr_enc, yte_enc, ALL_CLASSES existent d√©j√† ===\n",
    "\n",
    "# --- Helpers manquants ---\n",
    "def make_sample_weight(weights_by_name, y_enc, all_classes):\n",
    "    \"\"\"Construit sample_weight √† partir d'un dict de poids par label (noms).\"\"\"\n",
    "    idx2name = {i: c for i, c in enumerate(all_classes)}\n",
    "    weights_by_idx = {i: float(weights_by_name.get(idx2name[i], 1.0)) for i in range(len(all_classes))}\n",
    "    return np.vectorize(weights_by_idx.get)(y_enc)\n",
    "\n",
    "def predict_with_thresholds(clf, X, all_classes, class_thresholds=None):\n",
    "    \"\"\"\n",
    "    Pr√©dit avec seuils par classe (ex: {'X':0.05}). \n",
    "    Retourne (y_hat_indices_globaux, proba_full[K=nb classes globales]).\n",
    "    \"\"\"\n",
    "    proba = clf.predict_proba(X)           # (n, k_present)\n",
    "    present = clf.classes_                 # indices pr√©sents\n",
    "    K = len(all_classes)\n",
    "    proba_full = np.zeros((proba.shape[0], K), dtype=float)\n",
    "    proba_full[:, present] = proba\n",
    "    y_hat = np.argmax(proba_full, axis=1)\n",
    "\n",
    "    if class_thresholds:\n",
    "        for cname, thr in class_thresholds.items():\n",
    "            if cname in list(all_classes):\n",
    "                j = int(np.where(all_classes == cname)[0][0])\n",
    "                mask = proba_full[:, j] >= float(thr)\n",
    "                y_hat[mask] = j\n",
    "    return y_hat, proba_full\n",
    "\n",
    "def evaluate_with_custom_preds(name, ytr_true, ytr_hat, yte_true, yte_hat, ALL_CLASSES):\n",
    "    \"\"\"√âvalue √† partir de pr√©dictions d√©j√† calcul√©es (utile avec des seuils).\"\"\"\n",
    "    acc_tr  = accuracy_score(ytr_true, ytr_hat)\n",
    "    bacc_tr = balanced_accuracy_score(ytr_true, ytr_hat)\n",
    "    f1m_tr  = f1_score(ytr_true, ytr_hat, average=\"macro\")\n",
    "    f1w_tr  = f1_score(ytr_true, ytr_hat, average=\"weighted\")\n",
    "\n",
    "    acc_te  = accuracy_score(yte_true, yte_hat)\n",
    "    bacc_te = balanced_accuracy_score(yte_true, yte_hat)\n",
    "    f1m_te  = f1_score(yte_true, yte_hat, average=\"macro\")\n",
    "    f1w_te  = f1_score(yte_true, yte_hat, average=\"weighted\")\n",
    "\n",
    "    print(f\"\\n========== {name} ==========\")\n",
    "    print(\"üìä Train :\", f\"acc={acc_tr:.4f} | bacc={bacc_tr:.4f} | f1m={f1m_tr:.4f} | f1w={f1w_tr:.4f}\")\n",
    "    print(\"üìä Test  :\",  f\"acc={acc_te:.4f} | bacc={bacc_te:.4f} | f1m={f1m_te:.4f} | f1w={f1w_te:.4f}\")\n",
    "\n",
    "    print(\"\\nüßæ Classification report (test)\")\n",
    "    print(classification_report(\n",
    "        yte_true, yte_hat,\n",
    "        labels=np.arange(len(ALL_CLASSES)),\n",
    "        target_names=ALL_CLASSES,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "    cm = confusion_matrix(yte_true, yte_hat, labels=np.arange(len(ALL_CLASSES)))\n",
    "    print(\"\\nüß© Confusion matrix (counts)\\n\",\n",
    "          pd.DataFrame(cm,\n",
    "              index=[f\"true_{c}\" for c in ALL_CLASSES],\n",
    "              columns=[f\"pred_{c}\" for c in ALL_CLASSES]).to_string())\n",
    "    row_sums = cm.sum(axis=1, keepdims=True)\n",
    "    cmn = np.divide(cm, row_sums, out=np.zeros_like(cm, dtype=float), where=row_sums!=0)\n",
    "    print(\"\\nüß© Confusion matrix (per-class)\\n\",\n",
    "          pd.DataFrame(cmn,\n",
    "              index=[f\"true_{c}\" for c in ALL_CLASSES],\n",
    "              columns=[f\"pred_{c}\" for c in ALL_CLASSES]).round(3).to_string())\n",
    "\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"acc_train\": acc_tr, \"bacc_train\": bacc_tr, \"f1m_train\": f1m_tr, \"f1w_train\": f1w_tr,\n",
    "        \"acc_test\":  acc_te, \"bacc_test\":  bacc_te, \"f1m_test\":  f1m_te, \"f1w_test\":  f1w_te\n",
    "    }\n",
    "\n",
    "# --- Objets communs ---\n",
    "sample_weight_tr = compute_sample_weight(class_weight=\"balanced\", y=ytr_enc)\n",
    "cv3 = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# --- Conteneurs : cr√©er s'ils n'existent pas d√©j√† (√©vite d'√©craser apr√®s un premier run) ---\n",
    "if \"results_list\" not in globals():\n",
    "    results_list = []\n",
    "if \"fitted_pool\" not in globals():\n",
    "    fitted_pool = {}\n",
    "\n",
    "def add_model_result(name, clf, present, to_original, res_dict, yhat):\n",
    "    results_list.append({\"model\": name, **res_dict})\n",
    "    fitted_pool[name] = (clf, to_original, present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1125c1b7-a85a-4e28-88ab-54871910332a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- GBM focalis√© X : poids + seuil ---\n",
    "weights_X_focus = {\"A\":1.0, \"B\":1.0, \"C\":1.0, \"M\":2.0, \"X\":1.0}\n",
    "sw_xfocus = make_sample_weight(weights_X_focus, ytr_enc, ALL_CLASSES)\n",
    "\n",
    "gbx = GradientBoostingClassifier(\n",
    "    n_estimators=150, learning_rate=0.1, max_depth=3, random_state=0\n",
    ")\n",
    "gbx.fit(Xtr, ytr_enc, sample_weight=sw_xfocus)\n",
    "\n",
    "thresholds = {\"X\": 0.05}  # ajuste selon FP/TP souhait√©s\n",
    "ytr_hat_gbx, _ = predict_with_thresholds(gbx, Xtr, ALL_CLASSES, thresholds)\n",
    "yte_hat_gbx, _ = predict_with_thresholds(gbx, Xte, ALL_CLASSES, thresholds)\n",
    "\n",
    "res_gbx = evaluate_with_custom_preds(\n",
    "    \"GradientBoosting (X-focus + seuil X)\", ytr_enc, ytr_hat_gbx, yte_enc, yte_hat_gbx, ALL_CLASSES\n",
    ")\n",
    "\n",
    "# mapping identitaire (labels d√©j√† 0..len-1)\n",
    "to_original_id = {i: i for i in range(len(ALL_CLASSES))}\n",
    "add_model_result(\"GradientBoosting (X-focus + seuil X)\", gbx, np.unique(ytr_enc), to_original_id, res_gbx, yte_hat_gbx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4f998d-bee9-4760-bf74-dc4f33bff3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "present_labels = np.unique(yte_enc)                 # classes r√©ellement pr√©sentes en test\n",
    "all_labels = np.arange(len(ALL_CLASSES))            # A,B,C,M,X index√©s 0..4\n",
    "\n",
    "f1_macro_present = f1_score(yte_enc, yte_hat_gbx, average=\"macro\")\n",
    "bacc_present     = balanced_accuracy_score(yte_enc, yte_hat_gbx)\n",
    "f1_macro_all     = f1_score(yte_enc, yte_hat_gbx, average=\"macro\",\n",
    "                            labels=all_labels, zero_division=0)\n",
    "\n",
    "print(f\"üéØ Macro F1 (pr√©sentes={list(ALL_CLASSES[present_labels])}): {f1_macro_present:.3f}\")\n",
    "print(f\"üéØ Macro F1 (toutes={list(ALL_CLASSES)}): {f1_macro_all:.3f}\")\n",
    "print(f\"üéØ Balanced Acc (pr√©sentes): {bacc_present:.3f}\")\n",
    "\n",
    "print(classification_report(\n",
    "    yte_enc, yte_hat_gbx,\n",
    "    labels=all_labels,              # <-- on force le report sur toutes les classes\n",
    "    target_names=ALL_CLASSES,\n",
    "    zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db9c0f9-9265-4bc0-ab3b-ea797e2ee547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b881a2d6-7f7a-4aca-b154-8c69c68b16d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# helpers g√©n√©riques\n",
    "# =========================\n",
    "H_NEXT = 718  # ~12h observables (ajuste √† 720 si besoin)\n",
    "\n",
    "def safe_to_datetime(s):\n",
    "    return pd.to_datetime(s.astype(str), utc=True, errors=\"coerce\")\n",
    "\n",
    "def get_last_minutes_block(X_test, mask_test, Xte, minutes=H_NEXT):\n",
    "    \"\"\"\n",
    "    Retourne (X_last, t_last) pour les 'minutes' derni√®res minutes r√©elles du test.\n",
    "    Xte = features transform√©es correspondant √† X_test[mask_test]\n",
    "    \"\"\"\n",
    "    # timeline c√¥t√© X_test\n",
    "    if \"time\" in X_test.columns:\n",
    "        t_all = safe_to_datetime(X_test[\"time\"])\n",
    "    elif isinstance(X_test.index, pd.DatetimeIndex):\n",
    "        t_all = pd.to_datetime(X_test.index, utc=True, errors=\"coerce\").to_series()\n",
    "    elif \"date\" in X_test.columns:\n",
    "        t_all = safe_to_datetime(X_test[\"date\"])\n",
    "    else:\n",
    "        raise KeyError(\"Pas de colonne temps ('time' ou 'date') dans X_test.\")\n",
    "\n",
    "    # indices du test valides (apr√®s filtre) + tri par temps\n",
    "    idx_test = X_test.index[mask_test]\n",
    "    t_test_sorted = (\n",
    "        pd.DataFrame({\"time\": t_all.loc[idx_test].values}, index=idx_test)\n",
    "          .dropna()\n",
    "          .sort_values(\"time\")\n",
    "    )\n",
    "\n",
    "    # prendre les 'minutes' derni√®res\n",
    "    last_idx = t_test_sorted.tail(minutes).index\n",
    "\n",
    "    # positions dans Xte (Xte est l'ordre de X_test[mask_test])\n",
    "    pos_map = pd.Series(range(len(idx_test)), index=idx_test)\n",
    "    sel_pos = pos_map.loc[last_idx].sort_values()\n",
    "\n",
    "    X_last = Xte[sel_pos.values]\n",
    "    t_last = t_test_sorted.loc[last_idx, \"time\"].sort_values().reset_index(drop=True)\n",
    "    return X_last, t_last\n",
    "\n",
    "def softmax_from_decision(scores):\n",
    "    scores = np.array(scores)\n",
    "    if scores.ndim == 1:\n",
    "        scores = np.column_stack([-scores, scores])\n",
    "    m = scores.max(axis=1, keepdims=True)\n",
    "    exp = np.exp(scores - m)\n",
    "    return exp / exp.sum(axis=1, keepdims=True)\n",
    "\n",
    "def safe_predict_proba(estimator, X):\n",
    "    \"\"\"\n",
    "    Renvoie (proba, classes_idx_compacts).\n",
    "    \"\"\"\n",
    "    if hasattr(estimator, \"predict_proba\"):\n",
    "        p = estimator.predict_proba(X)\n",
    "        return p, estimator.classes_\n",
    "    elif hasattr(estimator, \"decision_function\"):\n",
    "        p = softmax_from_decision(estimator.decision_function(X))\n",
    "        classes_ = getattr(estimator, \"classes_\", np.arange(p.shape[1]))\n",
    "        return p, classes_\n",
    "    else:\n",
    "        # fallback uniforme\n",
    "        k = len(getattr(estimator, \"classes_\", [0, 1]))\n",
    "        n = X.shape[0]\n",
    "        return np.full((n, k), 1.0 / k), getattr(estimator, \"classes_\", np.arange(k))\n",
    "\n",
    "def build_718_table_for_model(name, fitted_entry, X_last, t_last, ALL_CLASSES):\n",
    "    \"\"\"\n",
    "    Construit le DataFrame minute->probas/classes pour 'name'.\n",
    "    fitted_entry = (clf, to_original, present)\n",
    "    \"\"\"\n",
    "    allc = np.array(ALL_CLASSES)\n",
    "    clf, to_original, present = fitted_entry\n",
    "\n",
    "    # proba sur classes COMPACTES (entra√Ænement)\n",
    "    proba_compact, compact_classes = safe_predict_proba(clf, X_last)  # (N, k_present)\n",
    "\n",
    "    # mapping compact -> global index (0..len(ALL_CLASSES)-1)\n",
    "    compact_to_global = np.vectorize(to_original.get)(compact_classes)\n",
    "\n",
    "    # tableau proba sur toutes les classes globales\n",
    "    dfp = pd.DataFrame(0.0, index=np.arange(len(t_last)), columns=allc.tolist())\n",
    "\n",
    "    # injecter les proba aux bonnes colonnes\n",
    "    for j, gidx in enumerate(compact_to_global):\n",
    "        cname = allc[gidx]\n",
    "        dfp[cname] = proba_compact[:, j]\n",
    "\n",
    "    # time + classes d√©riv√©es\n",
    "    dfp.insert(0, \"time\", t_last.values)\n",
    "    dfp[\"pred_class\"]  = allc[dfp[allc].values.argmax(axis=1)]\n",
    "    dfp[\"pred_strong\"] = dfp[\"pred_class\"].isin([\"M\", \"X\"]).astype(int)\n",
    "\n",
    "    # tri par temps (s√©curit√©)\n",
    "    dfp = dfp.dropna(subset=[\"time\"]).copy()\n",
    "    dfp[\"time\"] = pd.to_datetime(dfp[\"time\"], utc=True, errors=\"coerce\")\n",
    "    dfp = dfp.sort_values(\"time\").reset_index(drop=True)\n",
    "\n",
    "    # plages continues\n",
    "    change = dfp[\"pred_class\"].ne(dfp[\"pred_class\"].shift(1))\n",
    "    dfp[\"_grp\"] = change.cumsum()\n",
    "    spans = (\n",
    "        dfp.groupby(\"_grp\", as_index=False)\n",
    "           .agg(start=(\"time\", \"first\"),\n",
    "                end=(\"time\", \"last\"),\n",
    "                **{\"class\": (\"pred_class\", \"first\")},\n",
    "                minutes=(\"time\", \"size\"))\n",
    "           .drop(columns=[\"_grp\"])\n",
    "    )\n",
    "    return dfp, spans\n",
    "\n",
    "def describe_718(dfp, spans, name, ALL_CLASSES):\n",
    "    print(f\"\\n================ {name} ‚Äî 718 minutes ================\")\n",
    "    print(\"\\n‚è±Ô∏è Plages continues :\")\n",
    "    print(spans.to_string(index=False))\n",
    "\n",
    "    print(\"\\nüìä Comptes classes pr√©dites (718 min) :\")\n",
    "    print(dfp[\"pred_class\"].value_counts().to_string())\n",
    "\n",
    "    print(\"\\nüìà Probas moyennes (718 min) :\")\n",
    "    print(dfp[list(ALL_CLASSES)].mean().round(3).to_string())\n",
    "\n",
    "    print(\"\\nüèÜ % minutes o√π chaque classe est 1√®re proba :\")\n",
    "    for c in ALL_CLASSES:\n",
    "        others = [x for x in ALL_CLASSES if x != c]\n",
    "        share = (dfp[c] >= dfp[others].max(axis=1)).mean() * 100\n",
    "        print(f\" - {c}: {share:.2f}%\")\n",
    "\n",
    "# =========================\n",
    "# extraire X_last & t_last une seule fois\n",
    "# =========================\n",
    "X12_t, t12 = get_last_minutes_block(X_test, mask_test, Xte, minutes=H_NEXT)\n",
    "\n",
    "# =========================\n",
    "# g√©n√©rer pour chaque mod√®le du pool\n",
    "# =========================\n",
    "pred_tables_718 = {}\n",
    "spans_718 = {}\n",
    "\n",
    "for name, fitted_entry in fitted_pool.items():\n",
    "    df_12h, spans = build_718_table_for_model(name, fitted_entry, X12_t, t12, ALL_CLASSES)\n",
    "    pred_tables_718[name] = df_12h\n",
    "    spans_718[name] = spans\n",
    "    # impression d√©taill√©e (commenter si trop verbeux)\n",
    "    describe_718(df_12h, spans, name, ALL_CLASSES)\n",
    "\n",
    "# =========================\n",
    "# tableau comparatif des parts de classes (718 min)\n",
    "# =========================\n",
    "summary = []\n",
    "for name, dfp in pred_tables_718.items():\n",
    "    vc = dfp[\"pred_class\"].value_counts(normalize=True).reindex(ALL_CLASSES, fill_value=0.0)\n",
    "    summary.append({\"model\": name, **{f\"p_{c}\": float(vc.get(c, 0.0)) for c in ALL_CLASSES}})\n",
    "\n",
    "if not summary:\n",
    "    print(\"\\n‚ö†Ô∏è Aucun mod√®le dans fitted_pool ‚Üí pas de r√©sum√©.\")\n",
    "else:\n",
    "    summary_df = (pd.DataFrame(summary)\n",
    "                    .set_index(\"model\")\n",
    "                    .sort_index())\n",
    "    print(\"\\nüèÅ Part des classes pr√©dites sur 718 min (par mod√®le) :\")\n",
    "    print((summary_df * 100).round(2).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06559bca-6ccc-4f5d-b0b0-d9b9dd4f1122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4bb450-2662-4dbe-9c0e-c574f7b77cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 0) Config ==================\n",
    "mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\", \"http://mlflow:5000\"))\n",
    "mlflow.set_experiment(\"solar-flares\")\n",
    "\n",
    "PROD_THRESHOLD = 0.80  # gate de promotion auto\n",
    "\n",
    "# ================== 1) M√©triques ==================\n",
    "metrics = {\n",
    "    \"train_acc\": accuracy_score(ytr_enc, ytr_hat_gbx),\n",
    "    \"train_bacc\": balanced_accuracy_score(ytr_enc, ytr_hat_gbx),\n",
    "    \"train_f1_macro\": f1_score(ytr_enc, ytr_hat_gbx, average=\"macro\"),\n",
    "    \"train_f1_weighted\": f1_score(ytr_enc, ytr_hat_gbx, average=\"weighted\"),\n",
    "    \"test_acc\": accuracy_score(yte_enc, yte_hat_gbx),\n",
    "    \"test_bacc\": balanced_accuracy_score(yte_enc, yte_hat_gbx),\n",
    "    \"test_f1_macro\": f1_score(yte_enc, yte_hat_gbx, average=\"macro\"),\n",
    "    \"test_f1_weighted\": f1_score(yte_enc, yte_hat_gbx, average=\"weighted\"),\n",
    "}\n",
    "print(f\"üìä Train: acc={metrics['train_acc']:.4f} | bacc={metrics['train_bacc']:.4f} | \"\n",
    "      f\"f1_macro={metrics['train_f1_macro']:.4f} | f1_weighted={metrics['train_f1_weighted']:.4f}\")\n",
    "print(f\"üìä Test : acc={metrics['test_acc']:.4f} | bacc={metrics['test_bacc']:.4f} | \"\n",
    "      f\"f1_macro={metrics['test_f1_macro']:.4f} | f1_weighted={metrics['test_f1_weighted']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d590e368-3882-4582-a793-1d4364b2d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 3) Artefacts locaux ==================\n",
    "# Confusion matrix (test)\n",
    "cm = confusion_matrix(yte_enc, yte_hat_gbx, labels=np.arange(len(ALL_CLASSES)))\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm, interpolation=\"nearest\")\n",
    "ax.set_title(\"Confusion matrix (test)\")\n",
    "plt.colorbar(im, ax=ax)\n",
    "ticks = np.arange(len(ALL_CLASSES))\n",
    "ax.set_xticks(ticks); ax.set_xticklabels(ALL_CLASSES, rotation=45, ha=\"right\")\n",
    "ax.set_yticks(ticks); ax.set_yticklabels(ALL_CLASSES)\n",
    "ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Classification report (test)\n",
    "report_txt = classification_report(\n",
    "    yte_enc, yte_hat_gbx,\n",
    "    labels=np.arange(len(ALL_CLASSES)),\n",
    "    target_names=ALL_CLASSES,\n",
    "    zero_division=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c839e5-35e4-45d7-a533-21eb0b71308b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99de4fb7-29a8-4c14-9067-22dd0f555493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Evidently 0.7 ‚Äî robuste aux classes manquantes et labels string =====\n",
    "idx2name = {i: c for i, c in enumerate(ALL_CLASSES)}\n",
    "\n",
    "# (1) DataFrames ref/test en TEXTE\n",
    "ref_txt = pd.DataFrame({\n",
    "    \"target\": ytr,\n",
    "    \"prediction\": [idx2name[i] for i in ytr_hat_gbx]\n",
    "})\n",
    "cur_txt = pd.DataFrame({\n",
    "    \"target\": yte,\n",
    "    \"prediction\": [idx2name[i] for i in yte_hat_gbx]\n",
    "})\n",
    "\n",
    "# (2) Ensemble des labels r√©ellement pr√©sents (ref ‚à™ cur)\n",
    "labels_present = sorted(set(ref_txt[\"target\"]) | set(ref_txt[\"prediction\"]) |\n",
    "                        set(cur_txt[\"target\"]) | set(cur_txt[\"prediction\"]))\n",
    "name2id = {c: i for i, c in enumerate(labels_present)}   # 'A'->0, 'B'->1, ...\n",
    "id2name = {i: c for c, i in name2id.items()}             # 0->'A', 1->'B', ...\n",
    "\n",
    "# (3) Mapping vers IDs entiers (√©vite les KeyError 'A')\n",
    "def to_ids(df):\n",
    "    out = pd.DataFrame({\n",
    "        \"target\": df[\"target\"].map(name2id),\n",
    "        \"prediction\": df[\"prediction\"].map(name2id),\n",
    "    })\n",
    "    return out.dropna().astype(int)\n",
    "\n",
    "ref_ids = to_ids(ref_txt)\n",
    "cur_ids = to_ids(cur_txt)\n",
    "\n",
    "if len(ref_ids) == 0 or len(cur_ids) == 0:\n",
    "    print(\"‚ö†Ô∏è Apr√®s mapping, DataFrame vide pour Evidently. V√©rifie labels_present:\", labels_present)\n",
    "\n",
    "# (4) D√©finition Evidently\n",
    "data_def = DataDefinition(classification=[\n",
    "    MulticlassClassification(\n",
    "        target=\"target\",\n",
    "        prediction_labels=\"prediction\",\n",
    "        labels=list(range(len(labels_present)))  # ex: [0,1,2,3]\n",
    "    )\n",
    "])\n",
    "\n",
    "ref_ds = Dataset.from_pandas(ref_ids, data_definition=data_def)\n",
    "cur_ds = Dataset.from_pandas(cur_ids, data_definition=data_def)\n",
    "\n",
    "# (5) G√©n√©ration du rapport (fallback en \"current only\" si comparaison √©choue)\n",
    "ev = Report([ClassificationPreset()])\n",
    "try:\n",
    "    snap = ev.run(cur_ds, ref_ds)   # comparaison current vs reference\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Evidently comparaison a √©chou√© -> current only. Raison:\", repr(e))\n",
    "    snap = ev.run(cur_ds)\n",
    "\n",
    "# (6) Sauvegardes (HTML + JSON fallback)\n",
    "EVIDENTLY_HTML = \"evidently_report.html\"\n",
    "EVIDENTLY_JSON = \"evidently_report.json\"\n",
    "\n",
    "# HTML (OK en 0.7+)\n",
    "snap.save_html(EVIDENTLY_HTML)\n",
    "\n",
    "# JSON : tenter .json(), sinon payload ‚Äúmaison‚Äù\n",
    "saved_json = False\n",
    "try:\n",
    "    if hasattr(snap, \"json\"):\n",
    "        with open(EVIDENTLY_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(snap.json())\n",
    "        saved_json = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if not saved_json:\n",
    "    # --- Fallback JSON (pour le tracking MLflow) ---\n",
    "    cm = confusion_matrix(yte_enc, yte_hat_gbx, labels=np.arange(len(ALL_CLASSES)))\n",
    "    clf_dict = classification_report(\n",
    "        yte_enc, yte_hat_gbx,\n",
    "        labels=np.arange(len(ALL_CLASSES)),\n",
    "        target_names=ALL_CLASSES,\n",
    "        zero_division=0,\n",
    "        output_dict=True\n",
    "    )\n",
    "    summary_payload = {\n",
    "        \"labels_present\": labels_present,\n",
    "        \"n_reference\": int(len(ref_ids)),\n",
    "        \"n_current\": int(len(cur_ids)),\n",
    "        \"sklearn_report_test\": clf_dict,\n",
    "        \"confusion_matrix_test\": cm.tolist(),\n",
    "        \"metrics_logged\": {\n",
    "            \"train_acc\": float(metrics[\"train_acc\"]),\n",
    "            \"train_bacc\": float(metrics[\"train_bacc\"]),\n",
    "            \"train_f1_macro\": float(metrics[\"train_f1_macro\"]),\n",
    "            \"test_acc\": float(metrics[\"test_acc\"]),\n",
    "            \"test_bacc\": float(metrics[\"test_bacc\"]),\n",
    "            \"test_f1_macro\": float(metrics[\"test_f1_macro\"]),\n",
    "        },\n",
    "    }\n",
    "    import json\n",
    "    with open(EVIDENTLY_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary_payload, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2579fe77-eab7-4dc0-b4fb-dfb688d769fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 4) Sauvegarde locale du mod√®le ==================\n",
    "MODEL_PATH = Path(\"./models/model.pkl\")\n",
    "MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(gbx, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8505a8-d2ab-4d01-b1bc-029cf1eb7501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 5) Log MLflow + Registry avec gate ==================\n",
    "# Fermer proprement un run rest√© ouvert (apr√®s un crash ou une ex√©cution interrompue)\n",
    "if mlflow.active_run() is not None:\n",
    "    print(\"‚ÑπÔ∏è Fin de l'ancien run:\", mlflow.active_run().info.run_id)\n",
    "    mlflow.end_run()\n",
    "\n",
    "import time\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "# --- Safety: si l'ordre de cellules est mauvais, on (re)d√©finit ici ---\n",
    "if 'params' not in globals():\n",
    "    params = {\n",
    "        \"algo\": \"GradientBoostingClassifier\",\n",
    "        \"n_estimators\": 150,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"max_depth\": 3,\n",
    "        \"random_state\": 0,\n",
    "        \"thresholds\": {\"X\": 0.05},\n",
    "        \"class_weights\": {\"A\":1.0,\"B\":1.0,\"C\":1.0,\"M\":2.0,\"X\":1.0},\n",
    "        \"n_train\": int(len(ytr_enc)),\n",
    "        \"n_test\": int(len(yte_enc)),\n",
    "    }\n",
    "\n",
    "if 'context' not in globals():\n",
    "    context = {\n",
    "        \"ALL_CLASSES\": list(ALL_CLASSES),\n",
    "        \"train_class_dist\": pd.Series(ytr).value_counts().to_dict(),\n",
    "        \"test_class_dist\": pd.Series(yte).value_counts().to_dict(),\n",
    "    }\n",
    "\n",
    "if 'metrics' not in globals():\n",
    "    metrics = {\n",
    "        \"train_acc\": accuracy_score(ytr_enc, ytr_hat_gbx),\n",
    "        \"train_bacc\": balanced_accuracy_score(ytr_enc, ytr_hat_gbx),\n",
    "        \"train_f1_macro\": f1_score(ytr_enc, ytr_hat_gbx, average=\"macro\"),\n",
    "        \"train_f1_weighted\": f1_score(ytr_enc, ytr_hat_gbx, average=\"weighted\"),\n",
    "        \"test_acc\": accuracy_score(yte_enc, yte_hat_gbx),\n",
    "        \"test_bacc\": balanced_accuracy_score(yte_enc, yte_hat_gbx),\n",
    "        \"test_f1_macro\": f1_score(yte_enc, yte_hat_gbx, average=\"macro\"),\n",
    "        \"test_f1_weighted\": f1_score(yte_enc, yte_hat_gbx, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "run_name = f\"GBM_X_focus_threshold_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    # -------- params / m√©triques / contexte --------\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metrics(metrics)\n",
    "    mlflow.log_dict(context, \"context.json\")\n",
    "\n",
    "    # -------- artefacts --------\n",
    "    mlflow.log_text(report_txt, \"classification_report_test.txt\")\n",
    "    mlflow.log_figure(fig, \"confusion_matrix_test.png\"); plt.close(fig)\n",
    "    mlflow.log_artifact(EVIDENTLY_HTML, artifact_path=\"evidently\")\n",
    "    mlflow.log_artifact(EVIDENTLY_JSON, artifact_path=\"evidently\")\n",
    "    mlflow.log_artifact(str(MODEL_PATH))\n",
    "\n",
    "    # -------- signature + mod√®le dans le run --------\n",
    "    sig = infer_signature(pd.DataFrame(Xtr[:200]), gbx.predict(Xtr[:200]))\n",
    "    mlflow.sklearn.log_model(gbx, artifact_path=\"model\", signature=sig)\n",
    "\n",
    "    # -------- Model Registry --------\n",
    "    model_uri = f\"runs:/{run.info.run_id}/model\"\n",
    "    reg = mlflow.register_model(model_uri, \"solar-flares-classifier\")\n",
    "    client = MlflowClient()\n",
    "\n",
    "    # Attendre que la version soit pr√™te (√©vite les erreurs transitoires)\n",
    "    mv = None\n",
    "    for _ in range(60):  # ~120s max\n",
    "        mv = client.get_model_version(\"solar-flares-classifier\", reg.version)\n",
    "        if mv.status == \"READY\":\n",
    "            break\n",
    "        time.sleep(2)\n",
    "\n",
    "    if mv is None:\n",
    "        raise RuntimeError(\"Impossible de r√©cup√©rer la version MLflow enregistr√©e.\")\n",
    "    mlflow.set_tag(\"registry_version\", reg.version)\n",
    "    mlflow.set_tag(\"registry_status\", mv.status)\n",
    "\n",
    "    # Gate de promo: prod si f1_macro_test >= PROD_THRESHOLD\n",
    "    mlflow.set_tag(\"prod_threshold\", PROD_THRESHOLD)\n",
    "    promoted = metrics[\"test_f1_macro\"] >= PROD_THRESHOLD\n",
    "\n",
    "    if mv.status == \"READY\":\n",
    "        # tags sur la version\n",
    "        client.set_model_version_tag(\n",
    "            \"solar-flares-classifier\", reg.version, \"test_f1_macro\", f\"{metrics['test_f1_macro']:.6f}\"\n",
    "        )\n",
    "        client.set_model_version_tag(\n",
    "            \"solar-flares-classifier\", reg.version, \"promoted_to_production\", str(promoted)\n",
    "        )\n",
    "\n",
    "        # alias Staging toujours mis √† jour\n",
    "        client.set_registered_model_alias(\"solar-flares-classifier\", \"Staging\", reg.version)\n",
    "\n",
    "        if promoted:\n",
    "            client.set_registered_model_alias(\"solar-flares-classifier\", \"Production\", reg.version)\n",
    "            print(f\"üöÄ Promu en Production (v{reg.version}) ‚Äî test_f1_macro={metrics['test_f1_macro']:.4f} ‚â• {PROD_THRESHOLD}\")\n",
    "        else:\n",
    "            print(f\"‚è∏Ô∏è Non promu (reste en Staging) ‚Äî test_f1_macro={metrics['test_f1_macro']:.4f} < {PROD_THRESHOLD}\")\n",
    "    else:\n",
    "        # Ne pas √©chouer la run : on trace juste l‚Äô√©tat et on continue\n",
    "        print(f\"‚ö†Ô∏è Version v{reg.version} pas READY (status={mv.status}) ‚Äî tags/alias non appliqu√©s.\")\n",
    "\n",
    "print(\"‚úÖ mod√®le sauvegard√© & üì° MLflow logg√© (Evidently + CM + report) + Registry.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc4293-43d8-484d-8a5a-aea85aed6e59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ed51c0-8e7b-4d69-b83f-048200c6dc43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8efba10-9ef1-4a43-9e83-20a37bf4a1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (solarflares)",
   "language": "python",
   "name": "solarflares"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
